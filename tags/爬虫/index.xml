<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/tags/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 17 Feb 2020 23:10:40 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>python3 网络爬虫开发实战 爬取今日头条街拍图片</title>
      <link>https://anwangtanmi.github.io/posts/ade28d634366da4840814018c0d8bfde/</link>
      <pubDate>Mon, 17 Feb 2020 23:10:40 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ade28d634366da4840814018c0d8bfde/</guid>
      <description>2020/2/17&#xA;最近电脑坏掉了，可怜我刚买三个月的小新pro13啊，显示屏出了问题。不知道哪里坏掉了，打开黑屏，只有显示屏在某个特殊的角度才会显示亮（其他的角度其实是最暗的亮度，趴在上面能模模糊糊的看到轮廓，蜜汁问题），最骚的是我用一个比较重的东西压住电脑的左下角也就是我左手的位置的话，就显示的比较正常，？？？真搞不懂什么问题，不过今后对联想的电脑敬而远之吧。最近因为疫情，联想售后还不开门，香菇。&#xA;好，说了一大堆纯属发泄学习过程中的牢骚，这个电脑让我太难了。&#xA;刚把爬虫开发实战爬取今日头条图片的代码完成，发现网站有些变化，也就是书里的代码会无法爬取。主要有两个变化，1）模仿ajax请求爬取图片需要cookie；2）返回的json内容有所变化，并没有image_detail字段。下面是具体内容&#xA;1）添加cookie&#xA;cookie可以在街拍网页的第一个xhr请求中得到&#xA;而且，在第一个xhr请求之前可以看到有一个img请求的响应字段中设置了该cookie，而且过期时间很长，过期了再访问一次复制一下cookie就可以了。&#xA;所以，加上cookie之后的headers设置如下。&#xA;headers = { &#34;User-Agent&#34;:&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0&#34;, &#34;Referer&#34;:&#34;https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D&#34;, &#34;X-Requested-With&#34;:&#34;XMLHttpRequest&#34;, &#34;Cookie&#34;:&#34;__tasessionId=da6dc6d4q1581851848391; s_v_web_id=k6oxqyas_VGn20UCx_WXjQ_40eQ_9nhD_h0a5HUmjAsyD; csrftoken=cdcf90d6d3d490ab1326e261b2eff18a; tt_webid=6794001919121065486&#34; } 2）返回的json内容的格式发生了变化&#xA;在data中没有image_detail字段，有一个image_list字段，而且data中也不是每一个都有image_list字段&#xA;具体代码：&#xA;代码部分分为了三个部分，main函数；get_one_page函数:获取某个offset范围的json数据，并返回，这是一个生成器，可以在main中遍历；save_img函数：从生成器中得到img的url信息并保存&#xA;import requests, json, time,os from lxml import etree from requests import RequestException from hashlib import md5 base_url1 = &#34;https://www.toutiao.com/api/search/content/?aid=24&amp;amp;app_name=web_search&amp;amp;offset=&#34; base_url2 = &#34;&amp;amp;format=json&amp;amp;keyword=%E8%A1%97%E6%8B%8D&amp;amp;autoload=true&amp;amp;count=20&amp;amp;en_qc=1&amp;amp;cur_tab=1&amp;amp;from=search_tab&amp;amp;pd=synthesis&amp;amp;timestamp=&#34; headers = { &#34;User-Agent&#34;:&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0&#34;, &#34;Referer&#34;:&#34;https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D&#34;, &#34;X-Requested-With&#34;:&#34;XMLHttpRequest&#34;, &#34;Cookie&#34;:&#34;__tasessionId=da6dc6d4q1581851848391; s_v_web_id=k6oxqyas_VGn20UCx_WXjQ_40eQ_9nhD_h0a5HUmjAsyD; csrftoken=cdcf90d6d3d490ab1326e261b2eff18a; tt_webid=6794001919121065486&#34;</description>
    </item>
    <item>
      <title>[译文]构建一个高性能现代网络爬虫</title>
      <link>https://anwangtanmi.github.io/posts/3443ddf5de7ecce6bfcd021fc6cbcc86/</link>
      <pubDate>Wed, 25 Sep 2019 18:05:27 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3443ddf5de7ecce6bfcd021fc6cbcc86/</guid>
      <description>原文链接：&#xA;https://creekorful.me/building-fast-modern-web-crawler/ 文章目录 构建一个高性能现代网络爬虫 什么是爬虫？ Trandoshan：一个暗网爬虫 什么是暗网？ Trandoshan是如何设计的？ 怎么运行Trandoshan？ 如何使用Trandoshan？ 构建一个高性能现代网络爬虫 PS:本文为Building a fast modern web crawker的中文译文。&#xA;我一直对于爬虫具有很强烈的兴趣。我曾经使用过多种语言比如C++，Node.JS，Python等等来撰写爬虫程序，并且更吸引我的是爬虫背后的理论。&#xA;但是首先我们要讨论的问题是：什么是爬虫？&#xA;什么是爬虫？ 爬虫是一个通过浏览整个因特网从而去定位一些存在的页面、图片、PDF等等的计算机程序，并且允许用户去通过一个搜索引擎去检索它们。这就是隐藏在著名的Google搜索引擎背后的技术。&#xA;一个高性能的爬虫程序通常被设计为分布式结构：区别于运行在特定机器上的单个程序，它会在云上的多台机器上运行多个实例，这样的结构带来更好的任务再分配、更好的性能和更大的带宽。（ps:这里使用吞吐量会不会更合适？）&#xA;但是分布式软件并不是没有瑕疵的：一些因素可能会给你的的程序带来额外的延迟、或者可能会降低你的程序的性能，比如说网络的延迟、同步的问题、缺乏设计的通信协议等等。&#xA;为了追求更好的性能，一个分布式的爬虫应该有足够良好的设计：这让消除许多性能上的瓶颈成为可能，就像法国的海军上将Olivier Lajous说的：&#xA;一个链子的强度取决于最薄弱的一环。 Trandoshan：一个暗网爬虫 你可能知道很多非常成功的网络爬虫，比如Google。所以我并不想去做一个同样的东西。我当下想要去构建一个基于暗网的爬虫。（ps:trandoshan是星球大战中的狩猎种族）&#xA;什么是暗网？ 没必要去使用很多术语去形容什么是暗网，要写起暗网的来龙去脉可能要新建一篇文章。&#xA;Web是由三层结构组成的，我们可以将其视为一座冰山：&#xA;第一层：表面网络，或者说是净网是我们每天最常接触的网络的那部分。它们被一些炙手可热的网络爬虫比如Google，Qwant，Duckduckgo等等所定位。 第二层：更深层次的网络，是由一些无法被定位的网页组成的，这意味着你是用搜索引擎都找不到这些网页，但是你却可以直接使用URL和IP地址来访问这些页面。 第三层：暗网，这是一类你是用浏览器都无法访问到的网页。你需要使用特定的应用程序或者特定的代理才可以访问。最出名的暗网是隐藏在洋葱头网络下的。你可以使用以.onion结尾的URL去访问它们。 Trandoshan是如何设计的？ 在分别讲解这些进程的作用之前，我觉得首先要讲清楚的是这些进程之间如何通信。&#xA;进程间通讯（Inter Process Communication, IPC），主要是通过使用一个基于生产者/消费者模式的名为NATS（图中黄色的线）的通讯协议，每个在NATS中的消息都有一个主题（就像邮件里的那样），支持其他进程去识别并且仅读取它们想要读取的消息。NATS支持扩展：比如可以支持十个爬虫进程从一个消息服务器并发的读取消息（许多实例可以同时运行而不出任何bug）并因此可以提升性能。&#xA;Trandoshan分为四个主要的进程：&#xA;爬虫：用于爬取页面的进程：它们从NATS读取将要爬取的页面的URL（消息的主题是”todoUrls”），爬取它，并且获取整个页面中显示的全部URL，并且发送这些URL到NATS中（这些消息的主题是”crawledUrls”），而页面的内容则以主题”content”发送到NATS。 调度器：这个进程用于检查URL：它读取主题为”crawledUrls”，检查其是否是已经爬取过的URL，如果还没有被爬取过，则将URL以主题”todoUrls”发送到NATS。 持久器：这个进程用于网页内容的构建：它读取以”content”为主题的消息，并且存储到非关系型数据库中（MongoDB） 接口：给其他进程开放用于聚合数据的进程。比如开放给调度器的用于确定URL是否被爬取过的接口，相比于调度器直接和数据库进行交互，更倾向于调度器和API们交互。 不同的进程们都是使用Go语言进行编写的：因为它的性能很好（可以被编译为二进制文件）并且有很多的库。Go是用来构建高性能的分布式系统的完美解决方案。&#xA;Trandoshan的源码在github的这里：https://github.com/trandoshan-io&#xA;怎么运行Trandoshan？ 就像之前讲过的一样，Trandoshan被设计为运行在一个分布式的系统上，并且可以使用Docker的镜像来运行，这对于云来说是很好的支持。事实上我整理了一个存储着所有部署需要的配置文件的仓库，可以用于部署Trandoshan实例在K8S上。这些文件在这里：https://github.com/trandoshan-io/k8s 并且docker的镜像也都上传到了Docker Hub。&#xA;如果你拥有一个配置成功的kubectl（K8S的控制程序），你可以通过一条简单的命令部署Trandoshan：&#xA;./bootstrap.sh 不然的话你可以使用Docker和docker-compose在本地运行Trandoshan。在trandoshan-parent这个仓库中有构建文件和shell脚本，所以你可以使用以下命令来运行这个应用：&#xA;./deploy.sh 如何使用Trandoshan？ 现在有一个小型的Angular应用去检索定位内容。这个页面使用了API进程去完成对于数据库的检索工作。</description>
    </item>
    <item>
      <title>Java——网络爬虫基础</title>
      <link>https://anwangtanmi.github.io/posts/047f6181f0b9bd40bb4759875452a051/</link>
      <pubDate>Mon, 23 Sep 2019 13:25:01 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/047f6181f0b9bd40bb4759875452a051/</guid>
      <description> 网络爬虫 工作原理 网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型： 通用网络爬虫（General Purpose Web Crawler） 聚焦网络爬虫（Focused Web Crawler） 增量式网络爬虫（Incremental Web Crawler） 深层网络爬虫（Deep Web Crawler）。 实际的网络爬虫系统通常是几种爬虫技术相结合实现的 </description>
    </item>
    <item>
      <title>tor 网络, scrapy, shadowsock, polipo 处理ip 的反爬虫策略</title>
      <link>https://anwangtanmi.github.io/posts/53bf127d32adec349cfa0590a50743e9/</link>
      <pubDate>Tue, 29 Jan 2019 17:01:37 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/53bf127d32adec349cfa0590a50743e9/</guid>
      <description>tor 网络, scrapy, shadowsock, polipo 处理ip 的反爬虫策略&#xA;环境 ubuntu 18.04&#xA;1. 首先需要学会FQ, 在此不介绍, 也不敢介绍， 请查询shadowsock 相关&#xA;2. 下载 tor 浏览器（https://www.torproject.org/download/download.html），&#xA;https://dist.torproject.org/torbrowser/8.0.4/tor-browser-linux64-8.0.4_en-US.tar.xz&#xA;解压： xz -d tor-browser-linux64-8.0.4_en-US.tar.xz&#xA;解压： tar -xvf tor-browser-linux64-8.0.4_en-US.tar&#xA;启动： ./start-tor-browser&#xA;3. 给tor 配置代理 shadowsock。 使用 tor 浏览器访问： https://check.torproject.org/， 查看配置结果&#xA;4. 安装polipo(用于在和tor 网络完成协议转换)&#xA;5. 配置 polipo&#xA;先查看 tor 代理的端口： ps -ef | grep tor | grep Sock ， 一般是9150&#xA;修改: /etc/polipo/config 添加如下(注意别用冲突的端口)：&#xA;socksParentProxy = localhost:9150&#xA;proxyPort = 8123&#xA;6. 重启 polipo: service polipo restart</description>
    </item>
    <item>
      <title>(二)暗网信息爬取（python）</title>
      <link>https://anwangtanmi.github.io/posts/0c3bf4459264696f818771834147ebd3/</link>
      <pubDate>Wed, 24 May 2017 22:25:19 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0c3bf4459264696f818771834147ebd3/</guid>
      <description>首先要感谢舍友大佬提供的ShadowsocksR以及相应配置。感谢在本阶段帮助过我的学长学姐，谢谢。&#xA;暗网（深网，不可见网，隐藏网）是指那些储存在网络数据库里、不能通过超链接访问而需要通过动态网页技术访问的资源集合，不属于那些可以被标准搜索引擎索引的表面网络。 动态网页的url不固定，但可以被爬虫爬取，这是第一个点。 由于相关法律风险，本暗网爬虫代码不开源，但我会将我所理解的核心内容记录。 代码环境为ubuntu，使用语言python，使用库urllib2，socks, socket。 这里不用requests库，在参考许多教程如https://github.com/kennethreitz/requests/issues/3863/ 后发现，requests似乎无法使用socks5的代理端口，遂弃之 进入暗网的“门”为TorBrowser或者Tor 注意：torbrowser和tor并非一个东西，torbrowser是基于火狐浏览器的洋葱浏览器，通常用来做暗网入口，而且一般情况下已经足够了，但由于需要代码环境，我使用了纯Tor 以下是我的搭建步骤： 一，安装配置Tor 在ubuntu命令行输入&#xA;sudo apt-get install tor /etc/init.d/tor restart 启动后socks监听9050端口。&#xA;tor --hash-password mypassword 用来输入你的密码 编辑/etc/tor/torrc 在其中加上&#xA;ControlPort 9051 RunAsDaemon 1 Socks5Proxy 127.0.0.1:1080 HashedControlPassword 16:872860B76453A77D60CA2BB8C1A7042072093276A3D701AD684053EC4C 让ControlPort监听9051端口，后边那个16:开头的hash就是上一步得到的。Socks5Proxy是可以使shadowsocks为tor的前端代理（我使用的shadowsocks的端口为1080） 最后重启tor&#xA;/etc/init.d/tor restart （经过曲折的一番探索，可以基本断定，python的stem库控制tor，并不是控制tor浏览器，而是可以利用tor作为自己的匿名代理，或者监视tor的流量走向。stem库目测不能为暗网爬虫做出什么贡献）&#xA;二，设置前端代理shadowsocks 这一段本应该放在最前面，但由于本人的shadowsocks完全依靠了舍友的鼎力帮助，这一段还需要往后自己探索。 就本人理解，shadowsocks作为前端代理最主要的功能就是翻墙。。。 对外提供端口为1080。&#xA;三，利用Tor的9050端口爬取暗网。 使用urllib2库，socks库作为socks5代理，示例代码如下：&#xA;import socket import socks import urllib2 ipcheck_url = &#39;http://checkip.amazonaws.com/&#39; # Actual IP. print(urllib2.urlopen(ipcheck_url).read()) # Tor IP. socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, &#39;127.0.0.1&#39;, 9050) socket.socket = socks.socksocket print(urllib2.urlopen(ipcheck_url).read() 该代码引用自https://stackoverflow.com/questions/1096379/how-to-make-urllib2-requests-through-tor-in-python</description>
    </item>
    <item>
      <title>Scrapy: Run Using TOR and Multiple Agents</title>
      <link>https://anwangtanmi.github.io/posts/73f9007b80dfb2c16b2436fdb7f661f8/</link>
      <pubDate>Wed, 08 Apr 2015 16:24:36 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/73f9007b80dfb2c16b2436fdb7f661f8/</guid>
      <description>http://pkmishra.github.io/blog/2013/03/18/how-to-run-scrapy-with-TOR-and-multiple-browser-agents-part-1-mac/&#xA;Scrapy is a brilliant and well documented crawler written in python. Though it is not as scalable as Apache Nutch but it can easily handle thousands of sites easily. You can get up and running very quickly using the official documentation. Tor gives you power to keep your privacy and security.Tor can hide you so that website can not track your identity. You may read more about TOR in official site.</description>
    </item>
  </channel>
</rss>
