<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/tags/python/</link>
    <description>Recent content in python on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 17 Feb 2020 23:10:40 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>python3 网络爬虫开发实战 爬取今日头条街拍图片</title>
      <link>https://anwangtanmi.github.io/posts/ade28d634366da4840814018c0d8bfde/</link>
      <pubDate>Mon, 17 Feb 2020 23:10:40 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ade28d634366da4840814018c0d8bfde/</guid>
      <description>2020/2/17&#xA;最近电脑坏掉了，可怜我刚买三个月的小新pro13啊，显示屏出了问题。不知道哪里坏掉了，打开黑屏，只有显示屏在某个特殊的角度才会显示亮（其他的角度其实是最暗的亮度，趴在上面能模模糊糊的看到轮廓，蜜汁问题），最骚的是我用一个比较重的东西压住电脑的左下角也就是我左手的位置的话，就显示的比较正常，？？？真搞不懂什么问题，不过今后对联想的电脑敬而远之吧。最近因为疫情，联想售后还不开门，香菇。&#xA;好，说了一大堆纯属发泄学习过程中的牢骚，这个电脑让我太难了。&#xA;刚把爬虫开发实战爬取今日头条图片的代码完成，发现网站有些变化，也就是书里的代码会无法爬取。主要有两个变化，1）模仿ajax请求爬取图片需要cookie；2）返回的json内容有所变化，并没有image_detail字段。下面是具体内容&#xA;1）添加cookie&#xA;cookie可以在街拍网页的第一个xhr请求中得到&#xA;而且，在第一个xhr请求之前可以看到有一个img请求的响应字段中设置了该cookie，而且过期时间很长，过期了再访问一次复制一下cookie就可以了。&#xA;所以，加上cookie之后的headers设置如下。&#xA;headers = { &#34;User-Agent&#34;:&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0&#34;, &#34;Referer&#34;:&#34;https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D&#34;, &#34;X-Requested-With&#34;:&#34;XMLHttpRequest&#34;, &#34;Cookie&#34;:&#34;__tasessionId=da6dc6d4q1581851848391; s_v_web_id=k6oxqyas_VGn20UCx_WXjQ_40eQ_9nhD_h0a5HUmjAsyD; csrftoken=cdcf90d6d3d490ab1326e261b2eff18a; tt_webid=6794001919121065486&#34; } 2）返回的json内容的格式发生了变化&#xA;在data中没有image_detail字段，有一个image_list字段，而且data中也不是每一个都有image_list字段&#xA;具体代码：&#xA;代码部分分为了三个部分，main函数；get_one_page函数:获取某个offset范围的json数据，并返回，这是一个生成器，可以在main中遍历；save_img函数：从生成器中得到img的url信息并保存&#xA;import requests, json, time,os from lxml import etree from requests import RequestException from hashlib import md5 base_url1 = &#34;https://www.toutiao.com/api/search/content/?aid=24&amp;amp;app_name=web_search&amp;amp;offset=&#34; base_url2 = &#34;&amp;amp;format=json&amp;amp;keyword=%E8%A1%97%E6%8B%8D&amp;amp;autoload=true&amp;amp;count=20&amp;amp;en_qc=1&amp;amp;cur_tab=1&amp;amp;from=search_tab&amp;amp;pd=synthesis&amp;amp;timestamp=&#34; headers = { &#34;User-Agent&#34;:&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0&#34;, &#34;Referer&#34;:&#34;https://www.toutiao.com/search/?keyword=%E8%A1%97%E6%8B%8D&#34;, &#34;X-Requested-With&#34;:&#34;XMLHttpRequest&#34;, &#34;Cookie&#34;:&#34;__tasessionId=da6dc6d4q1581851848391; s_v_web_id=k6oxqyas_VGn20UCx_WXjQ_40eQ_9nhD_h0a5HUmjAsyD; csrftoken=cdcf90d6d3d490ab1326e261b2eff18a; tt_webid=6794001919121065486&#34;</description>
    </item>
    <item>
      <title>Python爬虫突破封禁的6种常见方法</title>
      <link>https://anwangtanmi.github.io/posts/13e2bf04b2d5b927025ddcee71a3e552/</link>
      <pubDate>Wed, 17 Aug 2016 22:36:59 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/13e2bf04b2d5b927025ddcee71a3e552/</guid>
      <description>在互联网上进行自动数据采集（抓取）这件事和互联网存在的时间差不多一样长。今天大众好像更倾向于用“网络数据采集”，有时会把网络数据采集程序称为网络机器人（bots）。最常用的方法是写一个自动化程序向网络服务器请求数据（通常是用HTML表单或其他网页文件），然后对数据进行解析，提取需要的信息。&#xA;本文假定读者已经了解如何用代码来抓取一个远程的URL，并具备表单如何提交及JavaScript在浏览器如何运行的机制。想更多了解网络数据采集基础知识，可以参考文后的资料。&#xA;在采集网站的时会遇到一些比数据显示在浏览器上却抓取不出来更令人沮丧的事情。也许是向服务器提交自认为已经处理得很好的表单却被拒绝，也许是自己的IP地址不知道什么原因直接被网站封杀，无法继续访问。&#xA;原因可能是一些最复杂的bug，也可能是这些bug让人意想不到（程序在一个网站上可以正常使用，但在另一个看起来完全一样的网站上却用不了）。最有可能出现的情况是：对方有意不让爬虫抓取信息。网站已经把你定性为一个网络机器人直接拒绝了，你无法找出原因。&#xA;接下来就介绍一些网络采集的黑魔法（HTTP headers、CSS和HTML表单等），以克服网站阻止自动采集。不过，先让我们聊聊道德问题。&#xA;网络爬虫的道德与礼仪 说实话，从道德角度讲，写作以下文字不易。我自己的网站被网络机器人、垃圾邮件生成器、网络爬虫和其他各种不受欢迎的虚拟访问者骚扰过很多次了，你的网站可能也一样。既然如此，为什么还要介绍那些更强大的网络机器人呢？有几个很重要的理由。&#xA;白帽子工作。在采集那些不想被采集的网站时，其实存在一些非常符合道德和法律规范的理由。比如我之前的工作就是做网络爬虫，我曾做过一个自动信息收集器，从未经许可的网站上自动收集客户的名称、地址、电话号码和其他个人信息，然后把采集的信息提交到网站上，让服务器删除这些客户信息。为了避免竞争，这些网站都会对网络爬虫严防死守。但是，我的工作要确保公司的客户们都匿名（这些人都是家庭暴力受害者，或者因其他正当理由想保持低调的人），这为网络数据采集工作创造了极其合理的条件，我很高兴自己有能力从事这项工作。 虽然不太可能建立一个完全“防爬虫”的网站（最起码得让合法的用户可以方便地访问网站），但我还是希望以下内容可以帮助人们保护自己的网站不被恶意攻击。下文将指出每一种网络数据采集技术的缺点，你可以利用这些缺点保护自己的网站。其实，大多数网络机器人一开始都只能做一些宽泛的信息和漏洞扫描，接下来介绍的几个简单技术就可以挡住99%的机器人。但是，它们进化的速度非常快，最好时刻准备迎接新的攻击。 和大多数程序员一样，我从来不相信禁止某一类信息的传播就可以让世界变得更和谐。 阅读之前，请牢记： 这里演示的许多程序和介绍的技术都不应该在网站上使用。&#xA;爬虫黑科技：网络机器人看起来像人类用户的一些方法 网站防采集的前提就是要正确地区分人类访问用户和网络机器人。虽然网站可以使用很多识别技术（比如验证码）来防止爬虫，但还是有一些十分简单的方法，可以让你的网络机器人看起来更像人类访问用户。&#xA;1.　构造合理的HTTP请求头 除了处理网站表单，requests模块还是一个设置请求头的利器。HTTP的请求头是在你每次向网络服务器发送请求时，传递的一组属性和配置信息。HTTP定义了十几种古怪的请求头类型，不过大多数都不常用。只有下面的七个字段被大多数浏览器用来初始化所有网络请求（表中信息是我自己浏览器的数据）。&#xA;经典的Python爬虫在使用urllib标准库时，都会发送如下的请求头：&#xA;如果你是一个防范爬虫的网站管理员，你会让哪个请求头访问你的网站呢？&#xA;安装Requests&#xA;可在模块的网站上找到下载链接 （http://docs.python-requests.org/en/latest/user/install/）和安装方法，或者用任意第三方Python模块安装器进行安装。 请求头可以通过requests模块进行自定义。https://www.whatismybrowser.com/网站就是一个非常棒的网站，可以让服务器测试浏览器的属性。我们用下面的程序来采集这个网站的信息，验证我们浏览器的cookie设置：&#xA;程序输出结果中的请求头应该和程序中设置的headers是一样的。&#xA;虽然网站可能会对HTTP请求头的每个属性进行“是否具有人性”的检查，但是我发现通常真正重要的参数就是User-Agent。无论做什么项目，一定要记得把User-Agent属性设置成不容易引起怀疑的内容，不要用Python-urllib/3.4。另外，如果你正在处理一个警觉性非常高的网站，就要注意那些经常用却很少检查的请求头，比如Accept-Language属性，也许它正是那个网站判断你是个人类访问者的关键。&#xA;请求头会改变你观看网络世界的方式 假设你想为一个机器学习的研究项目写一个语言翻译机，却没有大量的翻译文本来测试它的效果。很多大型网站都会为同样的内容提供不同的语言翻译，根据请求头的参数响应网站不同的语言版本。因此，你只要简单地把请求头属性从Accept-Language:en-US修改成Accept-Language:fr，就可以从网站上获得“Bonjour”（法语，你好）这些数据来改善翻译机的翻译效果了（大型跨国企业通常都是好的采集对象）。 请求头还可以让网站改变内容的布局样式。例如，用移动设备浏览网站时，通常会看到一个没有广告、Flash以及其他干扰的简化的网站版本。因此，把你的请求头User-Agent改成下面这样，就可以看到一个更容易采集的网站了！ User-Agent:Mozilla/5.0 (iPhone; CPU iPhone OS 712 like Mac OS X) App leWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53&#xA;2.　设置cookie的学问 虽然cookie是一把双刃剑，但正确地处理cookie可以避免许多采集问题。网站会用cookie跟踪你的访问过程，如果发现了爬虫异常行为就会中断你的访问，比如特别快速地填写表单，或者浏览大量页面。虽然这些行为可以通过关闭并重新连接或者改变IP地址来伪装，但是如果cookie暴露了你的身份，再多努力也是白费。&#xA;在采集一些网站时cookie是不可或缺的。要在一个网站上持续保持登录状态，需要在多个页面中保存一个cookie。有些网站不要求在每次登录时都获得一个新cookie，只要保存一个旧的“已登录”的cookie就可以访问。&#xA;如果你在采集一个或者几个目标网站，建议你检查这些网站生成的cookie，然后想想哪一个cookie是爬虫需要处理的。有一些浏览器插件可以为你显示访问网站和离开网站时cookie是如何设置的。EditThisCookie（http://www.editthiscookie.com/）是我最喜欢的Chrome浏览器插件之一。&#xA;因为requests模块不能执行JavaScript，所以它不能处理很多新式的跟踪软件生成的cookie，比如GoogleAnalytics，只有当客户端脚本执行后才设置cookie（或者在用户浏览页面时基于网页事件产生cookie，比如点击按钮）。要处理这些动作，需要用Selenium和PhantomJS包。&#xA;Selenium与PhantomJS Selenium（http://www.seleniumhq.org/）是一个强大的网络数据采集工具，最初是为网站自动化测试而开发的。近几年，它还被广泛用于获取精确的网站快照，因为它们可以直接运行在浏览器上。Selenium可以让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。 Selenium自己不带浏览器，它需要与第三方浏览器结合在一起使用。例如，如果你在Firefox上运行Selenium，可以直接看到Firefox窗口被打开，进入网站，然后执行你在代码中设置的动作。虽然这样可以看得更清楚，但是我更喜欢让程序在后台运行，所以我PhantomJS（http://phantomjs.org/download.html）代替真实的浏览器。 PhantomJS是一个“无头”（headless）浏览器。它会把网站加载到内存并执行页面上的JavaScript，但不会向用户展示网页的图形界面。将Selenium和PhantomJS结合在一起，就可以运行一个非常强大的网络爬虫了，可以处理cookie、JavaScrip、header，以及任何你需要做的事情。 可以从PyPI网站（https://pypi.python.org/simple/selenium/）下载Selenium库，也可以用第三方管理器（像pip）用命令行安装。&#xA;你可以对任意网站（本例用的是http://pythonscraping.com）调用webdriver的get_cookie()方法来查看cookie：&#xA;这样就可以获得一个非常典型的Google Analytics的cookie列表：&#xA;还可以调用deletecookie()、addcookie()和deleteallcookies()方法来处理cookie。另外，还可以保存cookie以备其他网络爬虫使用。下面的例子演示了如何把这些函数组合在一起：&#xA;在这个例子中，第一个webdriver获得了一个网站，打印cookie并把它们保存到变量savedCookies里。第二个webdriver加载同一个网站（技术提示：必须首先加载网站，这样Selenium才能知道cookie属于哪个网站，即使加载网站的行为对我们没任何用处），删除所有的cookie，然后替换成第一个webdriver得到的cookie。当再次加载这个页面时，两组cookie的时间戳、源代码和其他信息应该完全一致。从GoogleAnalytics的角度看，第二个webdriver现在和第一个webdriver完全一样。&#xA;3.　正常的时间访问路径 有一些防护措施完备的网站可能会阻止你快速地提交表单，或者快速地与网站进行交互。即使没有这些安全措施，用一个比普通人快很多的速度从一个网站下载大量信息也可能让自己被网站封杀。&#xA;因此，虽然多线程程序可能是一个快速加载页面的好办法——在一个线程中处理数据，另一个线程中加载页面——但是这对编写好的爬虫来说是恐怖的策略。还是应该尽量保证一次加载页面加载且数据请求最小化。如果条件允许，尽量为每个页面访问增加一点儿时间间隔，即使你要增加一行代码：&#xA;**&#xA;time.sleep(3) (小编：3 + 随机数 是不是更好一些？) **</description>
    </item>
    <item>
      <title>Scrapy: Run Using TOR and Multiple Agents</title>
      <link>https://anwangtanmi.github.io/posts/73f9007b80dfb2c16b2436fdb7f661f8/</link>
      <pubDate>Wed, 08 Apr 2015 16:24:36 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/73f9007b80dfb2c16b2436fdb7f661f8/</guid>
      <description>http://pkmishra.github.io/blog/2013/03/18/how-to-run-scrapy-with-TOR-and-multiple-browser-agents-part-1-mac/&#xA;Scrapy is a brilliant and well documented crawler written in python. Though it is not as scalable as Apache Nutch but it can easily handle thousands of sites easily. You can get up and running very quickly using the official documentation. Tor gives you power to keep your privacy and security.Tor can hide you so that website can not track your identity. You may read more about TOR in official site.</description>
    </item>
  </channel>
</rss>
