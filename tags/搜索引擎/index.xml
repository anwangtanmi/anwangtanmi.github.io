<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>搜索引擎 on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/</link>
    <description>Recent content in 搜索引擎 on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 04 Apr 2012 23:26:46 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>搜索引擎duckduckgo</title>
      <link>https://anwangtanmi.github.io/posts/e45e53d3472c39622c7552d0c10dd2f7/</link>
      <pubDate>Wed, 04 Apr 2012 23:26:46 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/e45e53d3472c39622c7552d0c10dd2f7/</guid>
      <description>北京时间3月31日消息，DuckDuckGo搜索引擎近来发展迅猛，3个月来搜索请求以平均每天227％的速度高速增长。虽然现在它尚不能撼动Google搜索霸主地位，但谷歌亦也应该提起重视。&#xA;报道称，不少国外企业家在其个人电脑上使用的是另类的搜索引擎DuckDuckGo，而不是谷歌或者Bing。另外据ycombinator报道称，使用这个搜索引擎的人也不是一个两个。&#xA;从下面的图表中可以看出，到今年为止，DuckDuckGo每天的搜索量如同曲棍球球棍一样快速增长，平均每天搜索请求增速达227％。自去年年底到现在为止，用户数已接近150万。&#xA;这巨大的上升势头，一方面是来自去年一月推动的视觉界面设计，另一方面也是数据隐私日活动的鞭策。正是因为DuckDuckGo对数据隐私的保护，黑客甚至都集体使用它。&#xA;因此有人提出建议，谷歌应该对此感到重视。不过分析人士指出，DuckDuckGo目前对谷歌而言还不是最大的威胁，谷歌现在还没精力关注它，这对DuckDuckGo来说无异是一件好事。&#xA;就目前许多人而言，DuckDuckGo已经被定位成一个有友好隐私保护的搜索引擎。因此它不会在众多竞争中被淘汰掉，也更不用说非要和其他的搜索引擎一决高下，因为DuckDuckgo推动发展方向和战略是对的。&#xA;此外在早些访谈中，DuckDuckGo创始人Gabriel Weinberg称，目前的工作重点将是对搜索算法进行改进和加快相应速度。&#xA;据悉DuckDuckGo是一个综合性搜索引擎，它索引的搜索结果包括了Google、Bing、维基百科、亚马逊等。另外DuckDuckGo把隐私放在第一位，它不存储用户IP地址、也不记录用户信息，同时DuckDuckGo搜索结果更加实时化，Spam也更少。http://duckduckgo.com/</description>
    </item>
    <item>
      <title>搜索引擎爬虫的基本需求和考核标准</title>
      <link>https://anwangtanmi.github.io/posts/8e3283aec0189c0946f87c063f4bcce8/</link>
      <pubDate>Fri, 09 Jul 2010 09:33:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/8e3283aec0189c0946f87c063f4bcce8/</guid>
      <description>需要包含以下基本功能：&#xA;（1）网站下载流速控制&#xA;国内国外的搜索爬虫，科研机构爬虫数量很多，不同的站点抗抓取能力大相径庭，对网站的下载做好控制，避免将网站抓死。&#xA;（2）网页抓全&#xA;将互联网网页抓全，是极大的挑战，暗网暂且不提，就是明网抓全也不是容易的事情，新站发现，sitemap协议等用站长主动提交的支持等等。&#xA;（3）网页抓新（更新及时性）&#xA;网页总在不断变化中，如何当网页变化后（更新，消亡）能够及时更新，实时性和死链率等是表征这方面工作的重要指标。&#xA;（4）网页重复抓取的避免&#xA;为了及时捕捉网页的更新，对同一个网址必须经常去抓取，同样网络是一个网状结构，同一个网址可能被多次引用，这些都导致重复抓取的可能性，如果避免网页抓重，同时控制合理的更新频率，是非常关键的。&#xA;（5）DNS自动解析&#xA;如果抓取每个网页都进行一次DNS解析，那成本就太大了，维护一个DNS自动解析系统，可以大大降低域名服务器的负担，且大大提高效率。&#xA;（6）镜像站点的识别&#xA;网页内容相同，但域名不同的情况比比皆是，其中镜像站点的识别尤为关键&#xA;（7）抓取的优先级调整&#xA;抓取队列总是满的，周而复始，但在抓取的时候会出现，重要的，紧急的，不重要的，不紧急的内容，如何处理好排队的关系尤为重要，是单独开辟绿色通道，还是将其排队号前提都是需要细心打磨的。&#xA;（8）抓取深度控制&#xA;链接展开的深度控制，避免出现单个站点过分抓取，而使得其他站点持续饥饿&#xA;（9）多爬虫的协作&#xA;爬虫间的通行量要尽可能少，爬虫出现故障后的自动恢复，抓取主机的异地化等等，据说百度在国外部署的爬虫来抓取国外的站点。&#xA;（10）网页下载的存储&#xA;网页下载后的本地存储，链接提取，锚文本，链接关系的存储等等。&#xA;（11）死链、跳转的识别和处理&#xA;在抓取网页失败后，判断是死链还是当机，错误下载的网址再次抓取的时间间隔的控制，redirect的网页收集等等。&#xA;考核标准&#xA;（1）总有效的网页数（单机）&#xA;（2）新站发现数（单机）&#xA;（3）无效抓取的网页数（单机）&#xA;（4）镜像站点数（单机）&#xA;（5）全网站点的基本信息（更新周期，死链率，错误率）&#xA;（6）重要网页的抓取及时性（随机抽取盲测）&#xA;（7）抓取稳定性，故障率等</description>
    </item>
    <item>
      <title>CompletePlanet (动态数据库-搜索引擎)</title>
      <link>https://anwangtanmi.github.io/posts/25db0dad4a15282ed610550845659b39/</link>
      <pubDate>Thu, 01 Jul 2010 17:10:26 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/25db0dad4a15282ed610550845659b39/</guid>
      <description> www.completeplanet.com 隐匿查询&#xA;数据库里存储的大量的信息对标准的搜索引擎来说是不可见的，标准的搜索引擎只是索引网站上的内容，从一个链接到另一个链接。 隐匿搜索引擎专门用来搜索被称作Deep Web上的隐藏数据。&#xA;* 能查找动态数据库。 * 能在一定数据范围内查询。 * 有很好的帮助文档。 </description>
    </item>
    <item>
      <title>robots.txt</title>
      <link>https://anwangtanmi.github.io/posts/aa2646a667ee1cd83235786dccef4a26/</link>
      <pubDate>Fri, 07 Dec 2007 17:02:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/aa2646a667ee1cd83235786dccef4a26/</guid>
      <description>robots.txt是一个纯文本文件，在这个文件中网站管理者可以声明该网站中不想被robots访问的部分，或者指定搜索引擎只收录指定的内容。&#xA;当一个搜索机器人（有的叫搜索蜘蛛）访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，那么搜索机器人就沿着链接抓取。&#xA;robots.txt必须放置在一个站点的根目录下，而且文件名必须全部小写。&#xA;robots.txt写法&#xA;我们来看一个robots.txt范例：http://www.w3.org/robots.txt&#xA;访问以上具体地址，我们可以看到robots.txt的具体内容如下：&#xA;#&#xA;# robots.txt for http://www.w3.org/&#xA;#&#xA;# $Id: robots.txt,v 1.48 2007/10/16 05:31:15 gerald Exp $&#xA;#&#xA;# For use by search.w3.org&#xA;User-agent: W3C-gsa&#xA;Disallow: /Out-Of-Date&#xA;User-agent: W3T_SE&#xA;Disallow: /Out-Of-Date&#xA;User-agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT; MS Search 4.0 Robot)&#xA;Disallow: /&#xA;# W3C Link checker&#xA;User-agent: W3C-checklink&#xA;Disallow:&#xA;# exclude some access-controlled areas&#xA;User-agent: *&#xA;Disallow: /2004/ontaria/basic&#xA;Disallow: /Team&#xA;Disallow: /Project&#xA;Disallow: /Web</description>
    </item>
  </channel>
</rss>
