<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DeepLearning on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/categories/deeplearning/</link>
    <description>Recent content in DeepLearning on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 24 Sep 2019 17:55:58 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/categories/deeplearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SSD与MobileNet详解</title>
      <link>https://anwangtanmi.github.io/posts/88a0e48b264de2adf4cd3f203966f0aa/</link>
      <pubDate>Tue, 24 Sep 2019 17:55:58 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/88a0e48b264de2adf4cd3f203966f0aa/</guid>
      <description>1.概述&#xA;本文档阐述SSD检测算法原理，及以MobileNet为Backbone的网络搭建方式。&#xA;十里桃园首发，转载请注明&#xA;Why SSD？ 无论是机器学习或是深度学习一般都可分为两个部分：特征提取与分类任务。&#xA;在传统的机器学习方法中，特征提取需要依据图像以及特有的检测目的抓取特有特征，如偏重物体轮廓的HOG特征，注重明暗对比的Haar特征等，特征被描述之后送入机器学习算法分类，如SVM、Adaboost等，进而判断物体的分类。将上述的流程在图像上做滑框操作或代入已图像预处理的ROI框即完成了图像检测与识别任务。&#xA;图.1 机器学习目标检测简图&#xA;在深度学习中，特征提取需要由特有的特征提取神经网络来完成，如VGG、MobileNet，ResNet等，这些特征提取网络往往被称为Backbone。通常来讲在BackBone后面接全连接层(FC)来执行分类任务。但FC对目标的位置识别乏力。经过算法的发展，当前主要以特定的功能网络来代替FC的作用，如Mask-Rcnn、SSD、SSDlite、YOLO等。&#xA;图.2 深度神经网络特征提取+SSD分类器&#xA;本文主要阐述说明以MobileNet_v2为Backbone，以SSD为分类器来执行分类任务的具体架构。&#xA;SSD 分类框架 3.1．两种主流的深度学习目标检测分类算法&#xA;基于目标检测与识别算法分为两个类型：&#xA;1，Two-Stage方法，如R-CNN系列算法，其先产生一系列稀疏的候选框，然后对这些候选框进行分类和回归。&#xA;2，One-Stage算法，如Yolo、SSD等，其主要思路为均匀地在图片不同位置进行密集抽样，抽样时采用不同尺度和长宽比，进行分类和回归，整个过程只需一步，相应的其运行速度要远远优于Two-Stage方法&#xA;两种方法的mAP(Mean Average Precision)与运行速度如下图：&#xA;图.3 Two-Stage与One-Stage算法性能对比图&#xA;如上图，横轴为识别帧率，纵坐标为mAP。可以看出，One-Stage算法的帧率在保证mAP的前提下，普遍高于Two-Stage，更适合在嵌入式移动设备端部署。&#xA;3.2．SSD算法&#xA;和FC不同的是，SSD在多个尺度的特征图上分别执行目标检测工作。这样可以使得各个尺度的目标都能被兼顾，小尺度特征图预测大目标，大尺度特征图预测相对较小的目标。&#xA;3.2.1．感受野&#xA;感受野是指影响某个神经元的输入区域，也被称为理论感受野。&#xA;输入区域中每个像素点对输出影响的重要性不同，越靠近中心的像素点影响越大，呈高斯分布。中间一小部分区域对神经元的输出有绝对的影响，这中间一小部分被叫做有效感受野。&#xA;图.4 感受野与Default Box&#xA;如图.4，左侧整个黑色区域就是理论感受野，中间成高斯分布的白色点云区域为有效感受野。右侧图像蓝色框内对应理论感受野，绿色圆内对应有效感受野，而红色框内是Default Box大小，Default Box比理论感受野小很多，但是可以容纳大部分有效感受野内部信息。Default Box的具体含义与作用在下节给出。&#xA;3.2.2．Default Box&#xA;3.2.2.1 Default Box Acquisition&#xA;特征图是输入图像经过神经网络卷积产生的结果，表征的是神经空间内一种特征，其分辨率大小取决于先前卷积核的步长。SSD算法中共取6层不同尺度的特征图，在每层特征图的每个像素点处生产不同宽高比的框，此类的框统称为Default Box。&#xA;图.5 8*8特征图和Default Box&#xA;如上图，假定有8*8的特征图，特征图上的每个格子称为特征图小格，在每个特征图小格上面有一系列固定大小的Box，上图中每个小格上有4个Box，用虚线框标识。&#xA;泛化地来说，如果一个特征图的大小是mn，也即有mn个特征图小格。在每个小格上有k个Default Box，那么这层特征图Default Box的总数为mnk。&#xA;Default Box自身的尺度(scale)和宽高比(aspect ratio)也有特殊的规定的方式，假设一共用I个特征图做预测，对于每个特征图而言其Default Box的尺度按下面的公式计算：&#xA;尺度(scale)是相对于输入图像的分辨率，最终Default Box的实际像素尺寸需乘以输入图像的分辨率，如SDD300，则需乘以300。&#xA;3.2.3． Box regression&#xA;对于神经网络前向流动来说，需要将Prior Box与Ground Truth Box做匹配。匹配成功则说明，Prior Box所包含的是被检测的目标。但其力完整目标的Ground Truth Box还有一定的神经网络高维空间内的距离。显然地，将Prior Box的分类尽可能地通过一个高维向量分类并回归到Ground Truth Box为神经网络地最终目标之一。</description>
    </item>
    <item>
      <title>CNN – 卷积神经网络卷积计算详解</title>
      <link>https://anwangtanmi.github.io/posts/c93bf5abfcb49c51092f615ead6ec960/</link>
      <pubDate>Tue, 30 Jul 2019 15:24:21 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c93bf5abfcb49c51092f615ead6ec960/</guid>
      <description>卷积计算层：CONV Layer&#xA;人的大脑在识别图片的过程中，会由不同的皮质层处理不同方面的数据，比如：颜色、形状、光暗等，然后将不同皮质层的处理结果进行合并映射操作，得出最终的结果值，第一部分实质上是一个局部的观察结果，第二部分才是一个整体的结果合并。&#xA;基于人脑的图片识别过程，我们可以认为图像的空间联系也是局部的像素联系比较紧密，而较远的像素相关性比较弱，所以每个神经元没有必要对全局图像进行感知，只要对局部进行感知，而在更高层次对局部的信息进行综合操作得出全局信息。&#xA;卷积过程：&#xA;一个数据输入，假设为一个RGB的图片&#xA;在神经网络中，输入是一个向量，但是在卷积神经网络中，输入是一个多通道图像(比如这个例子中有3个通道，即RBG通道)&#xA;手动画一张图来理解一下吧，字丑见谅 … …&#xA;上述第3步骤， 表示 第 I 层特征图的第一个通道，后面的（1,1）表示该通道上位于第一行第一列的像素值。a,b,c分别表示卷积核的3个通道，它们的下标同样表示卷积核该通道上第n行第n列的参数值(权重值w)。&#xA;2、卷积过程实际意义理解：&#xA;对于一张图片“2”，我们来把这个’2‘分成块，一块一块看：你一块块看，是不是仍然会发现这是个数字’2‘？&#xA;也是说，你大脑会自动把这些分散的图片组合起来进行识别&#xA;也就是说，我们发现了两个现象：&#xA;如果我们只知道局部的图片，以及局部的相对位置，只要我们能将它正确组合起来，我们也可以对物体进行识别，即看做卷积核在特征图像上按步长扫描的过程，每个扫面块都是局部图片，且有局部相对位置。输出特征值也是按同样的相对位置生成在输出特征平面上。 同时局部与局部之间关联性不大，也就是局部的变化，很少影响到另外一个局部，即看做每个扫描块所有特征值经过卷积核卷积生成一个输出特征值，即局部图片通过卷积发生了变化。 实际上，我们还会遇到两个问题：&#xA;一张图片特征这么多，一个卷积层提取的特征数量有限的，提取不过来啊！&#xA;我怎么知道最后采样层选出来的特征是不是重要的呢？&#xA;我来大概介绍一下级联分类器：&#xA;大概意思就是我从一堆弱分类器里面，挑出一个最符合要求的弱分类器，用着这个弱分类器把不想要的数据剔除，保留想要的数据。&#xA;然后再从剩下的弱分类器里，再挑出一个最符合要求的弱分类器，对上一级保留的数据，把不想要的数据剔除，保留想要的数据。&#xA;最后，通过不断串联几个弱分类器，进过数据层层筛选，最后得到我们想要的数据。&#xA;那么，针对刚才的问题，我们是否也可以级联一个卷积层和池化层？&#xA;于是就有了CNN中[ ( 卷积 &amp;gt; 激活 ) * N &amp;gt; 池化? ] * M的过程。&#xA;大致上可以理解为：&#xA;通过第一个卷积层提取最初特征，输出特征图（feature map） 通过第一个采样层对最初的特征图（feature map ）进行特征选择，去除多余特征,重构新的特征图 第二个卷积层是对上一层的采样层的输出特征图（feature map）进行二次特征提取 第二个采样层也对上层输出进行二次特征选择 全连接层就是根据得到的特征进行分类 其实整个框架很好理解，我举个生动形象的例子：&#xA;输入图像好比甘蔗。&#xA;卷积层好比A君要吃甘蔗，吃完之后（卷积），他得出一系列结论，这个甘蔗真好吃，我还想再吃！&#xA;啊不是，说错了&#xA;他得出结论，这个甘蔗是圆柱形，长条，甜的，白的，多汁的等等（提取特征，提取的特征即输出特征值）&#xA;采样层就好比第一个吃甘蔗的人告诉B君，吃甘蔗，重要的是吃个开心，为什么开心，因为它又甜又多汁，还嘎嘣脆（特征选取）&#xA;第二个卷积层就好比，B君并没有去吃剩下的甘蔗，而是&#xA;头也不回。&#xA;拦也拦不住的&#xA;去吃A君吐出的甘蔗渣&#xA;然后得出结论，嗯~~，&#xA;咦~~？&#xA;哦~~！&#xA;‘原来这甘蔗渣是涩的，是苦的，不易嚼，不好咽’B君这么说道（二次提取特征，提取的特征即输出特征值）&#xA;第二个采样层就好比，B君对别人说，这个甘蔗渣啊，吃的我不开心，因为它很涩，不好咽（二次特征选取）&#xA;如果你们要吃，注意点！&#xA;注意点!</description>
    </item>
    <item>
      <title>deeplabv3&#43;二：详细代码解读 data generator 数据生成器</title>
      <link>https://anwangtanmi.github.io/posts/4310a146a50aee08ba69e6cfd56cbca3/</link>
      <pubDate>Tue, 23 Jul 2019 15:50:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4310a146a50aee08ba69e6cfd56cbca3/</guid>
      <description>3+支持三种数据库，voc2012，cityscapes，ade20k，&#xA;代码文件夹&#xA;-deeplab&#xA;-datasets&#xA;-data_generator.py&#xA;在开始之前，始终记住，网络模型的输入是非常简单的image，规格化到[-1,1]或[0,1]，或者数据扩增(水平翻转，随机裁剪，明暗变化，模糊)，以及一个实施了相同数据扩增的label（毕竟需要pixel对上），test的话只需要一个image。是非常简单的数据格式，也许程序员会为了存储的压缩量以及读取处理的速度（指的就是使用tf.example 与 tf.record）写复杂的代码，但是最终的结果始终都是很简单的。&#xA;觉得自己一定要先搞清楚tf.example 与tf.record:https://zhuanlan.zhihu.com/p/33223782&#xA;目录&#xA;数据库分析&#xA;代码重点类Dataset&#xA;1.方法_parse_function()&#xA;2. 方法_preprocess_image()&#xA;2.1 input_preprocess的preprocess_image_and_label方法介绍&#xA;3.方法 _get_all_files(self):&#xA;4.方法 get_one_shot_iterator(self)&#xA;Class TFRecordDataset&#xA;代码使用是在train.py里面：&#xA;代码：先放代码，你可以尝试自己看，看得懂就不用往下翻浪费时间了。&#xA;# Copyright 2018 The TensorFlow Authors All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.</description>
    </item>
    <item>
      <title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems（用于Web级推荐系统的图形卷积神经网络）</title>
      <link>https://anwangtanmi.github.io/posts/c63629bfc04e210195eb67afbaba6cf1/</link>
      <pubDate>Sun, 07 Jul 2019 09:52:26 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c63629bfc04e210195eb67afbaba6cf1/</guid>
      <description>Graph Convolutional Neural Networks for Web-Scale Recommender Systems 用于Web级推荐系统的图形卷积神经网络 ABSTRACT Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge.&#xA;Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.</description>
    </item>
    <item>
      <title>Win/Mac系统下，软件使用tor的多层socks5代理网络</title>
      <link>https://anwangtanmi.github.io/posts/f982b0a1f35696601682941d12690a4e/</link>
      <pubDate>Wed, 29 May 2019 13:44:56 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f982b0a1f35696601682941d12690a4e/</guid>
      <description>不论win还是MAC，首先要打开tor洋葱头浏览器，&#xA;至于怎么安装运行起tor我就不多说了，使用代理或者vpn都可以。&#xA;tor启动后，默认配置了另一个9150端口的Socks5服务，其他软件接入这个端口就可以使用tor的网络：&#xA;win系统下，使用SocksCap64，先把tor的代理添加进去：&#xA;然后把要使用tor代理的程序拉进SocksCap64工具里：&#xA;点击SocksCap64工具里的程序图标启动程序，这样程序就会自动使用tor的代理网络，&#xA;tor断线的时候，程序会断线，&#xA;tor不须用特殊设置，软件运行中IP会不定时每隔几分钟自动更换。&#xA;MAC系统下，使用Proxifier，可能很多人都用过。&#xA;也是先把tor的代理添加进去：&#xA;添加代理规则：&#xA;测试效果：</description>
    </item>
    <item>
      <title>吴恩达卷积神经网络第一周笔记</title>
      <link>https://anwangtanmi.github.io/posts/5328dc25f857620dabcc0d18a5698b41/</link>
      <pubDate>Fri, 24 May 2019 15:54:02 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/5328dc25f857620dabcc0d18a5698b41/</guid>
      <description>第一周 卷积神经网络 1.1 计算机视觉 计算机视觉问题：图片分类、物体检测、神经风格转换&#xA;图片像素过多，用传统神经网络计算量过大&#xA;1.2 边缘检测 垂直检测 6×6 矩阵和 3×3 卷积核进行卷积运算，得到 4*4 矩阵&#xA;下图卷积核是垂直检测所用卷积核&#xA;通过该卷积核的卷积运算得到的结果能够成功检测出垂直边缘&#xA;1.3 更多边缘检测内容 左明右暗中间白（正），左暗右明中间黑（负）&#xA;上明下暗中间白（正），上暗下明中间黑（负）&#xA;还有一切其他的可以用来进行边缘检测的卷积核，大都是人为设计出来的效果比较好的卷积核&#xA;除此之外，还可以让机器自己学习卷积核的参数设置，通过不断训练迭代取得更贴合问题的卷积核参数&#xA;1.4 Padding 6×6 的矩阵和 3×3 的卷积核做卷积得到 4×4 的矩阵 n×n * f×f ——&amp;gt; (n-f+1)×(n-f+1)&#xA;规模变小的缺陷：&#xA;多次卷积运算之后图片可能就变成 1×1 缺失很多信息 边缘像素点和中心像素点运算的次数（权重）不同，容易失去边缘信息 如果不希望矩阵的规模发生改变，就需要对原来的矩阵进行 padding&#xA;p 是边缘填充像素点的个数，因此卷积之后的矩阵变成 (n+2p)×(n+2p) * f×f ——&amp;gt; (n+2p-f+1)×(n+2p-f)&#xA;Padding 方法 Valid：不填充 n×n * f×f ——&amp;gt; (n-f+1)×(n-f+1) Same：填充运算后得到相同大小矩阵 (n+2p)×(n+2p) * f×f ——&amp;gt; (n+2p-f+1)×(n+2p-f) p&#xA;=&#xA;f&#xA;−&#xA;1</description>
    </item>
    <item>
      <title>Python PIL库处理图片常用操作，图像识别数据增强的方法</title>
      <link>https://anwangtanmi.github.io/posts/abce8af11f1416d23f06936d3112b470/</link>
      <pubDate>Sat, 11 May 2019 13:58:30 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/abce8af11f1416d23f06936d3112b470/</guid>
      <description>在博客AlexNet原理及tensorflow实现训练神经网络的时候，做了数据增强，对图片的处理采用的是PIL(Python Image Library), PIL是Python常用的图像处理库.&#xA;下面对PIL中常用到的操作进行整理：&#xA;1. 改变图片的大小&#xA;from PIL import Image, ImageFont, ImageDraw def image_resize(image, save, size=(100, 100)): &#34;&#34;&#34; :param image: 原图片 :param save: 保存地址 :param size: 大小 :return: &#34;&#34;&#34; image = Image.open(image) # 读取图片 image.convert(&#34;RGB&#34;) re_sized = image.resize(size, Image.BILINEAR) # 双线性法 re_sized.save(save) # 保存图片 return re_sized 2. 对图片进行旋转：&#xA;from PIL import Image, ImageFont, ImageDraw import matplotlib.pyplot as plt def image_rotate(image_path, save_path, angle): &#34;&#34;&#34; 对图像进行一定角度的旋转 :param image_path: 图像路径 :param save_path: 保存路径 :param angle: 旋转角度 :return: &#34;</description>
    </item>
    <item>
      <title>使用darknet识别点选验证码详细过程（附带源码）</title>
      <link>https://anwangtanmi.github.io/posts/fa5d6293d855226797c6e18b8bce47cb/</link>
      <pubDate>Thu, 21 Mar 2019 23:06:33 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/fa5d6293d855226797c6e18b8bce47cb/</guid>
      <description>项目源码：https://github.com/nickliqian/darknet_captcha&#xA;darknet_captcha 项目基于darknet开发了一系列的快速启动脚本，旨在让图像识别新手或者开发人员能够快速的启动一个目标检测（定位）的项目。&#xA;如果有没有讲清楚的地方，欢迎提issue和PR，希望能和大家共同完善！&#xA;本项目分为两个部分：&#xA;提供两个目标检测（单分类和多分类点选验证码）的例子，你可以通过例子熟悉定位yolo3定位网络的使用方式 基于darknet提供一系列API，用于使用自己的数据进行目标检测模型的训练，并提供web server的代码&#xA;目录 项目结构 开始一个例子：单类型目标检测 第二个例子：多类型目标检测 训练自己的数据 Web服务 API文档 其他问题 使用阿里云OSS加速下载 GPU云推荐 CPU和GPU识别速度对比 报错解决办法 TODO 项目结构 项目分为darknet、extent、app三部分&#xA;darknet: 这部分是darknet项目源码，没有作任何改动。 extent: 扩展部分，包含生成配置、生成样本、训练、识别demo、api程序。 app: 每一个新的识别需求都以app区分，其中包含配置文件、样本和标签文件等。 开始一个例子：单类型目标检测 以点选验证码为例&#xA;darknet实际上给我们提供了一系列的深度学习算法，我们要做的就是使用比较简单的步骤来调用darknet训练我们的识别模型。&#xA;推荐使用的操作系统是ubuntu，遇到的坑会少很多。 如果使用windowns系统，需要先安装cygwin，便于编译darknet。（参考我的博客：安装cygwin） 下面的步骤都已经通过ubuntu16.04测试。&#xA;1.下载项目 git clone https://github.com/nickliqian/darknet_captcha.git 2.编译darknet 进入darknet_captcha目录，下载darknet项目，覆盖darknet目录：&#xA;cd darknet_captcha git clone https://github.com/pjreddie/darknet.git 进入darknet目录，修改darknet/Makefile配置文件&#xA;cd darknet vim Makefile 如果使用GPU训练则下面的GPU=1 使用CPU训练则下面的GPU=0 GPU=1 CUDNN=0 OPENCV=0 OPENMP=0 DEBUG=0 然后使用make编译darknet：&#xA;make 不建议使用CPU进行训练，因为使用CPU不管是训练还是预测，耗时都非常久。&#xA;如果你需要租用临时且价格低的GPU主机进行测试，后面介绍了一些推荐的GPU云服务。&#xA;如果在编译过程中会出错，可以在darknet的issue找一下解决办法，也可以发邮件找我要旧版本的darknet。&#xA;3.安装python3环境 使用pip执行下面的语句，并确保你的系统上已经安装了tk：&#xA;pip install -r requirement.txt sudo apt-get install python3-tk 4.</description>
    </item>
    <item>
      <title>SILVACO 使用中遇到的问题</title>
      <link>https://anwangtanmi.github.io/posts/718020b71033928a5eae9655e85b194d/</link>
      <pubDate>Thu, 21 Mar 2019 09:46:10 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/718020b71033928a5eae9655e85b194d/</guid>
      <description>bias step cut back more than 4 times. Cannot trap 这个是因为计算结果太粗糙而导致不收敛，使用trap参数可定义计算的折半次数。如果计算开始时候数值梯度太大，无法满足计算精度，电极的偏置步长将从最初值减小到原来的一半重新计算，如果结果还是太粗糙的话有折半计算知道满足精度要求。默认trap 次数是4次，如果初始值折半4次还很粗糙，计算将停止，并在实时输出窗口中显示不收敛的报错信息：max trap more than 4</description>
    </item>
    <item>
      <title>基于深度学习的语义匹配若干模型DSSM，ESIM, BIMPM, ABCNN</title>
      <link>https://anwangtanmi.github.io/posts/08083d9c037c8c46884de2becffa28fc/</link>
      <pubDate>Wed, 13 Mar 2019 17:19:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/08083d9c037c8c46884de2becffa28fc/</guid>
      <description>本项目介绍了语义匹配的几个模型，分别为：&#xA;DSSM&#xA;Learning Deep Structured Semantic Models for Web Search using Clickthrough Data&#xA;ESIM&#xA;Enhanced LSTM for Natural Language Inference&#xA;Pair-CNN&#xA;Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks&#xA;ABCNN&#xA;ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs&#xA;BIMPM&#xA;bilateral multi-perspective matching for natural language sentences&#xA;顺带介绍料WMD的相关知识&#xA;论文集在papers文件夹中 github:https://github.com/pengming617/text_matching&#xA;纯属个人学习 如有侵权 实在抱歉 请联系我&#xA;上文中介绍的Bimpm代码复制于 https://zhuanlan.zhihu.com/p/50184415，特此感谢 简枫 同学</description>
    </item>
    <item>
      <title>【社工&amp;信息捕获】有个姑娘网恋奔现发现自己被人卖了，跟她见面的是另一个人</title>
      <link>https://anwangtanmi.github.io/posts/59ca98bccac874b4d7406735c9f326ec/</link>
      <pubDate>Fri, 08 Mar 2019 09:56:57 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/59ca98bccac874b4d7406735c9f326ec/</guid>
      <description>大家好，我是凌云&#xA;十多天前，我接到一条线索，原本以为是一件很普通的事，当我追查的时候发现和暗网有牵连，自己不但暴露了还被对方反查到真实身份。&#xA;这件事得从今年的2月10日说起，那天我和平常一样登公众号后台看私信内容。&#xA;有个叫李琴的姑娘连续几天都在给我发信息——在网上谈了一男朋友，聊了一个多月他来见她，发现不是同一个人，还差点被下药。&#xA;我跟李琴说修图太过分也很正常啊，怎么不是一个人了？&#xA;她说不是，刚见面的时候他说话带着一股小沈阳的味道，在网上他给我发语音普通话很正，没带口音的。&#xA;我当时就留了个心眼，以前他跟我说过对西瓜过敏的，我那天故意给他点了杯西瓜汁，他喝了啥事都没有。&#xA;然后他就跟我说让我去酒店陪他放行李，当时我就不敢跟他去了，我跟他说我去个厕所先，然后给我闺蜜发了条信息告诉她现在的情况，在回来的时候我看到他拿着一瓶东西倒我杯子里。&#xA;幸好当时我闺蜜打电话问我啥情况，我就借着这个机会说我还有个朋友要来，我出去门口接她一下，就偷偷跑了。&#xA;我琢磨了一会之后，给李琴留了个微信，让她微信上详细聊。&#xA;很快她加上了我，我问了李琴一些能证明身份的话，以及让她整理一份事件经过、把对方的所有信息发到我邮箱上&#xA;那天晚上，李琴和我说：“他向我坦白了，说把我的信息卖给了另一个男的。”&#xA;听到这，心里一阵无语，这种东西也能卖的吗？&#xA;我的打算是从卖李琴信息的人开始调查，如果有机会的话还要揪出跟她见面的人。&#xA;只要对方有把柄落我手上，自然就能问到我需要的信息。&#xA;而李琴网恋对象的信息，我觉得参杂了很多水分，毕竟刚开始就没打算跟她在一起的，自然给的是假信息，所以我现在要核验哪些信息是真的，哪些是假的。&#xA;0x0x 核验信息 我托在通讯公司上班的朋友查他给李琴的手机号，但我朋友说：“这个手机机主姓名叫陈杰”&#xA;而跟李琴网恋的人叫黄文民，根本不符合，所以他给李琴的姓名是假的。&#xA;后来当我尝试追查微信的时候发现，该微信是从16年就已经注册了&#xA;从一些蜘丝马迹来看，这个号码原来主人应该是中年人使用的&#xA;不然历史头像不会用一盆花作为头像，所以能推断微信号是从号贩子那买来的。&#xA;但很奇怪的是——我追查他留给李琴的QQ号的时候，发现这是一个常用号码，QQ等级和使用时间都是很正常的，并不像他买的微信号那样透露着不对劲。&#xA;难道他会给李琴自己私人的QQ吗？立马去问她这QQ号码是怎么来的&#xA;李琴说是在快见面之前的时候，当时让他陪我打王者，因为我玩的是QQ区的，他玩的是微信区的，不同区的没法一起玩，得加QQ好友才行，然后他一直让我等等，我生气之后他马上给了我一个QQ。&#xA;王者荣耀一共有两大分区，分别是QQ和微信，而它们是不互通的。&#xA;我问李琴：“那他的王者是小号吗”&#xA;不是，他QQ区都打上钻石了&#xA;琢磨了一会之后得出一个大胆的猜测——当时李琴催的急，没时间买新的QQ或者是新号打王者会有新手指引，我就特烦这个指引，站在我的角度去思考，我认为，这个QQ就是他的私人号码&#xA;大胆猜测，小心求证，我去QQ安全中心选择忘记密码，通过密保手机验证看到的这个QQ绑定的手机号是152开头的，而他给李琴的手机号是130开头的，这个QQ还真有可能就是他的私人号码&#xA;综合以上的简单分析，我决定把调查方向转到他QQ上&#xA;0x02猜解手机号 用这个QQ号的邮箱去查了下注册了哪些帐号，我想尝试去猜解出绑定的手机号剩下的数字&#xA;搜索支付宝帐号，有个实名信息叫刘*俊的帐号，很幸运的是这个帐号绑定了手机，在忘记密码的时候选择手机号和银行卡验证拿到了最后两位数的号码。&#xA;想猜解手机号，必须要了解手机号的格式（前三位+四位归属地+随机位）&#xA;我现在还不知道他到底是哪个地区的，他跟李琴说是湖北人，我肯定是不信的，还得自己查。&#xA;检索他这个QQ号在网上遗留的痕迹时，我发现了他的贴吧帐号，没发过帖子，头像也是默认的，但在他关注里看到关注了亳州这个贴吧。&#xA;经验告诉我，他百分之九十五是安徽毫州人，那么现在能确认地区了，剩下就要猜解出剩下的五位手机号了。&#xA;手机号的结构是前三位+四位归属地+随机位，我在网上找了一个全国号码生成器查找安徽移动所有的开头为152的号码，一共有400个&#xA;然后把数据导入到通讯录里一个个筛选QQ的通讯录好友，和头像符合的QQ。&#xA;花了挺长一段时间，终于找到了和他QQ头像昵称符合的了。&#xA;第一轮调查后我知道了他的手机号，姓名和地区，再次整理并且筛选假信息，得到下图。&#xA;目前知道了他叫刘X俊，152的手机号和35开头的QQ都是他的私人号码。&#xA;0x03钓鱼攻击 琢磨了一会之后，着手搭建了一个钓鱼网站，我的计划是利用钓鱼网站去尝试拿到他的QQ号，希望能在他QQ里捕捉到买家的一些线索。&#xA;钓鱼网站搭建完之后去某个短信验证平台上给他手机发了一条106开头的短信，说他的QQ被人多次申诉，现在已经限制部分功能，要想解除得登录下面的链接。&#xA;点开短信的链接会跳转到我搭建的虚假页面中，提示他要登录QQ才可以操作。当他输入QQ帐号和密码之后会提示密码错误，其实这个设置还真有点贱，哈哈哈。&#xA;很快在钓鱼网站后台里看到了他的QQ帐号和密码&#xA;但是在登录他QQ的时候发现有设备锁，用以前的漏洞无法绕过，这时陷入了困境，点了一根烟开始思考该用哪个方法去绕过设备锁。&#xA;0x04入侵邮箱 很明显我当时钻了牛角尖，一拍大腿，我可以登录QQ邮箱啊，为了避免再出现异地登录的情况，这次我用代理了安徽的IP，这次很顺畅进去了他的邮箱。&#xA;我在他邮箱里看到最近三个月频繁给一个名：kon***@sina.com的新浪邮箱发邮件，而每封邮件里都有一个附件，打开附件的Word文档全是某个女生的个人信息，包括照片，姓名，身份，毕业学校，兴趣爱好，父母家人等非常详细的信息。&#xA;我还翻到了他出售李琴个人信息的邮件，标题非常龌蹉！&#xA;随后我尝试撞库攻击他的其他社交帐号，再次追查他手机号绑定了哪些社交帐号&#xA;我查到他在18年注册了OPPO的帐号，用刚刚钓鱼来的QQ密码试着登陆一下，还真是同一个密码！&#xA;突然想起OPPO手机有个查找手机功能，这是手机丢失后提供给用户找回手机功能的。&#xA;而我利用这个功能知道了他家的大概区域&#xA;安徽毫州拓佳欢乐广场附近&#xA;我并没有停止撞库攻击，尝试登陆他的12306帐号，嚯！没想到这厮连那么重要的网站都用同一个密码，赶紧点开个人信息查看。&#xA;那么目前已经知道他的真实身份，跟李琴网恋并且倒卖她信息的人叫刘*俊，26岁，住在安徽亳州某个广场附近，年龄在23岁&#xA;现在买家的线索也浮出了水面，我打算顺着刘X俊这条线索调查跟李琴见面的人是谁。&#xA;0x05 买家浮出水面 我保留了他贩卖信息的证据后，选择跟他聊一聊，换了张不记名的电话卡之后，给他打了个电话，但一直不肯接，要么就是直接挂断了，打了五六个我给他编辑了条短信。&#xA;我跟他说已经掌握了他犯罪的证据，问几个问题，老实说我就放过你，不然就送你进去。&#xA;我问他这些信息都是从哪来的那么详细，他说：“都是跟她们网恋慢慢钓出来的”。&#xA;我顺着问：“你把这些信息卖给了谁，你跟他从哪里认识的？“&#xA;暗网的一个论坛上，他当时发了一个帖子，收网恋对象的信息。</description>
    </item>
    <item>
      <title>低照度图像增强之卷积神经网络RetinexNet</title>
      <link>https://anwangtanmi.github.io/posts/b626af0a0393720309286479e0d11fce/</link>
      <pubDate>Thu, 07 Mar 2019 11:14:57 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b626af0a0393720309286479e0d11fce/</guid>
      <description>转载请标注：https://blog.csdn.net/weixin_38285131&#xA;目录&#xA;一丶Retinex理论——图像分解&#xA;二丶RetinexNet卷积神经网络&#xA;1.训练数据说明&#xA;2.分解网络——Decom&#xA;3.增强网络——Relight&#xA;4.结果展示&#xA;低照度图像增强一直是计算机视觉领域的一个热门研究方向，之前传统的基于Retinex理论的研究方法已经出现很多，比如：MSR,MSRCR,MSRCP等，这些方法在低照度图像增强方面效果有明显提升，上一篇博客主要介绍了基于Retinex理论的集中图像增强方法，并给出了python代码。博客链接如下： [图像增强Retinex算法之python实现——MSR,MSRCR,MSRCP,autoMSRCR](https://blog.csdn.net/weixin_38285131/article/details/88097771)&#xA;但是基于传统的图像图像处理方法处理每一张图像会比较耗时，一副4000*8000的图像需要耗时十几分钟，这样就没法批量处理低照度图像，因此本文介绍一种基于Retinex理论的卷积神经网络模型——RetinexNet,该模型是北大的童鞋在2018年发表在BMVC上的，论文名字是——Deep Retinex Decomposition for Low-Light Enhancement&#xA;[论文PDF]&#xA;RetinexNet项目介绍，数据集，PPT等&#xA;一丶Retinex理论——图像分解 这个理论在上一篇博客中已有介绍，任何一幅图像可以分解为光照图像（illumination）和反射图像（reflectance），反射图像是物体的本身性质决定的即为不变的部分，光照图则受外界影响比较到，可以去除光照影响或者对光照图像进行校正，则可以达到增强图像的目的。如下图所示：&#xA;左边即为低照度图像，中间即为光照图，右边即为反射图像，原图S（x,y）=I(x,y)*R(x,y),将图像变换到log域则变成了相减，然后将光照图像减去即可达到增强图像的目的。&#xA;二丶RetinexNet卷积神经网络 摘抄自论文摘要：&#xA;Retinex理论是一种有效的微光图像增强工具。假设观测图像可以分解为反射图像和照度图像。大多数现有的基于Retinex卢纶的方法都为这种高度不适定的分解精心设计了手工约束和参数，当应用于各种场景时，这些约束和参数可能会受到模型容量的限制。Retinex包括一个用于分解的分解网络（Decom）和一个用于照明调节（Relight）的增强网络。在分解网络的训练过程中，没有分解反射和光照的地面真值。该网络的学习只有关键的约束条件，包括低/正常光图像共享的一致反射率，以及光照的平滑度。在分解的基础上，利用增强网络增强网对光照进行后续的亮度增强，联合去噪对反射率进行去噪操作。视网膜网是端到端可训练的，学习分解的性质有利于亮度的调节。&#xA;理论整体路线即为下图所示：&#xA;输入:低照度图像&#xA;分解网络（Decom）：对图像进行分解&#xA;增强网络（Relight）：对图像进行增强和调整&#xA;输出：校正之后的图像&#xA;1.训练数据说明 数据主要是利用单反相机不同的光圈值和感光度拍摄同一个场景，作为图像对进行训练，也就是说同一场景拍摄一个低照度图像，然后在拍摄一个正常图像进行训练，具体数据如下图所示：&#xA;作图为正常拍摄图像，右图为低照度图像，大概拍摄了500张图像对作为训练数据&#xA;2.分解网络——Decom 文中的分解网络应该是一个五层的卷积神经网络，代码中是利用relu函数进行激活，没有什么特别的处理，具体结构如下：&#xA;可以看到将图像对中的低照度图像和正常图像作为输入数据送入卷积神经网络进行分解，最后得到光照图像和反射图像，根据Retinex理论反射图像基本接近，但是两者光照图像相差很大，这样把每一张训练图像进行分解，然后再送入后面的增强网络进行训练。&#xA;3.增强网络——Relight 增强网络，我看代码应该是一个九层的卷积神经网络，利用relu进行激活，中间还进行最邻近差值的resize操作，具体如下图：&#xA;对刚才分解的反射图像进行去噪音操作，他中间用了平滑，但是我觉得用了平滑之后降低了图像本身的锐度，图像变得模糊，个人觉得这一步还应该做一下增强处理，回头看看能不能修改一下这个中间处理操作。应该可以用拉普拉斯进行一下图像恢复吧，我觉得这一步降低图像锐度不太好，重建图像稍显模糊。&#xA;4.结果展示 中间即为校正之后的，右边为正常光照图像，虽然和正常图像比不了，但是已经很不错了，&#xA;最后与其他算法做了一些对比工作，感觉各有千秋吧，不过都有一定的亮度提升，还是很值得研究一哈的 做一下改进。&#xA;下一步，我准备用这个神经网络训练一下自己的数据，然后测试一下增强效果。&#xA;数据集百度网盘地址&#xA;github地址&#xA;转载请注明地址：https://blog.csdn.net/weixin_38285131</description>
    </item>
    <item>
      <title>(1) [Nature15] Deep Learning</title>
      <link>https://anwangtanmi.github.io/posts/32ff0f1814c8df34a072cc972cdece57/</link>
      <pubDate>Sat, 23 Feb 2019 23:18:56 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/32ff0f1814c8df34a072cc972cdece57/</guid>
      <description>计划完成深度学习入门的126篇论文第一篇，摘自Yann LeCun和Youshua Bengio以及Geoffrey Hinton三人合著发表在nature2015的论文，同时也算是DeepLearning这本书的序文。 摘要Abstract 深度学习是使用multiple processing layers即多层网络来学习数据的内涵表示。这些方法极大程度上提高了state-of-art在语音识别speech recognition, 图像识别visual object recognition, 目标检测object detection，以及药物发现drug discovery和基因学genomics。通过使用反向传播算法backpropagation algorithm来改变内部参数internal parameters。而深度网络DCNN带来在图像、音频、语音方面的重大突破，特别是RNN在连续型数据像文本和语音有重大突破。&#xA;Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer.</description>
    </item>
    <item>
      <title>DeepFake——学习资料</title>
      <link>https://anwangtanmi.github.io/posts/4cc43f1c70a9d7ad04938d7f3b060eb3/</link>
      <pubDate>Tue, 22 Jan 2019 13:55:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4cc43f1c70a9d7ad04938d7f3b060eb3/</guid>
      <description>文章目录 教你用200行Python代码“换脸” dlib 的68个关键点 传统算法和深度学习的结合和实践，解读与优化 deepfake 深度解密换脸应用Deepfake Exploring DeepFakes 玩深度学习选哪块英伟达 GPU？ CPU、GPU、CUDA，CuDNN 简介 Deepfake技术教学，现场教你用深度学习换一张脸 Floyd学习 nvidia driver、CUDN、CUDNN homebrew配置 Fake APP 1、An Introduction to DeepFakes 3、How To Install FakeApp 4、A Practical Tutorial for FakeApp ——FakeApp的实用教程。 5、An Introduction to Neural Networks and Autoencoders 6、Understanding the Technology Behind DeepFakes 阿里云基础知识学习 教你用200行Python代码“换脸” 介绍&#xA;本文将介绍如何编写一个只有200行的Python脚本，为两张肖像照上人物的“换脸”。&#xA;这个过程可分为四步：&#xA;检测面部标记。 旋转、缩放和转换第二张图像，使之与第一张图像相适应。 调整第二张图像的色彩平衡，使之与第一个相匹配。 把第二张图像的特性混合在第一张图像中。&#xA;完整的源代码可以从这里下载: https://github.com/matthewearl/faceswap/blob/master/faceswap.py&#xA;1．使用dlib提取面部标记&#xA;该脚本使用dlib的Python绑定来提取面部标记： 用Dlib实现了论文One Millisecond Face Alignment with an Ensemble of Regression Trees中的算法（http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf，作者为Vahid Kazemi 和Josephine Sullivan） 。算法本身非常复杂，但dlib接口使用起来非常简单：</description>
    </item>
    <item>
      <title>机器学习GPU环境搭建&#43;惠普暗影精灵&#43;win10&#43;GTX1060&#43;VS2015&#43;CUDA8.0.61&#43;xgboost GPU</title>
      <link>https://anwangtanmi.github.io/posts/f697775948ce6fa9920f6e5176dbc1d0/</link>
      <pubDate>Mon, 21 Jan 2019 23:41:07 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f697775948ce6fa9920f6e5176dbc1d0/</guid>
      <description>Win10+GTX 1060显卡安装CUDA8.0 1. 下载visual studio 2015并安装 Visual Studio Professional 2015简体中文版（专业版）下载地址：&#xA;http://download.microsoft.com/download/B/8/9/B898E46E-CBAE-4045-A8E2-2D33DD36F3C4/vs2015.pro_chs.iso&#xA;SHA1: 629E7154E2695F08A3C692C0B3F6CE19DF6D3A72&#xA;激活密钥：HMGNV-WCYXV-X7G9W-YCX63-B98R2&#xA;（引用博客：https://www.cnblogs.com/bwlluck/p/5514424.html） 安装注意：&#xA;VS2015 一定注意安装Win10 SDK，如下图：&#xA;（引用博客：https://blog.csdn.net/u012348774/article/details/78711777?locationNum=2&amp;amp;fps=1） 2. 下载CUDA8.0.61并安装 下载CUDA8.0.61&#xA;下载地址1：https://developer.nvidia.com/cuda-80-ga2-download-archive&#xA;（官网地址，下载到99%左右时，易连接失败，我放弃了官网下载）&#xA;下载地址2：百度网盘链接：https://pan.baidu.com/s/1JKJ6m9htjGZcc39NZDqV1A&#xA;密码：d44h （引用博客：https://blog.csdn.net/congcong7267/article/details/80634858）&#xA;安装NVIDIA图形显卡驱动时，出现黑屏很长时间（可达1h），解决办法是：&#xA;卸载已安装的NVIDIA图形显卡驱动，如下图中的该驱动，然后再运行安装CUDA8.0.61就可以了&#xA;安装cuda8.0，去勾选visual studio integration，否则，容易失败，如下图所示：&#xA;验证cuda8.0是否安装成功 : 进入cmd，输入 nvcc -V 命令，如下图所示：验证cuda8.0是否安装成功 : 进入cmd，输入 nvcc -V 命令，如下图所示：&#xA;Win10+GTX 1060显卡安装XGBoost 引用博客：https://blog.csdn.net/m0_37327467/article/details/81324690&#xA;这篇博客已经写的比较清楚了，但是安装过程中仍有点问题，因此将自己的完整过程写下来以备忘。&#xA;1. 下载xgboost源码 下载地址：https://github.com/dmlc/xgboost&#xA;2. 下载xgboost.dll文件 下载地址：http://ssl.picnet.com.au/xgboost/&#xA;支持GPU版已编译好的DLL文件，并放在第一步下载好的文件xgboost-master/python-package/xgboost目录下。&#xA;3. 安装xgboost 以管理员身份打开git bash（第一次安装未用管理员身份，则安装失败），并进入上一步的xgboost-master/python-package/目录，然后在git bash命令行窗口运行如下命令：python setup.py install&#xA;如下图所示：&#xA;4. 检查xgboost是否安装成功 在git bash中，进入\xgboost-master\tests\benchmark目录下，执行 python benchmark_tree.py –tree_method gpu_hist命令；</description>
    </item>
    <item>
      <title>【实践】广告ctr模型之Deep cross network (dcn)</title>
      <link>https://anwangtanmi.github.io/posts/3a779e343bf2d77be5173a4e2ac6126d/</link>
      <pubDate>Fri, 07 Dec 2018 11:44:28 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3a779e343bf2d77be5173a4e2ac6126d/</guid>
      <description>广告ctr模型可用的深度模型其本质架构都一样（可见https://blog.csdn.net/dengxing1234/article/details/79916532），这也是限制了模型的发展路线。Deep cross network在广告ctr模型也是应用很常见，它聚焦于解决特征工程的问题，对比paper中提到：【DNN可以自动地学习特征地交互作用，然而，它们隐式地的生成所有的特征交互，这对于学习所有类型的交叉特征不一定有效。于是提出了一种能够保持深度神经网络良好收益的深度交叉网络（DCN），除此之外，它还引入了一个新的交叉网络，更有效地学习在一定限度下的特征相互作用，更有甚，DCN在每一层确切地应用交叉特征而不需要人工特征工程，这相比于DNN模型增加地额外地复杂度可以忽略不计】。自己按照paper和高级的tensorflow api，实现l一版dcn，源码文件3个都已共享。希望有问题各位同行人能指出交流。&#xA;原版paper：https://arxiv.org/abs/1708.05123&#xA;my_core.py from tensorflow.python.framework import tensor_shape from tensorflow.python.layers import base from tensorflow.python.ops import init_ops from tensorflow.python.ops import standard_ops from tensorflow.python.framework import ops class CrossLayer(base.Layer): def __init__(self, use_bias=True, kernel_initializer=None, bias_initializer=init_ops.zeros_initializer(), kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, trainable=True, name=None, **kwargs): super(CrossLayer, self).__init__(trainable=trainable, name=name, activity_regularizer=activity_regularizer, **kwargs) self.use_bias = use_bias self.kernel_initializer = kernel_initializer self.bias_initializer = bias_initializer self.kernel_regularizer = kernel_regularizer self.bias_regularizer = bias_regularizer self.kernel_constraint = kernel_constraint self.bias_constraint = bias_constraint self.input_spec = base.</description>
    </item>
    <item>
      <title>深度学习AI美颜系列—人像审美</title>
      <link>https://anwangtanmi.github.io/posts/d884c7265b302e5cb66b9593cdbe4529/</link>
      <pubDate>Thu, 18 Oct 2018 14:18:08 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d884c7265b302e5cb66b9593cdbe4529/</guid>
      <description>美颜是个常见的话题，暗藏了一个基础性的问题，什么是美，也就是人像之审美。&#xA;中国人审美的标准：&#xA;1，面部轮廓——“三庭五眼”；&#xA;2，人脸正中垂直轴上——“四高三低”；&#xA;3，人脸正中水平轴上——“丰”；&#xA;4，五官精致&#xA;符合上述三个条件，即中国人眼中的美貌了。&#xA;下面我们来具体分析：&#xA;①三庭五眼&#xA;世界各国普遍认为“瓜子脸、鹅蛋脸”是最美的脸形，从标准脸形的美学标准来看，面部长度与宽度的比例为1.618∶1，也就是黄金分割比例。&#xA;我们中国人所谓的三庭五眼，是人的脸长与脸宽的一般标准比例，从额头顶端发际线到眉毛、从眉毛到鼻子、从鼻子到下巴，各占1/3，这就是“三庭”；脸的宽度以眼睛的宽度为测量标准，分成5个等份，这就是“五眼”，具体如下图所示：&#xA;②四高三低&#xA;所谓“四高”是指：&#xA;第一高点，额部；&#xA;第二个高点，鼻尖；&#xA;第三高点，唇珠；&#xA;第四高点，下巴尖；&#xA;所谓“三低”是指：&#xA;第一低，两个眼睛之间，鼻额交界处必须是凹陷的；&#xA;第二低，在唇珠的上方，人中沟是凹陷的，美女的人中沟都很深，人中脊明显；&#xA;第三低，在下嘴唇唇的下方，有一个小小的凹陷；&#xA;“四高三低”在头侧面相上最明确，如下图所示：&#xA;③“丰”字审美&#xA;“丰”是指人脸正中横轴上符合“丰”字审美准则。在人的面部上画上一个“丰”字，来判断美丑。先作面部的中轴线，再通过太阳穴(颞部)作一条水平线，通过两侧颧骨最高点作一条平行线，再通过口角到下颌角作一条平行线。形成一个“丰”字。在“丰”字的三横上面，颞部不能太凹陷，也不能太突起；如下图所示：&#xA;④五官精致&#xA;五官精致包含一下几个方面：&#xA;1、眼睛美：&#xA;双眼对称，眼窝深浅适中&#xA;2、鼻子美：&#xA;鼻根与双眼皮位置等高&#xA;3、耳廓美：&#xA;双耳对称，大小及形态相同&#xA;4、口唇美：&#xA;上唇下1/3部微向前翘&#xA;5、牙齿美：&#xA;静止状态时上前牙覆盖下前牙形1/3；正中上前牙与面形相同，牙齿整齐，洁白；微笑时露出双侧尖牙；&#xA;上面这些就是我们东方人所认为的貌美审美观，也是本人根据网络资源所做的一些总结。当然这只是一种大众认可，而并非所有人。对于真正的美女，不单单是这些内容，还要有形体与身材的审美等。&#xA;本文这里给大家普及的内容，主要是作为美颜算法研究的知识储备，所谓知己知彼，只有我们更了解什么是美，那么，我们的美颜算法才能更上一层楼！</description>
    </item>
    <item>
      <title>win10 安装 GPU&#43;Tensorflow</title>
      <link>https://anwangtanmi.github.io/posts/6fcb8942ca091eaefbb1dcc1590e4fc2/</link>
      <pubDate>Tue, 16 Oct 2018 12:37:38 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/6fcb8942ca091eaefbb1dcc1590e4fc2/</guid>
      <description>安装环境： 惠普暗影精灵4 1050Ti Win10&#xA;cpu的安装极其容易，pip或者conda均可&#xA;gpu的安装：&#xA;pip安装极其缓慢。采用清华大学的镜像文件，倒是光速下载，但是在引入包的时候遇到ddl缺失的问题，在tensorflow官网上没有找到合适的解决办法，但是问题被列出来了。。详见https://github.com/tensorflow/tensorflow/issues/22794。。&#xA;决定改用conda安装。基本思想参考了这位大佬https://www.pugetsystems.com/labs/hpc/The-Best-Way-to-Install-TensorFlow-with-GPU-Support-on-Windows-10-Without-Installing-CUDA-1187/。。conda安装不需要单独安装cuda和cudnn。甚至于这篇博文都有些落伍了。现在几乎一条指令完事。&#xA;conda install tensorflow-gpu 会根据你的python版本自动下载相应的tensorflow，cuda和cudnn。&#xA;cuda和cudnn无需手动安装，无疑是太方便了。基本原理是通过conda创造了一个虚拟环境，conda自动在则会个环境里帮你解决了包的依赖问题。</description>
    </item>
    <item>
      <title>深度学习笔记(4)：1.1-1.3 边缘检测(edge detection)</title>
      <link>https://anwangtanmi.github.io/posts/de2089c6f98154f839e13f40ed846f8a/</link>
      <pubDate>Mon, 17 Sep 2018 23:19:20 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/de2089c6f98154f839e13f40ed846f8a/</guid>
      <description>第四课开始，我们开始学习卷积神经网络。&#xA;1.1 计算机视觉(computer vision) 深度学习在计算机视觉方面的应用非常振奋人心，一方面是该应用使许多不可能变成了可能，另一方面是深度学习在计算机视觉方面的应用能够给深度学习在其他方面的应用带来一些方法的改进和思考，比如语音识别等。&#xA;计算机视觉包含哪些问题呢？如下图所示：&#xA;比如图片分类，判断一个图片是否是猫；或者目标识别，比如在无人驾驶任务中，识别路上其他的车以及距离，以便无人驾驶能够安全执行；还有就是图片风格转化，比如我们有一个美女的照片(是吴老师的妻子，哈哈)和一个毕加索的绘画，把二者融合起来，就可以得到人像的轮廓和毕加索的风格。&#xA;比如在图片分类中，我们使用的图片可能是小图片，比如像素是64*64，加上颜色的三个channel(RGB channels)，该图片作为输入是一个64*64*3=12288维的向量，还好。但若是大图片呢？比如一个像素为1000*1000的图片，那该图片作为输入就是3m(3百万)维的向量，假设第一层隐藏节点有1000个，且在完全连接的情况下，第一层的权重矩阵就是1000*3m维的矩阵，相当于第一层有30亿的权重，这时很难有足够多的样本去训练这么多的权重，容易发生过拟合，同时这么多的权重还会对内存有一定要求，而且我们不想仅限于处理小图片。为了解决衍生出来的这么多问题，我们需要使用卷积运算，见下节。&#xA;1.2 边缘检测例子(edge detection example) 卷积运算是卷积神经网络最基本的组成部分。这节课我们通过边缘检测例子来学习卷积运算，如下图所示：&#xA;在神经网络模型中，前几层神经网络能检测到边界，后几层可能检测到物体小部分，再后面几层可能检测到具体的物体，在这个例子中就是人脸。接下来我们讲如何在图片中识别边界，也就是为何神经网络前几层能够检测到图片中的边界。&#xA;假设我们有上图中这样一个照片，我们可以通过垂直边缘检测器和水平边缘检测器分别检测出图片中的垂直和水平边缘，检测器检测出的结果如上图所示。但是这些检测器是如何工作的呢？我们以垂直检测器为例。在介绍垂直检测器之前，我们先介绍一种运算，卷积运算(Convolution operation)，如下图所示：&#xA;假设我们的图片如上图左侧矩阵所示，这是一个6*6的矩阵，因为我们没有考虑彩色，这仅仅是一个黑白图片，所以不存在RGB通道，这里图片是6*6*1的向量，接下来我们对其作卷积运算。&#xA;首先介绍一下中间的3*3矩阵，我们称之为‘核(kernel)’或‘过滤器(filter)’，中间‘*’号表示卷积运算，这与我们在程序中使用方法不同，一般‘*’在程序中都表示乘法，或者说是element-wise的乘法。&#xA;那么卷积运算怎么做呢？我曾在深度学习书中看过这样一种比喻，我觉得比较恰当，在这里跟大家分析一下，想象你是一个探险家，在黑暗中找到一幅画卷，为了仔细看清画卷内容，于是你打开手电筒从画的左上角还是扫描直至画的右下角，过滤器做的就是类似手电筒的工作。计算也非常简单，就是element-wise，首先将filter对应于左上角九宫格，然后按上图中红色式子进行计算，就是对这两个3*3矩阵进行element-wise乘法再求和，我们得到-5，写于右侧矩阵的(1，1)位置，就这样从左到右，从上到下，最终得到一个4*4的矩阵，其中最后(4,4)位置对应的-16是由左侧紫色框内矩阵和filter做卷积计算所得。&#xA;了解了卷积运算，接下来我们介绍垂直边缘检测器，如下图所示：&#xA;同样是做卷积运算，不过我们在这里对filter做了‘手脚’，使其能够检测出图片中的垂直边界，怎么说呢？&#xA;在原图中，也就是左矩阵中，我们用0来表示灰色(或者你认为黑色都可以)，10来表示白色，即数值越大颜色越亮，我们想检测出白色和灰色中间那条边界，怎么做呢？我们使用了一个3*3的filter，它的第一列都为1，第二列为0，第三列为-1，经过卷积运算我们得到了一个4*4的矩阵，中间两列值都为30，两边为0，即我们清晰地找出了边界。&#xA;首先为什么能够找出边界，背后的思想我们可以这样认为，边界两侧的数值肯定是有很大差异的，不是边界的数值差异不大，所以kernel这样取值就能使数值差异不大的地方通过卷积运算得出来的值近似为0，而当数值有很大差异时，使用该kernel就无法抵消，这样就可以找出边界，当然这里要注意我们设置kernel的维度要注意原图的维度以及边界的宽度，比如这里最后的出来的边界看起来很宽，那是因为原图太小了，仅仅是6*6，如果是1000*1000，边界效果就会很好了。&#xA;1.3 更多边缘检测内容(more edge detection) 这小节我们将学习更多边缘检测的内容，比如学会如何区分正边和反边，也就是区分由亮到暗和由暗到亮的区别，如下图所示：&#xA;还是之前那个例子，假设我们将图片左右翻转一下，现在左侧是暗的，右侧是亮的，但是我们还使用同一个filter，所以自然会得到一个相反的数值，如上图所示，表示边界的值由30变为-30，这里数值的变化就就能够告知我们一些准确的信息，比如30告诉我们这个边界是正边，即颜色是由亮到暗，而-30就告诉我们这个边界是负边，及颜色是由暗到亮，当然如果你不care正边还是负边可以直接对卷积计算结果取绝对值，即只确定边界并不考虑其颜色是如何变化的。&#xA;除了垂直边缘检测，我们还可以进行水平边缘检测，如下图所示：&#xA;思想很简单，将垂直边缘检测的kernel作转置我们就可以得到水平边缘检测的kernel了。套用在例子中如上图所示，假设我们现在有一个这样的6*6矩阵，其对应图片为左下部和右上部偏暗，显然这个图片既存在水平边缘也存在垂直边缘，现在我们使用水平边缘检测的kernel去检测水平边缘，结果如上图所示，10和-10的出现在这里是因为垂直边缘的影响，同样，因为我们这里举例子的图片太小(6*6)，所以这里显得10和-10占了很大比重，如果我们是1000*1000的图片，结果就可以不考虑10和-10的影响了。&#xA;对于kernel中的数值选取，很多学者在文献中公平地讨论过该如何搭配数据才是合理的，如下图所示：&#xA;除了第一列都是1，第三列都是-1这种filter，我们还可以对数据做其他改变，比如第二行都乘上2，我们称之为sobel filter，相当于在原来基础上进行了加权，它的优点在于增加了中间一行的权重，也就是处在中间的像素，这样可以增加结果的鲁棒性(robust)，还有另一种filter，将第一行和第三行都乘上3，第二行乘上10，我们称之为scharr filter，这样改变也会改进一些性能。&#xA;但实际中，我们一般将这九个值设为九个参数，然后通过神经网络的反向传播去学习，为啥呢么这样做呢？首先是之前的filter过于简单，针对的仅仅是垂直或水平边缘的情况，即90度，且数值是实验前已经设定好的，不一定适合每个问题，而实际问题要复杂得多，比如上图中所示的情况，所以我们希望filter中的数值能够作为参数从数据集中学习而得到，这样一方面是准确，一方面是我们可以应对任何复杂的情况，不论是45度、70度或是73度的边缘我们都可以检测出来。&#xA;通过反向传播去学习kernel中的参数已经成为计算机视觉中最有效的思想之一。接下来我们会学习如何使用反向出传播去学习这九个参数，但在此之前，我们会先学习一下有关卷积运算的基础知识，详见下节。&#xA;版权声明：尊重博主原创文章，转载请注明出处https://blog.csdn.net/kkkkkiko/article/details/81812841</description>
    </item>
    <item>
      <title>目标检测网络的知识蒸馏</title>
      <link>https://anwangtanmi.github.io/posts/1fee8f12289e90fa180ddb237b89dee3/</link>
      <pubDate>Thu, 06 Sep 2018 17:01:21 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1fee8f12289e90fa180ddb237b89dee3/</guid>
      <description>“Learning Efficient Object Detection Models with Knowledge Distillation”这篇文章通过知识蒸馏（Knowledge Distillation）与Hint指导学习（Hint Learning），提升了主干精简的多分类目标检测网络的推理精度（文章以Faster RCNN为例），例如Faster RCNN-Alexnet、Faster-RCNN-VGGM等，具体框架如下图所示：&#xA;教师网络的暗知识提取分为三点：中间层Feature Maps的Hint；RPN/RCN中分类层的暗知识；以及RPN/RCN中回归层的暗知识。具体如下：&#xA;具体指导学生网络学习时，RPN与RCN的分类损失由分类层softmax输出与hard target的交叉熵loss、以及分类层softmax输出与soft target的交叉熵loss构成：&#xA;由于检测器需要鉴别的不同类别之间存在样本不均衡（imbalance），因此在L_soft中需要对不同类别的交叉熵分配不同的权重，其中背景类的权重为1.5（较大的比例），其他分类的权重均为1.0：&#xA;RPN与RCN的回归损失由正常的smooth L1 loss、以及文章所定义的teacher bounded regression loss构成：&#xA;其中Ls_L1表示正常的smooth L1 loss，Lb表示文章定义的teacher bounded regression loss。当学生网络的位置回归与ground truth的L2距离超过教师网络的位置回归与ground truth的L2距离、且大于某一阈值时，Lb取学生网络的位置回归与ground truth之间的L2距离，否则Lb置0。&#xA;Hint learning需要计算教师网络与学生网络中间层输出的Feature Maps之间的L2 loss，并且在学生网络中需要添加可学习的适配层（adaptation layer），以确保guided layer输出的Feature Maps与教师网络输出的Hint维度一致：&#xA;通过知识蒸馏、Hint指导学习，提升了精简网络的泛化性、并有助于加快收敛，最后取得了良好的实验结果，具体见文章实验部分。&#xA;以SSD为例，KD loss与Teacher bounded L2 loss设计如下：&#xA;# -*- coding: utf-8 -*- import torch import torch.nn as nn import torch.nn.functional as F from ..box_utils import match, log_sum_exp eps = 1e-5 def KL_div(p, q, pos_w, neg_w): p = p + eps q = q + eps log_p = p * torch.</description>
    </item>
    <item>
      <title>Deep 3D Face Identification</title>
      <link>https://anwangtanmi.github.io/posts/14403d52f688e023f64bbd87eb624025/</link>
      <pubDate>Wed, 29 Aug 2018 15:47:54 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/14403d52f688e023f64bbd87eb624025/</guid>
      <description>Deep 3D Face Identification 论文地址:https://arxiv.org/abs/1703.10714 github开源代码: https://github.com/jongmoochoi/irisfaceRGBD</description>
    </item>
    <item>
      <title>DeepDive-信息抽取工具安装教程</title>
      <link>https://anwangtanmi.github.io/posts/2fe5adbb0a756f61a9e0a26feebc3dd6/</link>
      <pubDate>Tue, 14 Aug 2018 10:53:08 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2fe5adbb0a756f61a9e0a26feebc3dd6/</guid>
      <description>一、DeepDive简介 DeepDive是信息抽取的工具，它可以从各种dark data（文本、图片、表格）中将非结构数据抽取到关系数据库中。DeepDive的主要功能是抽取dark data中的实体以及实体之间的关系。&#xA;DeepDive文档：&#xA;http://deepdive.stanford.edu/&#xA;DeepDive GitHub：&#xA;https://github.com/HazyResearch/deepdive&#xA;二、DeepDive安装 DeepDive有三种安装方式，Docker镜像安装、快速安装、源包安装&#xA;采用快速安装的方式进行，机器系统为CentOS-7.3.1611&#xA;1.bash &amp;lt;(curl -fsSL git.io/getdeepdive) deepdive //安装deepdive包 1.1 vi ~/.bash_profile //编辑bash_profile文件，将deepdive命令加入当前user环境 1.2 export PATH=~/local/bin:&#34;$PATH&#34; //将这句追加到bash_profile 1.3 source ~/.bash_profile //使配置生效 2.bash &amp;lt;(curl -fsSL git.io/getdeepdive) spouse_example //安装spouse demo包 3.安装postgre作为关系数据库（刚开始想使用mysql，但是deepdive文档中说 minimal support mysql，所以还是使用推荐的postgre），使用bash &amp;lt;(curl -fsSL git.io/getdeepdive) progres 安装会有各种权限的问题，建议使用root用户安装 3.1 yum install https://download.postgresql.org/pub/repos/yum/9.5/redhat/rhel-7-x86_64/pgdg-centos95-9.5-2.noarch.rpm //添加RPM 3.2 yum install postgresql95-server postgresql95-contrib //安装PostgreSQL 9.5 3.3 /usr/pgsql-9.5/bin/postgresql95-setup initdb //初始化数据库 3.4 systemctl enable postgresql-9.5.service //设置开机启动 3.5 systemctl start postgresql-9.</description>
    </item>
    <item>
      <title>深度估计方法的介绍</title>
      <link>https://anwangtanmi.github.io/posts/3965e7fdacdaf6cd7760190d6bce5814/</link>
      <pubDate>Thu, 19 Jul 2018 14:49:22 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3965e7fdacdaf6cd7760190d6bce5814/</guid>
      <description>目前大多数深度估计方法是通过2D的图片到2.5D的表面形状（场景深度）。 比较成功的基于几何图像方法包括：Structure from motion，Shape from X，Monocular stereo，Binocular stereo和Multi-view stereo&#xA;其中Shape from X中的X包括：shading（单幅图像明暗）、stereo vision（立体视觉法–上边的单目，双目和多目立体视觉）、Photometric stereo（光度立体法）、texture（纹理）、motion（运动–structure from motion）、contour（轮廓）、shadow（阴影）&#xA;目前通过深度学习的方法结合传统几何图像法：（单目，双目，多目）—监督学习，非监督学习，半监督学习 具体方法引自： https://wenku.baidu.com/view/ad3b9a6ce009581b6ad9eb06.html&#xA;从明暗恢复形状( shape f rom shading , 简称SFS)： 是计算机视觉中三维形状恢复问题的关键技术之一，其任务是利用单幅图象中物体表面的明暗变化来恢复其表面各点的相对高度或表面法方向等参数值，为进一步对物体进行三维重构奠定基础。由单幅图像灰度明暗变化恢复三维形状的过程可以看作成像过程的逆过程。对实际图像而言，其表面点图像亮度受到了许多因素，如光源、物体表面材料性质和形状，以及摄像机(或观察者)位置和参数等的影响。由单幅图像灰度明暗变化恢复三维形状是在一定的约束条件下从平滑变化的灰度图像恢复出表面法向信息，即根据物体表面反射模型建立物体表面三维形状与采集的图像灰度之间关系的反射图方程，以及由先验知识所建立的对物体表面形状参数的约束条件，对这些关系求解可得到物体表面三维形状。传统SFS方法均进行了如下假设：( 1)光源为无限远处点光源；( 2)反射模型为朗伯体表面反射模型( Lambertian)； ( 3)成象几何关系为正交投影。 立体视觉法(shape from Stereo vision) 可以分为双目和多目立体视觉两种类型。简要说明双目立体视觉的原理。与人类双目视觉的感知过程类似，双目立体视觉从两个不同视点观察同一物体可以得到不同视角下的图像，通过分析不同图像中同一像点的不同视差来获取物体表面的三维空间信息。立体视觉系统可以分为图像采集、摄像机标定、特征提取、立体匹配、深度恢复及三维表面插值等部分组成。目前有MTI人工智能实验室、Yale机器视觉机器人实验室、哈尔滨工业大学、中科院自动化所、西安交通大学、Sony公司、Intel公司等国内外多家研究机构都在从事立体视觉方面的研究。立体视觉法测量方法简单，但该方法的主要缺点是摄像机需要标定，图像特征匹算法复杂。 光度立体法(shape from photometric Stereo) 避免了对应点匹配问题，使用单目多幅图像中蕴涵的三维信息恢复被测对象三维形状。一幅图像像素点的灰度主要由如下因素决定：物体的形状、物体相对于光源和摄像机的位置、光源和摄像机的相对位置，以及物体的物理表面反射特性等。光度立体法固定摄像机和物体的位置，通过控制光源方向，在一系列不同光照条件下采集图像，然后由这几幅图像的反射图方程求解物体表面法向量，进而重构物体三维形状。Ikeuchi等使用光度立体法开发出一套机器人视觉系统，Lee提出一种将光度立体法和SFS方法相结合的三维重构方法。光度立体方法不需要求解反射图方程，方法实现简便。但需要改变光源位置，采集多幅图像，无法使用于自然光或固定光源的物体表面三维重构。 由纹理恢复形状方法(ShaPe from texture) 利用物体表面的纹理信息确定表面方向进而恢复出表面三维形状。纹理由纹理元组成，纹理元可以看作是图像区域中具有重复性和不变性的视觉基元，纹理元在不同的位置和方向反复出现。由纹理元的变化可以对物体表面法向量方向进行恢复。常用的纹理恢复形状方法有三类：利用纹理元尺寸变化、利用纹理元形状变化以及利用纹理元之间关系变化对物体表面梯度进行恢复。Gibson在1950年首先提出了由纹理或纹理梯度表面深度的变化，Kender提出了一种恢复由规则的平行线组成的栅格表面取向的方法。这种方法对物体表面的纹理信息要求严格,需要掌握成像投影中纹理元的畸变信息，只有在纹理特性确定的条件下才能应用。该方法精度较低，而且适用性差，实际应用较少。 由运动恢复形状方法(Shape from motion) 当目标与摄像机在发生相对运动时，摄像机拍摄对应的图像序列,可通过分析该图象序列获得场景的三维信息。摄像机与场景目标间有相对运动时所观察到的亮度模式变化显示出的运动称为光流(optical flow)。光流表示图像的变化，它包含了目标的运动信息，由此可以确定观测者与目标的相对运动，并且可以根据光流求解表面法向量。从运动恢复形状方法适用于被测对象处于运动状态,利用目标与摄像机相对运动来获得场景中目标之间的位置关系，需要多幅图像，不使用静态的场景。同时序列图象像素间的匹配对测量计算精确度影响较大。 由轮廓恢复形状方法(Shape from Contour) 图像的轮廓是物体表面的边缘在图像平面的投影。Barrow与Tennnenboum将轮廓线分为两类，一是不连续轮廓线，它对应物体表面的中断或转折处，形成原因是物体表面法向量在这里发生不连续变化，另一种是Occluding轮廓线，它对应物体表面的法向量光滑地与摄像机垂直，形成原因是物体表面到摄像机的距离在这里发生不连续变化。不连续轮廓线应用于多面体结构的重构和定位，occluding轮廓线用于恢复物体表面的局部特征或全局特征。Karl研究了正交投影下用Occluding轮廓线全局地恢复非退化二次曲面形状。 由阴影恢复形状方法(Shape from shadow) 图像的阴影边界蕴涵了图像的轮廓信息，因此可以根据不同的光照条件下图像的阴影恢复物体表面的三维形状。Michael提出了一种由阴影恢复形状的优化算法。目前国内外对这种方法的研究比较少。其他的非接触三维测量方法有如CT方法，飞行时间法等。</description>
    </item>
    <item>
      <title>Learning to See in the Dark</title>
      <link>https://anwangtanmi.github.io/posts/b9a3712f1c1d692a1891e95a6f1c772c/</link>
      <pubDate>Sat, 30 Jun 2018 18:54:16 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b9a3712f1c1d692a1891e95a6f1c772c/</guid>
      <description>Learning to See in the Dark ref：http://web.engr.illinois.edu/~cchen156/SID.html&#xA;介绍 去噪、去模糊都有，不过在低光照下还是比较难的。高ISO可以提亮，但也会放大噪声。直接缩放或者拉伸直方图，也有一定效果，不过并不能弥补光子不足的缺陷。增加曝光时间，如果移动了会变模糊。这篇文章就是喜欢严重缺少亮度的低光照条件，最好曝光时间还短。传统的去噪方法不行，拍一系列暗图也不行，这些在极低光照条件下基本都会失效。&#xA;本文采用的是数据驱动的方法，设计了一个深度神经网络，能实现颜色转换、去马赛克、减少噪声和图像增强。这种端到端的设计能减少噪声的放大和误差的积累。这篇文章提到以往大家做低光照的研究都是用的合成图像，或者没有ground truth的低光照，因此他们就收集了一些与低光照对应的清晰图像（可作为benchmark）。&#xA;相关工作： 去噪：全变分、小波域变换、稀疏编码、核范数最小化、BM3D（光滑、稀疏、低秩、自适应）。缺点：数据集一般都是合成的。一般认为BM3D在真实图片的表现结果比大部分其他算法好（Benchmarking denoising algorithms with real photographs）。多图效果不错，但本文想单图。&#xA;低光照图像增强：直方图均衡化、图像灰度校正（伽马校正）、暗通道反转、小波变换、Retinex model、光强映射估计。这些方法都认为退化图已经包含比较好的潜在信息，而没有考虑噪声和颜色扭曲的影响。&#xA;含噪数据集：RENOIR对应图片不是完美匹配、HDR+没有极低光照照片、DND也是白天获取的。本文收集了一些数据。&#xA;“看见黑暗”数据集 数据集是作者采集的，有5094张低曝光和高曝光的数据集。场景包含室内室外，都是在固定位置用三脚架拍的。采用app远程设置光圈、ISO等参数，室外光照在0.2勒克斯和5勒克斯之间，室内的更加黑暗。数据采集的原则是高曝光的只要保持视觉效果好即可，不一味追求移除全部噪声。他们称自己的数据集叫SID&#xA;方法 pipeline 部分方法流程图，其中L3指local,linear and learned filters。传统和L3都没有很好处理极低光照的情况。burst序列虽然可以满足一定的需求，但比较苛刻，而且需要“lucky imaging”。&#xA;本文用的是全卷积网络（FCN），而且不是处理普通的sRGB图片，而是用原始的传感器数据。&#xA;文章用到多尺度聚集网络（multi-scale context aggregation network (CAN)）和U-net（U-net: Convolutional networks for biomedical image segmentation），其中U-net是本文默认的网络。其他方法如残差并不适合这里，也许是因为色彩空间不一样。另外要尽量避免全连接层，因为完整的分辨率图可能有6000×4000。&#xA;放大率决定了输出的亮度，这是输入的时候提供的，网络的最终输出直接就是sRGB空间。&#xA;训练 L1和Adam，剪裁成512，学习率1e-4到1e-5，共4000个epoch。&#xA;实验 质量和感知 与传统的比，传统的含噪严重，颜色扭曲。&#xA;他说BM3D在真实图片的去噪效果好，所以就用BM3D来比。不过BM3D需要手动输入一个预估的噪声等级，对结果非常有影响。&#xA;实验结果放到Amazon Mechanical Turk平台上对比，由10个工作者来完成。&#xA;控制变量实验 用CAN替换U-net、用sRGB替换原始信号，SSIM或L2替换L1等等。其中不用原始信号那个掉得最快，其余的差并不太多。&#xA;整体的实验效果显然这个算法无敌，看起来就和白天一样。有时候会稍微过平滑一点，不过看上去视觉效果都还可以。&#xA;讨论 低光照成像有很少的光子数量和低信噪比。本文设计了数据驱动的方法，能有效提高成像表现，还贡献了SID数据集。&#xA;未来的方向：没有处理亮度域的映射，可能会损失一些两端的信息。缺少动态物体。放大倍数也要人为来选，以后可以让它像自动ISO那样。&#xA;算法也不够实时，在两个数据集上需要0.38或0.66秒。&#xA;我个人觉得这个算法是很不错的benchmark，但是数据集实在太大了，而且RAM的消耗非常大。</description>
    </item>
    <item>
      <title>【R语言】kaggle-CNN手写数据集识别</title>
      <link>https://anwangtanmi.github.io/posts/95150504329de857f6d1b19484d4481b/</link>
      <pubDate>Sun, 10 Jun 2018 21:14:33 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/95150504329de857f6d1b19484d4481b/</guid>
      <description>一、Kaggle的任务描述 kaggle-CNN手写数据集下载 网址 MNIST（“国家标准与技术研究院修改版”）是计算机视觉领域的“hello world”数据集。训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据. 数据文件train.csv和test.csv包含从零到九的手绘数字的灰度图像。每张图像的高度为28像素，宽度为28像素，总共为784像素。每个像素都有一个与之相关的像素值，表示该像素的亮度或暗度，较高的数字意味着较暗。此像素值是一个介于0和255之间的整数，包括0和255。 训练数据集（train.csv）有785列。称为“标签”的第一列是由用户绘制的数字。其余列包含关联图像的像素值。 训练集中的每个像素列都有一个像pixelx这样的名称，其中x是0到783之间的整数，包括0和783之间的整数。为了在图像上定位这个像素，假设我们已经将x分解为x = i * 28 + j，其中i和j是0到27之间的整数，包括0和27。然后，pixelx位于28 x 28矩阵的第i行和第j列（索引为零）。 例如，像素31指示位于2行4列的位置。在视觉上，如果我们省略“像素”前缀，则像素组成如下图像：&#xA;000 001 002 003 … 026 027 028 029 030 031 … 054 055 056 057 058 059 … 082 083 | | | | … | | 728 729 730 731 … 754 755 756 757 758 759 … 782 783 测试数据集（test.</description>
    </item>
    <item>
      <title>知识蒸馏（Knowledge Distillation）</title>
      <link>https://anwangtanmi.github.io/posts/2dce89990369a5b867fe62ce0d97c0a7/</link>
      <pubDate>Mon, 04 Jun 2018 16:55:38 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2dce89990369a5b867fe62ce0d97c0a7/</guid>
      <description>1、Distilling the Knowledge in a Neural Network Hinton的文章”Distilling the Knowledge in a Neural Network”首次提出了知识蒸馏（暗知识提取）的概念，通过引入与教师网络（teacher network：复杂、但推理性能优越）相关的软目标（soft-target）作为total loss的一部分，以诱导学生网络（student network：精简、低复杂度）的训练，实现知识迁移（knowledge transfer）。&#xA;如上图所示，教师网络（左侧）的预测输出除以温度参数（Temperature）之后、再做softmax变换，可以获得软化的概率分布（软目标），数值介于0~1之间，取值分布较为缓和。Temperature数值越大，分布越缓和；而Temperature数值减小，容易放大错误分类的概率，引入不必要的噪声。针对较困难的分类或检测任务，Temperature通常取1，确保教师网络中正确预测的贡献。硬目标则是样本的真实标注，可以用one-hot矢量表示。total loss设计为软目标与硬目标所对应的交叉熵的加权平均（表示为KD loss与CE loss），其中软目标交叉熵的加权系数越大，表明迁移诱导越依赖教师网络的贡献，这对训练初期阶段是很有必要的，有助于让学生网络更轻松的鉴别简单样本，但训练后期需要适当减小软目标的比重，让真实标注帮助鉴别困难样本。另外，教师网络的推理性能通常要优于学生网络，而模型容量则无具体限制，且教师网络推理精度越高，越有利于学生网络的学习。&#xA;教师网络与学生网络也可以联合训练，此时教师网络的暗知识及学习方式都会影响学生网络的学习，具体如下（式中三项分别为教师网络softmax输出的交叉熵loss、学生网络softmax输出的交叉熵loss、以及教师网络数值输出与学生网络softmax输出的交叉熵loss）：&#xA;联合训练的Paper地址：https://arxiv.org/abs/1711.05852&#xA;2、Exploring Knowledge Distillation of Deep Neural Networks for Efficient Hardware Solutions 这篇文章将total loss重新定义如下：&#xA;GitHub地址：https://github.com/peterliht/knowledge-distillation-pytorch&#xA;total loss的Pytorch代码如下，引入了精简网络输出与教师网络输出的KL散度，并在诱导训练期间，先将teacher network的预测输出缓存到CPU内存中，可以减轻GPU显存的overhead：&#xA;def loss_fn_kd(outputs, labels, teacher_outputs, params): &#34;&#34;&#34; Compute the knowledge-distillation (KD) loss given outputs, labels. &#34;Hyperparameters&#34;: temperature and alpha NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher and student expects the input tensor to be log probabilities!</description>
    </item>
    <item>
      <title>【论文阅读笔记】Learning to see in the dark</title>
      <link>https://anwangtanmi.github.io/posts/662e92fcaaae709be9fd3827105c537b/</link>
      <pubDate>Thu, 31 May 2018 11:15:59 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/662e92fcaaae709be9fd3827105c537b/</guid>
      <description>本文是CVPR2018论文，主要提出一种通过FCN方法将在黑暗环境中进行的拍摄还原的方法，实现让机器让机器“看破”黑暗。本文的主要创新点为：&#xA;1.提出了一个新的照片数据集，包含原始的short-exposure low-light图像，并附有long-exposure reference图像作为Groud truth，以往类似的研究使用的都是人工合成的图像；&#xA;2.与以往方法使用相机拍摄出的sRGB图像进行复原不同，本文使用的是原始的传感器数据。&#xA;3.提出了一种端到端的学习方法，通过训练一个全卷积网络FCN来直接处理快速成像系统中的低亮度图像。结构如图：&#xA;本文最后提出了该模型待改进的几个地方：&#xA;1.数据集中目前不包含人和运动物体；&#xA;2.模型中的放大率amplification ratio是人工选择的，如果能根据图像自动选择，效果会更好。&#xA;3.可以进行进一步的运行时优化，目前处理一幅照片的时间不能满足实时处理的时限要求。&#xA;………………………………………………………………………………………………………………………………………………………………………………….&#xA;下面的内容转载自：https://blog.csdn.net/linchunmian/article/details/80291921，个人认为是对本文比较好的一篇翻译：&#xA;整理下最近一篇论文的学习笔记。这是由UIUC的陈晨和Intel Labs的陈启峰、许佳、Vladlen Koltun 合作提出的一种在黑暗中也能快速、清晰的成像系统，让机器“看破”黑暗。以下是论文的主要部分。&#xA;摘要&#xA;在暗光条件下，受到低信噪比和低亮度的影响，图片的质量会受到很大的影响。此外，低曝光率的照片会出现很多噪声，而长曝光时间会让照片变得模糊、不真实。目前，很多关于去噪、去模糊、图像增强等技术的研究已被相继提出，但是在一些极端条件下，这些技术的作用就很有限了。为了发展基于学习的低亮度图像处理技术，本文提出了一种在黑暗中也能快速、清晰的成像系统，效果令人非常惊讶。此外，我们引入了一个数据集，包含有原始的低曝光率、低亮度图片，同时还有对应的长曝光率图像。利用该数据集，提出了一种端到端训练模式的全卷积网络结构，用于处理低亮度图像。该网络直接使用原始传感器数据，并替代了大量的传统图像处理流程。最终，实验结果表明这种网络结构在新数据集上能够表现出出色的性能，并在未来工作中有很大前途。&#xA;简介&#xA;任何的图像成像系统都存在噪声，但这很大地影响在弱光条件下图像的质量。高ISO 可以用于增加亮度，但它同时也会放大噪音。诸如缩放或直方图拉伸等图像后处理可以缓解这种噪声影响，但这并不能从根本上解决低信噪比 (SNR) 问题。在物理学上，这可以解释为在弱光条件下增加SNR，包括开放光圈，延长曝光时间以及使用闪光灯等，但这些也都有其自身的缺陷。例如，曝光时间的延长可能会引起相机抖动或物体运动模糊。&#xA;众所周知，暗光条件下的快速成像系统一直都是计算摄影界的一大挑战，也是一直以来开放性的研究领域。目前，许多关于图像去噪，去模糊和低光图像增强等技术相继提出，但这些技术通常假设这些在昏暗环境下捕获到的图像带有中等程度的噪音。相反，我们更感兴趣的是在极端低光条件下，如光照严重受限 (例如月光) 和短时间曝光 (理想情况下是视频率) 等条件下的图像成像系统。在这种情况下，传统相机的处理方式显然已不适用，图像必须根据原始的传感器数据来重建。&#xA;为此，本文提出了一种新的图像处理技术：通过一种数据驱动的方法来解决极端低光条件下快速成像系统的挑战。具体来说，我们训练深度神经网络来学习低光照条件下原始数据的图像处理技术，包括颜色转换，去马赛克，降噪和图像增强等。我们通过端对端的训练方式来避免放大噪声，还能表征这种环境下传统相机处理的累积误差。&#xA;据我们所知，现有用于处理低光图像的方法，在合成数据或真实的低光图像上测试都缺乏事实根据。此外，用于处理不同真实环境下的低光图像数据集也相当匮乏。因此，我们收集了一个在低光条件下快速曝光的原始图像数据集。每个低光图像都有对应的长曝光时间的高质量图像用于参考。在新的数据集上我们的方法表现出不出色的结果：将低光图像放大300倍，成功减少了图像中的噪音并正确实现了颜色转换。我们系统地分析方法中的关键要素并讨论未来的研究方向。&#xA;下图1展示了我们的设置。我们可以看到，在很高的ISO 8,000条件下，尽管使用全帧的索尼高光灵敏度相机，但相机仍会产生全黑的图像。在ISO 409,600条件下，图像仍会产生朦胧，嘈杂，颜色扭曲等现象。换而言之，即使是当前最先进的图像去噪技术也无法消除这种噪音，也无法解决颜色偏差问题。而我们提出的全卷积网络结构能够有效地克服这些问题。&#xA;图1卷积网络下的极端低光成像。黑暗的室内环境：:相机的照度 &amp;lt;0.1 lux。Sony α7S II传感器曝光1/30秒。左图：ISO 8,000相机产生的图像。中间图：ISO 409,600相机产生的图像，图像受到噪声和颜色偏差的影响。右图：由我们的全卷积网络生生的图像。 数据集 (SID)&#xA;我们收集了一个新的数据集，用于原始低光图像的训练和基准测试。See-in-the-Dark(SID) 数据集包含5094张原始的短曝光图像，每张都有相应的长曝光时间的参考图像。值得注意的是，多张短曝光的图像可以对应于相同的长曝光时间的参考图像。例如，我们收集了短时间曝光图像用于评估去燥方法。序列中的每张图像都可视为一张独特的低光图像，这样包含真实世界伪像的图片能够更有利于模型的训练和培训测试。SID 数据集中长时间曝光的参考图像是424。&#xA;此外，我们的数据集包含了室内和室外图像。室外图像通常是在月光或街道照明条件下拍摄。在室外场景下，相机的亮度一般在0.2 lux 和5 lux 之间。室内图像通常更暗。在室内场景中的相机亮度一般在0.03 lux 和0.3 lux 之间。输入图像的曝光时间设置为1/30和1/10秒。相应的参考图像 (真实图像) 的曝光时间通常会延长100到300倍：即10至30秒。各数据集的具体情况如下表1中所示。&#xA;表1. SID 数据集包含5094个原始的短曝光率图像，每张图像都有一个长曝光的参考图像。图像由顶部和底部两台相机收集得到。表中的指标参数分别是(从左到右)：输入与参考图像之间的曝光时间率，滤波器阵列，输入图像的曝光时间以及在每种条件下的图像数量。&#xA;下图2显示了数据集中一部分的参考图像。在每种条件下，我们随机选择大约20％的图像是组成测试集，另外选定10％的数据用于模型验证。&#xA;图2 SID 数据库的实例。前两行是SID 数据集中室外的图像，底部两行是室内的图像。长曝光时间的参考图像 (地面实况) 显示在前面。短曝光的输入图像(基本上是黑色) 显示在背部。室外场景下相机的亮度一般在0.2到5 lux，而室内的相机亮度在0.</description>
    </item>
    <item>
      <title>Ubuntu——从安装到放弃（/逃 ）【此教程包括ubuntu的安装、关机卡死（Nvidia安装）、无法连接wifi等解决方案】</title>
      <link>https://anwangtanmi.github.io/posts/f52ec135039a4e8ec10e5506fcfb9b30/</link>
      <pubDate>Sat, 31 Mar 2018 16:32:08 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f52ec135039a4e8ec10e5506fcfb9b30/</guid>
      <description>前两天为了深度学习要安装ubuntu系统，心想那还不简单，搞个U盘刻录一下安装下，岂不是完美。 谁曾想安装这东西来来回回数十遍，期间还重置了电脑，好不容易安装好了，杂七杂八的问题贼多，好在最后都解决了。 。。。。。。深深地体会到了从安装到放弃。“新的风暴已经出现，怎么能够停滞不前，穿越时空，坚定向前” 开始我的坚强吧。&#xA;1、开辟空闲空间 右键我的电脑，选择管理&#xA;选择磁盘管理，选择一个硬盘(别选C盘），然后右键，选择压缩卷，会显示要压缩的大小。 建议不要太小，我压缩了100G（10 0000MB）&#xA;2、制作系统启动U盘 直接去官网，按如下图步骤&#xA;下载好之后，下一个映像写入软件。这里我用的是UltraISO,按照下面图片步骤。&#xA;3、安装ubuntu系统 电脑先关机，再打开，在开机时（狂）按F12进入BOOT Manager 里面选择U盘启动（不同品牌的电脑可能打开方式不同 ，自行百度，我的是联想）然后会出现一个选择界面，你可以选择Try without installing ，也可以选择Install Ubuntu，都一样。&#xA;选择语言&#xA;为图形…这个选项可选可不选，我也没有去查过资料，不知道有什么大的区别。（建议先选上）&#xA;这里选择其他选项，来配置分区&#xA;这里的分区教程照片是引用https://blog.csdn.net/sinat_18897273/article/details/71191389博主的图片。（如果有问题，可以联系我，我进行修改）。 分区大小推荐设置：（选择空闲，即你之前在windows分配的空间，然后再点击左下角的加号） swap （电脑内存）8000MB 逻辑分区 用于交换空间 / 30000MB 逻辑分区 Ext4日志文件系统 /boot 500MB 主分区 Ext4日志文件系统 /home 剩余空间 逻辑分区 Ext4日志文件系统 这里一定要注意新分区的类型和用于 配置完成后，在安装启动引导器的设备这一栏选择与你/boot相同的sdb，看接下来的第五张图。&#xA;然后点继续，安装就行了&#xA;在重启时，（狂）按F12，进入Boot Manager选择windows启动，进行最后的配置。我这里用EasyBCD软件来操作。&#xA;重启即可。如果你没碰到什么问题，那你真是个lucky guy。 如果你每次开机时都是直接进入一个系统，这个问题我也没解决（只能通过开机按F12来选择系统启动）&#xA;4、关机卡死解决方案 如果你发现你每次ubuntu系统关机都会卡死，那你先去看看系统设置-&amp;gt;软件和更新-&amp;gt;附加驱动里面有没有显卡驱动。有的话关机还卡死的话，我也不太清楚，如果没有显卡驱动，按下面步骤来。此处引用https://blog.csdn.net/tianrolin/article/details/52830422的博文（如有问题，请联系我进行修改）&#xA;搜索NVIDIA linux&#xA;选择你对应的版本进行下载。如果你的ubuntu无法连wifi（如果你有宽带，那就没这个问题，可怜我们学校的闪讯没有linux版，无法连接wifi的问题我待会再讲），你可以先去windows系统下载，拷贝到U盘里，再复制过来。（这里我建议你把文件名字改成简单易记，后面命令要用到）&#xA;拷贝过来后，你可以直接将文件移动到/home/（你的用户名）的目录下（因为待会我的命令就是基于文件已经在那个文件夹下）当然你也可以在输入命令时加上相应的路径。&#xA;1、首先我们要禁止集成的nouveau驱动。（按Ctrl+Alt+T调出终端）按下面命令进行输入（黑体字不用输入） 查看属性(下面的Is和lh是小写的L） sudo ls -lh /etc/modprobe.d/blacklist.conf&#xA;修改属性 sudo chmod 666 /etc/modprobe.d/blacklist.conf&#xA;用gedit编辑器打开 sudo gedit /etc/modprobe.</description>
    </item>
    <item>
      <title>CVPR 2018 挑战赛</title>
      <link>https://anwangtanmi.github.io/posts/64e707908c62f55b90fc9b803b8daa85/</link>
      <pubDate>Sun, 11 Mar 2018 12:42:34 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/64e707908c62f55b90fc9b803b8daa85/</guid>
      <description>﻿﻿ 6.18-22 日，CVPR 2018 将在美国盐湖城举办。&#xA;所有workshop，见如下网址http://cvpr2018.thecvf.com/program/workshops，有时间的同学参考下。&#xA;Date &amp;amp; Time Location Workshop Organizer(s) Monday, June 18, 2018 TBA First International Workshop on Disguised Faces in the Wild Nalini Ratha Monday, June 18, 2018 TBA Fine-grained Instructional Video undERstanding (FIVER) Jason Corso Monday, June 18, 2018 TBA Low-Power Image Recognition Challenge Yung-Hsiang Lu Monday, June 18, 2018 TBA NVIDIA AI City Challenge Milind Naphade Monday, June 18, 2018 TBA DeepGlobe: A Challenge for Parsing the Earth through Satellite Images Ilke Demir Monday, June 18, 2018 TBA VQA Challenge and Visual Dialog Workshop Yash Goyal Monday, June 18, 2018 TBA Visual Understanding of Humans in Crowd Scene and the 2nd Look Into Person (LIP) Challenge Xiaodan Liang, Jian Zhao Monday, June 18, 2018 TBA Language and Vision Andrei Barbu Monday, June 18, 2018 TBA Robust Vision Challenge Andreas Geiger Monday, June 18, 2018 TBA Workshop and Challenge on Learnt Image Compression George Toderici Monday, June 18, 2018 (PM) TBA Large-Scale Landmark Recognition: A Challenge Bohyung Han Monday, June 18, 2018 TBA The DAVIS Challenge on Video Object Segmentation 2018 Jordi Pont-Tuset Monday, June 18, 2018 TBA Bridging the Gap between Computational Photography and Visual Recognition: the UG2 Prize Challenge Walter J.</description>
    </item>
    <item>
      <title>DSSMs: Deep Structured Semantic Models</title>
      <link>https://anwangtanmi.github.io/posts/dd0b360844d402d4feac7e43ff3bebed/</link>
      <pubDate>Thu, 08 Feb 2018 09:24:55 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/dd0b360844d402d4feac7e43ff3bebed/</guid>
      <description>Attention! 我的Dr.Sure项目正式上线了，主旨在分享学习Tensorflow以及DeepLearning中的一些想法。期间随时更新我的论文心得以及想法。&#xA;Github地址：https://github.com/wangqingbaidu/Dr.Sure&#xA;CSDN地址：http://blog.csdn.net/wangqingbaidu&#xA;个人博客地址：http://www.wangqingbaidu.cn/&#xA;DSSMs: Deep Structured Semantic Models DSSM(Deep Structured Semantic Model):基于深度网络的语义模型，这篇论文的核心思想是把文本数据以及用户的点击历史记录映射到一个相同维度的语义空间，通过最大化两个空间的cosine相似度，最终达到信息检索的目的。&#xA;DSSM这篇论文是在13年被首次提出，14，15经过两年的发展，分别演化出了基于卷积网络的C-DSSM(Convolutional DSSM)、基于循环神经网络的R-DSSM(Recurrent DSSM)、针对不同信息来源的MV-DSSM(Multi-View DSSM)、最后还有一个专门针对Caption的DMSM(Deep Multimodal Similarity Model)。&#xA;注：C-DSSM在论文里面的模型名称叫CLSM(Convolutional Latent Semantic Model)，但是为了前后的连贯性，Dr.Sure就私自改成了C-DSSM。&#xA;DSSM的设计初衷是为了做信息检索（IR，应用到搜索引擎），但是后来逐渐被演化到不同数据源的语义空间映射问题，包括但是不仅仅包括信息检索、图文匹配、Caption、Sent2Vec。&#xA;这几个相关工作都有Xiaodong He的工作，他在CVPR2015上做了一个主题报告，专门针对DSSM以及内容理解做了一个比较详细的阐述，详见PPT。他在slides里面highlight出来的一些问题，感觉相当有参考价值。&#xA;这个博客汇总了13年到15年5篇论文有关DSSM的相关工作，详细介绍DSSM相关算法的发展。&#xA;一、 DSSM: Deep Structured Semantic Models 从模型的名称中可以看出，这个模型是基于深度学习的算法，并且是将数据映射到一个语义空间的模型。所以相比于以往其它的语义模型来说，最大的优势就是引入了深度网络。&#xA;1. 相关Semantic Models a.) Latent Semantic Models 这些模型里面最常被提及的就是Latent Semantic Analysis(LSA)、Probabilistic LSA(PLSA)以及LDA等，这些都是基于无监督的模型，而且大部分是基于矩阵分析以及概率模型的基础建立而来。对于不同单词的相同语义不能很好的建模。这里引用He博士在slides中的一个例子：&#xA;Minnesota became a state on? When was the state Minnesota created? 这个例子只有Minnesota state关键词是共享的，句式包括其他的单词和句式都不相同，这样的数据放到传统的语义模型中很可能不会被映射到一个相同的语义空间，然而对于理解来说，这两句话的语义是完全相同的。&#xA;b.) Auto Encoder Decoder 前面基于矩阵分析的模型存在最大的问题就是算法为无监督的，虽然这些算法具有很强的鲁棒性，但是真是的应用场景中，每个任务的目标函数各不相同，这就好像用一个瑞士军刀削一个苹果，反而不如一个更合适的水果刀。&#xA;Auto encoder decoder的提出就是想解决上面的一些问题，使用深度网络，引入监督信息，通过encoder编码输入数据，映射到一个语义空间，然后在用decoder还原成原始数据。&#xA;其网络结构如下：</description>
    </item>
    <item>
      <title>推荐系统中特征工程的自动化——Deep Crossing模型</title>
      <link>https://anwangtanmi.github.io/posts/af8194729f8757bb1e85a0c72b2c50e3/</link>
      <pubDate>Mon, 05 Feb 2018 23:07:49 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/af8194729f8757bb1e85a0c72b2c50e3/</guid>
      <description>本文是（2016）Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features笔记&#xA;主题：在搜索页面推荐广告 维度：用户、广告商、搜索平台 目标：展示用户最想看的广告 概念： query：搜索语句 Campaign：广告主在一段明确的期间里(如一年)，推出一系列拥有共同主题或讯息的广告，以期建立广告讯息的累积效果，塑造品牌与企业一致的形象，并给予目标受众持续而深刻的刺激与冲击。&#xA;特征： 分类变量——one-hot vector campaign id——分成一组特征对： a）CampaignID：点击Top10000的id做one-hot，（0,0,0，，，0）代表所有剩下的id b）CampainIDCount：campaign的统计量，比如特征值（be referred to as counting feature）。 交叉特征：deep crossing 不使用交叉特征。&#xA;模型结构： feature#2：低于256维，跳过embedding层 其他feature：进入embedding层（单层ReLU）&#xA;embedding层的输入+低维变量，合并为k维特征进入stacking层。&#xA;residual层： 残差层由如下的残差单元组成，它们是由图像识别模型Residual Net中的残差单元改造而来： 残差单元的特殊之处在于，输入会在经过两个ReLU层后，加上它本身。也就是说，F函数拟合的是输出与输入的残差值。&#xA;模型对比：作者尝试过深层神经网络的模型，但是效果并没有好于2-3层的神经网络。deep cross是可以轻易优于2-3层神经网络的模型。</description>
    </item>
    <item>
      <title>keras瞎搞系列-卷积自编码去噪</title>
      <link>https://anwangtanmi.github.io/posts/b814f001fe34bb0136ad9e16baf8f832/</link>
      <pubDate>Fri, 19 Jan 2018 19:59:51 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b814f001fe34bb0136ad9e16baf8f832/</guid>
      <description>keras瞎搞系列-卷积自编码去噪 我们把训练样本用噪声污染，然后使解码器解码出干净的照片，以获得去噪自动编码器。首先我们把原图片加入高斯噪声，然后把像素值clip到0~1。&#xA;头文件 from keras.layers import Input,Convolution2D from keras.models import Model from keras.datasets import mnist from keras.callbacks import EarlyStopping import numpy as np import matplotlib.pyplot as plt 导入数据 (X_train,_),(X_test,_) = mnist.load_data() X_train = X_train.astype(&#39;float32&#39;)/255. X_test = X_test.astype(&#39;float32&#39;)/255. 将值归一化到0-1 调整大小&#xA;这里所需要的是[28,28,1]维的，所以得调整大小。 X_train = np.reshape(X_train,(len(X_train),28,28,1)) X_test = np.reshape(X_test,(len(X_test),28,28,1)) 添加噪声 noise_factor = 0.5 X_train_noisy = X_train+noise_factor*np.random.normal(loc=0.0,scale=1.0,size=X_train.shape) X_test_noisy = X_test+noise_factor*np.random.normal(loc=0.0,scale=1.0,size=X_test.shape) X_test_noisy = np.clip(X_test_noisy,0.,1.) X_train_noisy = np.clip(X_train_noisy,0.,1.) 构建模型 input_img = Input(shape=(28,28,1)) x = Convolution2D(32,(3,3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(input_img) x = MaxPooling2D((2,2),padding=&#39;same&#39;)(x) x = Convolution2D(32,(3,3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x) encoded = MaxPooling2D((2,2),padding=&#39;same&#39;)(x) x = Convolution2D(32,(3,3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(encoded) x = UpSampling2D((2,2))(x) x = Convolution2D(32,(3,3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x) x = UpSampling2D((2,2))(x) decoded = Convolution2D(1,(3,3),activation=&#39;sigmoid&#39;,padding=&#39;same&#39;)(x) autoencoder = Model(inputs=input_img,outputs = decoded) 模型编译和训练 autoencoder.</description>
    </item>
    <item>
      <title>数据扩增 data-augmentation的方法及代码</title>
      <link>https://anwangtanmi.github.io/posts/308b0b6c29b9d19297066e3906e9e639/</link>
      <pubDate>Sun, 24 Dec 2017 22:56:30 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/308b0b6c29b9d19297066e3906e9e639/</guid>
      <description>为了扩增数据集，采用了2种方式来进行数据的扩增。&#xA;1、使用keras的数据增强处理&#xA;2、使用skimage的数据增强处理&#xA;keras包括的处理，有featurewise视觉上图像会稍微变暗，samplewise视觉上图像会变成类x光图像形式，zca处理视觉上图像会变成灰白图像，rotation range 随机旋转图像，水平平移,垂直平移，错切变换，图像缩放，图片的整体的颜色变换，水平翻转操作，上下翻转操作， rescale。&#xA;存在的问题是图像的变换是随机的，即有的图像可能不会变换。&#xA;skimage的数据增强处理，有resize， gray，rescale，noise，flip，rotate， shift， zoom，gaussian zoom，shear，contrast，channelshift，PCA，polar。&#xA;目前极坐标变换polar，还有些问题，即转换后只有灰度图，有待解决。&#xA;相关的代码见如下地址。供参考。&#xA;github地址：https://github.com/renwoxing2016/data-augmentation</description>
    </item>
    <item>
      <title>tensorflow离线源码安装</title>
      <link>https://anwangtanmi.github.io/posts/9c3a122b4f7d4e357283608a252d9da7/</link>
      <pubDate>Fri, 22 Dec 2017 14:43:51 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9c3a122b4f7d4e357283608a252d9da7/</guid>
      <description>从源码离线编译安装tensorflow，记录踩过的坑：&#xA;1.tensorflow的外部依赖&#xA;tensorflow的外部依赖很多，所依赖的外部库均在WORKSPACE文件和//tensorflow/workspace.bzl文件中给出了url地址&#xA;使用如下命令可以拉取全部的external外部依赖&#xA;bazel fetch //... 为能够离线安装tensorflow，需要建立内部局域网的http服务。（建立http服务可以使用wampserver，可参考&#xA;http://blog.csdn.net/huang_yx005/article/details/50914735）&#xA;在外部联网机器中使用脚本下载全部的外部依赖，并拷贝到内部局域网的http服务的根目录下。&#xA;将WORKSPACE文件和//tensorflow/workspace.bzl文件中的外部依赖的url地址的前缀全部换成内部局域网的http服务的地址。&#xA;至此，编译安装需要外部依赖时会从内部http下载相应的库&#xA;2.换行符问题&#xA;从github上clone下来的文件的可能采用dos换行符，也可能采用unix换行符&#xA;如果某个可执行文件采用了dos换行符，此时在liunx下运行就会出错&#xA;解决方法：&#xA;使用vim打开，输入vim命令 :set ff=unix&#xA;上述命令将文件中所有的dos换行符’\r\n’转换成unix换行符‘\n’&#xA;3.运行如下的命令时产生错误&#xA;bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg&#xA;error in tensorflow setup: ‘install_requires’ must be a string or list of string containing valid project/version requirement specifiers;&#xA;Expected ‘,’ or end-of-list in backposts.weakref &amp;gt;= 1.0rc1;&#xA;解决办法：&#xA;python版本高于3.4，python安装backposts.weakref.10rc1&#xA;4.源码编译的大致流程：&#xA;export TEST_TMPDIR=…&#xA;export LD_LIBRARY_PATH=… 需包含cuda, cudnn相关的路径&#xA;./configure 对tensorflow进行配置&#xA;bazel build ––config=opt —config=cuda //tensorflow/tools/pip_package:build_pip_package 编译目标程序，开启GPU&#xA;If you would like to prevent new dependencies from being added during builds, you can specify the --fetch=false flag</description>
    </item>
    <item>
      <title>Tensorflow1.4安装gpu教程</title>
      <link>https://anwangtanmi.github.io/posts/0d73a4c3c60d1dd9fbd72544d50a807f/</link>
      <pubDate>Wed, 20 Dec 2017 16:07:41 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0d73a4c3c60d1dd9fbd72544d50a807f/</guid>
      <description>以前安装的是tensorflow-cpu，但是随着运算量的提高产生了鼓捣tensorflow-gpu的想法。官网的教程看起来很简单，但是坑很多。本文主要是帮助大家绕过那些坑，成功安装可以运行的tensorflow-gpu。&#xA;官网安装教程链接 这要求我们安装CUDA Toolkit 8.0和cuDNN v6.1，注意Tensorflow 1.4必须是CUDA Toolkit 8.0，但是官网默认下载CUDA Toolkit 9.1。 下载CUDA Toolkit 8.0 ,之后一直点默认的就可以安装完成 下载cuDNN v6.0,注意下载这个安装包需要注册并且填一堆问卷，下下来以后解压，然后直接拷到cuda路径对应的文件夹下面就行。具体操作是讲将下图对应三个文件夹里面的文件分别拷贝到cuda安装目录对应文件夹 cuda安装目录如下图所示 如果现实如图所示就证明tensorflow-gpu安装成功了 如果不明白的话，大家可以参考点击</description>
    </item>
    <item>
      <title>Coursera-Deep Learning Specialization 课程之（四）：Convolutional Neural Networks: -weak4编程作业</title>
      <link>https://anwangtanmi.github.io/posts/75cfc7acbf149c30c82048cc1348b6cf/</link>
      <pubDate>Tue, 14 Nov 2017 22:27:13 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/75cfc7acbf149c30c82048cc1348b6cf/</guid>
      <description>人脸识别 Face Recognition for the Happy House from keras.models import Sequential from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate from keras.models import Model from keras.layers.normalization import BatchNormalization from keras.layers.pooling import MaxPooling2D, AveragePooling2D from keras.layers.merge import Concatenate from keras.layers.core import Lambda, Flatten, Dense from keras.initializers import glorot_uniform from keras.engine.topology import Layer from keras import backend as K K.set_image_data_format(&#39;channels_first&#39;) import cv2 import os import numpy as np from numpy import genfromtxt import pandas as pd import tensorflow as tf from fr_utils import * from inception_blocks_v2 import * %matplotlib inline %load_ext autoreload %autoreload 2 np.</description>
    </item>
    <item>
      <title>Windows10（64bit,显卡GTX1050Ti）环境下的python3.5.2&#43;tensorflow（gpu）&#43;opencv安装配置</title>
      <link>https://anwangtanmi.github.io/posts/844afdd9327d131a34551d467d574638/</link>
      <pubDate>Sat, 04 Nov 2017 17:34:35 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/844afdd9327d131a34551d467d574638/</guid>
      <description>Windows10（64bit,显卡GTX1050Ti）环境下的python3.5.2+tensorflow（gpu）+opencv安装配置 笔记本环境: windows10(64位)，显卡GTX050Ti 安装前的注意事项: 1.TensorFlow目前在windows下只支持64-bit Python 3.5 2.tensorflow1.3当前只支持CUDA8.0&#xA;1.安装python 3.5 这里，笔者使用的是python 3.5.2，具体安装如下： (1).Python3.5.2安装教程_百度经验http://jingyan.baidu.com/article/a17d5285ed78e88098c8f222.html 备注：想要下载安装anaconda 3.5的朋友可以参考下面这篇博文： http://blog.csdn.net/sb19931201/article/details/53648615&#xA;2.配置tensorflow(gpu)+opencv+其他 (2).pip升级（python3.5.2使用） 最新版本指令：打开CMD，输入 python -m pip install –upgrade pip&#xA;(3).安装tensorflow 命令:pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow/tensorflow-gpu 或pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow（CPU版）&#xA;(4).安装opencv 命令:pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python&#xA;(5).安装matplotlib 指令：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib&#xA;(6).安装scipy（找到安装包路径，在安装） 由于scipy依赖于有mkl的numpy库，而从pip安装的numpy的库不带mkl，所以需要从上面的网站下载。&#xA;这里，笔者分别下载了scipy-1.0.0-cp35-cp35m-win_amd64.whl和numpy-1.13.3+mkl-cp35-cp35m-win_amd64.whl放在python\Scripts文件夹下。 scipy下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy&#xA;numpy+mkl下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy&#xA;a.安装numpy+mkl，pip install &amp;lt;安装包的路径&amp;gt;numpy-1.13.3+mkl-cp35-cp35m-win_amd64.whl&#xA;b.安装scipy：pip install &amp;lt;安装包的路径&amp;gt;scipy-1.0.0-cp35-cp35m-win_amd64.whl (7).安装sklearn pip install -i https://pypi.tuna.tsinghua.edu.cn/simple sklearn&#xA;（8）.安装pyinstaller pip install -i https://pypi.</description>
    </item>
    <item>
      <title>人车密度估计–Towards perspective-free object counting with deep learning</title>
      <link>https://anwangtanmi.github.io/posts/134ba845646ae3eb1bebf313754e8728/</link>
      <pubDate>Sat, 30 Sep 2017 10:26:47 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/134ba845646ae3eb1bebf313754e8728/</guid>
      <description>Towards perspective-free object counting with deep learning ECCV2016 https://github.com/gramuah/ccnn&#xA;本文针对人车密度估计问题，主要做了两个工作：1）提出了一个 novel convolutional neural network：Counting CNN (CCNN)，将图像块回归到密度图，2）第二个工作就是 提出了一个 scale-aware counting model，Hydra CNN，用于学习 multiscale non-linear regression model 这里我们将人车密度估计问题转为回归问题 3 Deep learning to count objects 3.1 Counting objects model ground truth density map D 真值密度图 由 高斯核对人车位置进行卷积得到，有了密度图通过积分得到图像中总的人车数&#xA;3.2 The Counting CNN 这个网络使用了两个 max-pooling，输入尺寸是 72×72 ，输出的密度图尺寸是18×18 变为原来的 1/4&#xA;Given a test image, we first densely extract image patches 给定一张测试图像，我们从图像中提出很多重叠的图像块，对图像块进行密度估计，再有这些图像块密度图组合为完整图像的密度估计图&#xA;3.3 The Hydra CNN 对于一般的基于回归的计数模型，通常需要对输入特征进行 geometric correction， using an annotated perspective map of the scene 为什么需要这个矫正了？ 主要还是 perspective distortion Technically, the perspective distortion exhibited by an image, causes that features extracted from the same object but at different scene depths would have a huge difference in values.</description>
    </item>
    <item>
      <title>百度开源移动端深度学习框架mobile-deep-learning</title>
      <link>https://anwangtanmi.github.io/posts/4841daa65010b2c313444c3d46e9ddfc/</link>
      <pubDate>Tue, 26 Sep 2017 13:07:41 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4841daa65010b2c313444c3d46e9ddfc/</guid>
      <description>2017 年 9 月 25 日，百度在 GitHub 开源了移动端深度学习框架 mobile-deep-learning（MDL）的全部代码以及脚本，希望这个项目在社区的带动下能够更好地发展。&#xA;写在前面 深度学习技术已经在互联网的诸多方向产生影响，每天科技新闻中关于深度学习和神经网络的讨论越来越多。深度学习技术在近两年飞速发展，各种互联网产品都争相应用深度学习技术，产品对深度学习的引入也将进一步影响人们的生活。随着移动设备的广泛使用，在移动互联网产品应用深度学习和神经网络技术已经成为必然趋势。&#xA;与深度学习紧密联系在一起的图像技术同样在业界广泛应用。传统计算机视觉和深度学习的结合使图像技术得以快速发展。&#xA;GitHub 地址：https://github.com/baidu/mobile-deep-learning&#xA;移动端深度学习技术应用 百度应用案例&#xA;在移动端应用深度学习技术比较典型的就是 CNN（Convolutional Neural Network）技术，即常被人提起的卷积神经网络。mobile-deep-learning（MDL）是一个基于卷积神经网络实现的移动端框架。&#xA;MDL 在移动端主要有哪些应用场景呢？比较常见的如分辨出一张图片中的物体是什么，也就是 分类；或者识别出一张图片中的物体在哪、有多大，也就是 主体识别。&#xA;下面这个 App 叫拾相，可以在 Android 平台的应用宝中找到。它可以自动为用户将照片分类，对于拥有大量照片的用户来讲，这个功能很有吸引力。&#xA;另外，在手机百度搜索框右侧可以打开图像搜索，打开图像搜索后的界面效果如下图。当用户在通用垂直类别下开启自动拍开关（图中下方标注）时，手停稳它就会自动找到物体进行框选，并无需拍照直接发起图像搜索。整个过程可以给用户带来流畅的体验，无需用户手动拍照。图片中的框体应用的就是典型的深度学习主体识别技术，使用的就是 mobile-deep-learning（MDL）框架。MDL 目前在手机百度中稳定运行了多个版本，经过数次迭代后可靠性大幅提升。&#xA;业界其他案例&#xA;互联网行业在移动端应用神经网络的案例已经越来越多。&#xA;目前的流派主要有两种，其一是完全在客户端运行神经网络，这种方式的优点显而易见，那就是不需要经过网络，如果能保证运行速度，用户体验会非常流畅。如果能保证移动端高效运行神经网络，可以使用户感觉不到加载过程。使用完全脱离互联网网络在移动端运算神经网络的 App 已经举例，如前述拾相和手机百度中的图像搜索。&#xA;其二是另一种，运算神经网络过程依赖互联网网络，客户端只负责 UI 展示。在客户端神经网络落地之前，绝大部分 App 都使用了这种运算在服务端、展示在客户端的方式。这种方式的优点是实现相对容易，开发成本更低。&#xA;为了更好理解上述两种神经网络的实现方法，下面展示两个识别植物花卉的例子，分别用到了识花和形色两个 App。这两款 App 都使用了典型分类方法，都可以在 iOS 平台的 App Store 中找到。下图是一张莲花图片，这张图片使用识花和形色两个 App 分类都能得到较好的分类结果。你可以尝试安装这两款 App 并根据使用效果来判断它们分别使用了上述哪一种方法。&#xA;识花&#xA;近一年来涌现出很多花卉识别的 App。微软「识花」是微软亚洲研究院推出的一款用于识别花卉的 App，用户可以在拍摄后选择花卉，App 会给出该类花卉的相关信息。精准的花卉分类是其对外宣传的一大亮点。&#xA;形色&#xA;这款「形色」App，只需要对准植物 (花、草、树) 拍照，就能快速给出植物的名字，还有不少有趣的植物知识，如这个植物还有什么别名、植物的花语、相关古诗词、植物文化、趣味故事以及养护方法，看完收获不少。&#xA;移动端应用深度学习的难点 一直以来由于技术门槛和硬件条件的限制，在移动端应用深度学习的成功案例不多。传统移动端 UI 工程师在编写神经网络代码时，可以查阅到的移动端深度学习资料也很少。另一方面，时下的互联网竞争又颇为激烈，先入咸阳者王，可以率先将深度学习技术在移动端应用起来，就可以更早地把握时代先机。&#xA;移动端设备的运算能力相对 PC 端非常弱小。由于移动端的 CPU 要将功耗指标维持在很低的水平，制约了性能指标的提升。在 App 中做神经网络运算会使 CPU 运算量猛增。如何协调好用户功耗指标和性能指标就显得至关重要。</description>
    </item>
    <item>
      <title>sensor横纹和竖纹</title>
      <link>https://anwangtanmi.github.io/posts/a22217e9ad5f54c6909df876861271a6/</link>
      <pubDate>Sat, 12 Aug 2017 14:08:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a22217e9ad5f54c6909df876861271a6/</guid>
      <description>暗态下：横纹和竖纹比较明显&#xA;横纹:&#xA;产生原因：横纹与sensor的模拟电源的纹波相关（power denoise）。&#xA;1.avdd过低&#xA;解决办法：在sensor的模拟电源线上加一个LC低通或者使用一个LDO后效果就会好很多。&#xA;一般sensor的电源纹波最好能控制在1~2%以内，否则会很严重的影响低照度效果。&#xA;竖纹(FPN):&#xA;产生原因：coms sensor的工艺问题&#xA;1是外部电源输入，2是模组层级芯片自生数模干扰，3是某些芯片层级自生数模干扰。尽量滤波做好点。&#xA;解决办法：&#xA;1.sensor厂家一般会有FPN校正的配置&#xA;2.看看MIPI线或者并口线，看看是否做了屏蔽处理</description>
    </item>
    <item>
      <title>神经网络基础</title>
      <link>https://anwangtanmi.github.io/posts/01f6af9ea9402a7328dee6f44bcc75c0/</link>
      <pubDate>Sun, 16 Jul 2017 19:52:24 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/01f6af9ea9402a7328dee6f44bcc75c0/</guid>
      <description>图像数据 图像是像素点组成的矩阵， [0-255]越小越暗越大越亮 RGB通道，彩色图。 32X32X3 彩色图 32X32X1 灰度图&#xA;线性分类： 得分函数： 像素点做一个转换，拉长。矩阵变成一个列向量。 对每一个像素点来说都有权重参数。使用神经网络的时候，W的shape是多大的必须明确给出。分类——分几类。 b参数，影响很小，有几个分类有几个b。 损失函数： 错误分类得分-正确得分&amp;gt;0 有损失，只对正数算有损失，对负数是没有损失的 所有损失加在一起 +1（可以容忍的范围区间） -0.9本来不计损失，但是不在容忍范围之内，所以计算损失值。 通过损失值衡量权重参数是否达到标准。 在迭代的时候可能是一个batch一个batch的迭代的。 不一定我们是对一个数据计算损失函数，可能输入一批数据进行计算损失函数。 一次迭代样本不是一个的时候，我们要考虑样本个数，为了消除样本个数对损失函数的影响，我们需要除以样本个数。 消除样本个数对损失函数的影响&#xA;正则化： 得到相同结果的W，这里我们需要正则化惩罚项，我希望W模型越平稳越好。 4个像素点 W1X.T=W2X.T 结果相同，两种模型表达效果一样。正则化惩罚项λW^2。 W1只利用了一个像素点，但是对于W2来说，它利用了四个像素点，综合的考虑了每一个像素点的值，得到了一个更好的模型。所以对W1惩罚的更严重些。&#xA;损失函数=自身的损失值+正则化的损失值&#xA;softmax分类器： 每一个类别有一个明确的分值，我们将分值转换成概率。 softmax输出的是概率。 softmax是多分类的分类器。&#xA;e^x扩大数值，更好的进行分类任务，然后归一化，得分值转换成概率值。 正确类别概率越小，损失值越大。&#xA;LOSS值的不同 SVM loss: 对于上面的loss来说，它的结果是1.58。而下面的loss是0.452。上面的LOSS值有可能会等于0. Softmax loss: 对于唯一的概率值，它得到的loss值永远会存在不会存在损失值为0的情况。&#xA;最优化： 归一化后就是一个概率的结果值了。sigmoid函数。 对于神经网络来说，无法求驻节，我们需要进行一个迭代优化的操作，最优化问题。 求得loss值后我们就可以进行最优化的操作了。 X-&amp;gt;loss：这一系列过程称为前向传播过程，通常神经网络是由BP算法来求解的，BP算法主要是通过前向传播求得loss值，再经历反向传播，去优化loss参数。 梯度下降算法，每一次迭代，优化W参数，loss降低，达到优化作用。 epoch 是迭代完所有数据， 一次迭代是指跑完一个batch&#xA;学习率： W不太好的时候，我们想要让它向好的方向学，一次学多大？就是用学习率额，更新多大。W-=ΔW+LR 学习率不能过大，通常为0.001、0.0001一次少走一些，多一些迭代。学习率影响非常大，不同的学习率会模型发生翻天覆地变化。学习率必须慎重。</description>
    </item>
    <item>
      <title>TensorFlow学习（十）：图像预处理</title>
      <link>https://anwangtanmi.github.io/posts/f8270ba74eed63ec85b3386afa1b056c/</link>
      <pubDate>Fri, 26 May 2017 14:47:43 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f8270ba74eed63ec85b3386afa1b056c/</guid>
      <description>更新时间：&#xA;2018.6.2 增加了通过 tf.image 进行数据增强的内容，非常重要，可以直接跳到第四节。&#xA;之前做的一些任务都是从.csv文件里面读取数据来处理，这些元素都已经是处理好的值了，所以很方便。但是更多时候，我们是要从硬盘上的图片直接来做处理，所以，这里需要用到一些基本的图像处理有关的函数了。OpenCV肯定是可以使用的，但是tensorflow本身也提供了一些好用的函数。 因为通过Tensorflow完成图像有关的任务太多了，所以了解一点Tensorflow中自带的图像处理有关的函数是很有必要的。 Tensorflow中内置的图像处理的函数肯定没有OpenCV那么多那么强大啦，但是仅仅是作为简单的预处理的话，完全是够用了。&#xA;主要使用的模块就是tf.image，所以首先要是先把官方文档列出来：Module: tf.image，然后接下来就是按照图片处理的顺序来分别讲解各个函数的使用。 本节的完整测试代码，可以在我的GitHub：LearningTensorFlow/12.ImageProcess/上找到。&#xA;一.图像的编解码 Ⅰ.概览 下面是tensorflow自带编解码部分的函数，这里一起列出来，但是并不会全部都详细讲，因为使用方式大同小异，在例子中只是详细讲其中一个，其他的都可以类比或者看文档写出来，实在是很简单，就不需要多花笔墨。&#xA;decode_gif(…): Decode the first frame of a GIF-encoded image to a uint8 tensor. decode_jpeg(…): Decode a JPEG-encoded image to a uint8 tensor. decode_png(…): Decode a PNG-encoded image to a uint8 or uint16 tensor. decode_image(…): Convenience function for decode_gif, decode_jpeg, and decode_png. encode_jpeg(…): JPEG-encode an image. encode_png(…): PNG-encode an image.&#xA;在这一步，要是只是想把某个或者某些个文件读到ndarray中去，推荐更加高效的做法，就是使用matplot.image中的imread（）方法，或者opencv中的方法，都是很简单无脑的。 比如在这里，我文件夹下面有个叫做“1.jpg”的文件，那么就可以用比较简单的方法得到： 二.数据转化和形状变换 这一步的目的是什么呢？首先，很多图像像素默认是int类型的，在tensorflow里面，float类型的数据更加适合处理，然后形状来说，我们知道，对于图片来说，一个网络的输入尺寸是固定的，而训练的时候图片的尺寸确不一定是固定的，所以有必要用各种方式把图片尺寸转换为固定的适合网络输入的格式。&#xA;Ⅰ.数据类型转化 convert_image_dtype(image,dtype,saturate=False,name=None)</description>
    </item>
    <item>
      <title>深度抠图–Deep Image Matting</title>
      <link>https://anwangtanmi.github.io/posts/feffcbfbf76bc6a95e98c5152ee06632/</link>
      <pubDate>Mon, 20 Mar 2017 14:25:14 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/feffcbfbf76bc6a95e98c5152ee06632/</guid>
      <description>CVPR2017 https://arxiv.org/abs/1703.03872&#xA;GitHub: https://github.com/Joker316701882/Deep-Image-Matting&#xA;抠图问题还是比较难的，简单的用一个公式表达如下： 左边是图像位置 i 的 RGB 值，右边是 前景和背景的线性组合。 matte estimation alpha 是未知的。对于每个像素，有3个已知量，7个未知量，所以这个一个 underconstrained 问题，即变量个数大于方程个数。&#xA;当前针对抠图问题的方法主要有两个问题： 1）当前方法将 matting equation 设计为两个颜色的线性组合，即将抠图看做一个 color problem染色问题，这种方法基于一个假设就是颜色是一个可区分的特征，distinguishing feature（通常还加入了位置信息）。但是当背景和前景的颜色空间分布重叠时，这种方法的效果就不是很好了。&#xA;2） 当前基于抠图的数据库很小。 the alphamatting.com dataset 只有27张训练图像，8张测试图像。&#xA;本文解决了上述两个问题。针对数据库问题，我们建立了一个大的抠图数据库。建立方式如下： 找一些背景比较单一的图像，这些图像的真值比较容易得到。将人扣出来，然后再将其放到背景比较复杂的图中去。&#xA;4 Our method 整个网络分两个部分，一个是 deep convolutional encoder-decoder network，is penalized by the alpha prediction loss and a novel compositional loss 输入图像块和对应的 trimap，输出 alpha prediction。第二部分是一个小的卷积网络用于 refines 前面个网络的输出 alpha prediction。&#xA;4.1. Matting encoder-decoder stage&#xA;Losses: 我们这里设计了两个 Losses： 第一个loss 是 the alpha-prediction loss，是 预测的 alpha values 和alpha values的真值的绝对差。第二个loss是 the compositional loss ，预测的RGB颜色值和对应的真值绝对差。.</description>
    </item>
    <item>
      <title>Ubuntu 16.04&#43;Gtx1050Ti&#43;cuda 8.0&#43;cudnn 5.1 tensorflow 安装</title>
      <link>https://anwangtanmi.github.io/posts/26932ed36ff12176105479bd37c6fe2e/</link>
      <pubDate>Fri, 17 Feb 2017 19:12:22 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/26932ed36ff12176105479bd37c6fe2e/</guid>
      <description>现在tensorflow 终于支持高版本（cuda 8.0）一键安装了，今天尝试了一下，很爽。不用折腾那么多东西。 建议安装 anaconda ,一个特别好的Python包管理器，清华大学的源有其镜像，安装速度，更新也挺快的。 其独有的虚拟环境可以将一个个环境隔离开来，互不影响，这个特别好。 我们首先安装anaconda ，这个官网有教程，整个安装过过程傻瓜化。 首先创建虚拟环境： conda create -n tensoflow python=3.7 conda create -n tensoflow python=3.4&#xA;conda create -n tensoflow python=3.5&#xA;根据习惯选一个Python版本。 tf的gpu版本现在支持直接用pip直接安装，前提是安装好cuda8.0和cudnnv5.1（必须是8.0和5.1不然会报错），然后直接在命令行下pip install tensorflow-gpu&#xA;这里说一下cuda 的安装，官网给出的安装教程前提是没有安装官方N卡驱动的。按照官网方法一步步来可以安装成功，并且很快，不需要选择就可以安装好，然后根据CUDA 的sample进行测试。一般都会安装成功&#xA;如果之前单独安装过N卡的驱动，就不要按照官网的来进行安装了，安装的时候，后面不写参数，然后到第二步骤的时候，选择不安装驱动，如果选择安装的话，会出错。</description>
    </item>
    <item>
      <title>图像处理之灰度模糊图像与彩色清晰图像的变换</title>
      <link>https://anwangtanmi.github.io/posts/ffcae77037d225b828a7f7991ca37860/</link>
      <pubDate>Sat, 24 Dec 2016 17:18:25 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ffcae77037d225b828a7f7991ca37860/</guid>
      <description>针对模糊图像的处理，个人觉得主要分两条路，一种是自我激发型，另外一种属于外部学习型。接下来我们一起学习这两条路的具体方式。&#xA;第一种 自我激发型 基于图像处理的方法，如图像增强和图像复原，以及曾经很火的超分辨率算法。都是在不增加额外信息的前提下的实现方式。 １.　图像增强&#xA;图像增强是图像预处理中非常重要且常用的一种方法，图像增强不考虑图像质量下降的原因，只是选择地突出图像中感兴趣的特征，抑制其它不需要的特征，主要目的就是提高图像的视觉效果。先上一张示例图： 图像增强中常见的几种具体处理方法为：&#xA;直方图均衡&#xA;在图像处理中，图像直方图表示了图像中像素灰度值的分布情况。为使图像变得清晰，增大反差，凸显图像细节，通常希望图像灰度的分布从暗到亮大致均匀。直方图均衡就是把那些直方图分布不均匀的图像（如大部分像素灰度集中分布在某一段）经过一种函数变换，使之成一幅具有均匀灰度分布的新图像，其灰度直方图的动态范围扩大。用于直方均衡化的变换函数不是统一的，它是输入图像直方图的积分，即累积分布函数。&#xA;灰度变换&#xA;灰度变换可使图像动态范围增大，对比度得到扩展，使图像清晰、特征明显，是图像增强的重要手段之一。它主要利用图像的点运算来修正像素灰度，由输入像素点的灰度值确定相应输出像素点的灰度值，可以看作是“从像素到像素”的变换操作，不改变图像内的空间关系。像素灰度级的改变是根据输入图像f(x,y)灰度值和输出图像g(x,y)灰度值之间的转换函数g(x，y)=T[f(x，y)]进行的。 灰度变换包含的方法很多，如逆反处理、阈值变换、灰度拉伸、灰度切分、灰度级修正、动态范围调整等。&#xA;图像平滑&#xA;在空间域中进行平滑滤波技术主要用于消除图像中的噪声，主要有邻域平均法、中值滤波法等等。这种局部平均的方法在削弱噪声的同时，常常会带来图像细节信息的损失。 邻域平均，也称均值滤波，对于给定的图像f(x,y)中的每个像素点(x,y)，它所在邻域S中所有M个像素灰度值平均值为其滤波输出，即用一像素邻域内所有像素的灰度平均值来代替该像素原来的灰度。 中值滤波，对于给定像素点(x,y)所在领域S中的n个像素值数值{f1,f2,…,fn}，将它们按大小进行有序排列，位于中间位置的那个像素数值称为这n个数值的中值。某像素点中值滤波后的输出等于该像素点邻域中所有像素灰度的中值。中值滤波是一种非线性滤波，运算简单，实现方便，而且能较好的保护边界。&#xA;图像锐化&#xA;采集图像变得模糊的原因往往是图像受到了平均或者积分运算，因此，如果对其进行微分运算，就可以使边缘等细节信息变得清晰。这就是在空间域中的图像锐化处理，其的基本方法是对图像进行微分处理，并且将运算结果与原图像叠加。从频域中来看，锐化或微分运算意味着对高频分量的提升。常见的连续变量的微分运算有一阶的梯度运算、二阶的拉普拉斯算子运算，它们分别对应离散变量的一阶差分和二阶差分运算。&#xA;２.　图像复原&#xA;其目标是对退化（传播过程中的噪声啊，大气扰动啊好多原因）的图像进行处理，尽可能获得未退化的原始图像。如果把退化过程当一个黑匣子（系统H），图片经过这个系统变成了一个较烂的图。这类原因可能是光学系统的像差或离焦、摄像系统与被摄物之间的相对运动、电子或光学系统的噪声和介于摄像系统与被摄像物间的大气湍流等。图像复原常用二种方法。当不知道图像本身的性质时，可以建立退化源的数学模型，然后施行复原算法除去或减少退化源的影响。当有了关于图像本身的先验知识时，可以建立原始图像的模型，然后在观测到的退化图像中通过检测原始图像而复原图像。 ３.　图像超分辨率　一张图我们想脑补细节信息好难，但是相似的多幅图我们就能互相脑洞了。所以，我们可以通过一系列相似的低分辨图来共同脑补出一张高清晰图啊，有了这一张犯罪人的脸，我就可以画通缉令了啊。。。 超分辨率复原技术的目的就是要在提高图像质量的同时恢复成像系统截止频率之外的信息，重建高于系统分辨率的图像。继续说超分辨，它其实就是根据多幅低质量的图片间的关系以及一些先验知识来重构一个高分辨的图片。示例图如下： 第二种 外部学习型 外部学习型，就如同照葫芦画瓢一样的道理。其算法主要是深度学习中的卷积神经网络，我们在待处理信息量不可扩充的前提下（即模糊的图像本身就未包含场景中的细节信息），可以借助海量的同类数据或相似数据训练一个神经网络，然后让神经网络获得对图像内容进行理解、判断和预测的功能，这时候，再把待处理的模糊图像输入，神经网络就会自动为其添加细节，尽管这种添加仅仅是一种概率层面的预测，并非一定准确。&#xA;本文介绍一种在灰度图像复原成彩色RGB图像方面的代表性工作：《全局和局部图像的联合端到端学习图像自动着色并且同时进行分类》。利用神经网络给黑白图像上色，使其变为彩色图像。稍作解释，黑白图像，实际上只有一个通道的信息，即灰度信息。彩色图像，则为RGB图像（其他颜色空间不一一列举，仅以RGB为例讲解），有三个通道的信息。彩色图像转换为黑白图像极其简单，属于有损压缩数据；反之则很难，因为数据不会凭空增多。&#xA;搭建一个神经网络，给一张黑白图像，然后提供大量与其相同年代的彩色图像作为训练数据（色调比较接近），然后输入黑白图像，人工智能按照之前的训练结果为其上色，输出彩色图像，先来看一张效果图： 本文工作 • 用户无干预的灰度图像着色方法。 • 一个新颖的端到端网络，联合学习图像的全局和局部特征。 • 一种利用分类标签提高性能的学习方法。 • 基于利用全局特征的风格转换技术。 • 通过用户研究和许多不同的例子深入评估模型，包括百年的黑白照片。&#xA;着色框架 模型框架包括四个主要组件：低级特征提取网络，中级特征提取网络，全局特征提取网络和着色网络。 这些部件都以端对端的方式紧密耦合和训练。 模型的输出是图像的色度，其与亮度融合以形成输出图像。 与另外两个工作对比&#xA;• Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning Representations for Automatic Colorization. In ECCV 2016. •Richard Zhang, Phillip Isola, and Alexei A.</description>
    </item>
    <item>
      <title>Drools6.5部署Drools Workbench</title>
      <link>https://anwangtanmi.github.io/posts/075f8ac3f5cada8758ebb95a7abcc91e/</link>
      <pubDate>Thu, 08 Dec 2016 23:32:11 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/075f8ac3f5cada8758ebb95a7abcc91e/</guid>
      <description>最近两天看了一下规则引擎drools，看官方介绍所以手痒，本地部署了一下Drools Workbench，这一下部署让我掉进了深坑，不得不说，这个配置有点麻烦，请听我细细道来。&#xA;安装环境 jdk tomcat mysql 本地部署Drools Workbench首先我门得有一个tomcat。和mysql 数据库 这是基础，关于tomcat的安装使用我就不多说了。&#xA;其他的还需要这些步骤：&#xA;1.下载kie-drools-wb-6.5.0.Final-tomcat7.war 包 2.给tomcat添加所需依赖 3.添加配置文件btm-config.properties 4.添加配置文件resources.properties 5.修改配置文件context.xml 6.修改配置文件tomcat-users.xm 7.修改配置文件server.xml 8.添加脚本setenv.sh 9.修改配置文件persistence.xml 10.启动验证 开始部署 1. kie-drools-wb-6.5.0.Final-tomcat7.war 进入官网 传送门，不信你不点 下载kie-drools-wb-6.5.0.Final-tomcat7.war 下载后解压war包，并修改名字为kie-drools-wb 将其放在tomcat的webapp目录下&#xA;2.给tomcat添加所需依赖 将所需的jar包添加到tomcat下的lib 目录下 所需jar包地址，点我下载jar，不信你不点&#xA;copy following libs into TOMCAT_HOME/lib * btm-2.1.4.jar * btm-tomcat55-lifecycle-2.1.4.jar * h2-1.3.161.jar * jta-1.1.jar * slf4j-api-1.7.2.jar * slf4j-jdk14-1.7.2.jar&#xA;3.添加配置文件btm-config.properties 在tomcat的conf文件夹下新建文件btm-config.properties，并添加如下配置：&#xA;切记/Users/yangyibo/Software/apache-tomcat-7.0.70/ 改为你自己的tomcat地址。&#xA;bitronix.tm.serverId=tomcat-btm-node0 bitronix.tm.journal.disk.logPart1Filename=/Users/yangyibo/Software/apache-tomcat-7.0.70/work/btm1.tlog bitronix.tm.journal.disk.logPart2Filename=/Users/yangyibo/Software/apache-tomcat-7.0.70/work/btm2.tlog bitronix.tm.resource.configuration=/Users/yangyibo/Software/apache-tomcat-7.0.70/conf/resources.properties 4.添加配置文件resources.properties 在tomcat的conf文件夹下新建文件resources.properties，并添加如下配置：&#xA;切记 resource.ds1.driverProperties.user=root resource.ds1.driverProperties.password=admin 改为你自己的mysql 账号密码。并在本地的mysql数据库中创建一个名字为 drools 的数据库&#xA;resource.ds1.className=bitronix.tm.resource.jdbc.lrc.LrcXADataSource resource.</description>
    </item>
    <item>
      <title>图像检索系列一：Deep Learning of Binary Hash Codes for Fast Image Retrieval</title>
      <link>https://anwangtanmi.github.io/posts/d1b35c65fb111d5c9969c067b1d95ca1/</link>
      <pubDate>Wed, 23 Nov 2016 14:44:06 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d1b35c65fb111d5c9969c067b1d95ca1/</guid>
      <description>Deep Learning of Binary Hash Codes for Fast Image Retrieval 这篇文章发表在2015CVPR workshop&#xA;文章链接：http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf&#xA;代码链接：https://github.com/kevinlin311tw/caffe-cvprw15&#xA;图一 算法框架流程&#xA;这篇文章的想法很巧妙，在一个深层CNN的最后一个全连接层（fc8）和倒数第二个全连接层（fc7）之间加了一层全连接隐层，就是图一中绿色的latent layer （H）。这样一来，既可以得到深层的CNN特征，文中主要用的是fc7的特征，还可以得到二分的哈希编码，即来自H。这个隐层H不仅是对fc7的一个特征概括，而且是一个连接CNN网络的中层特征与高层特征的桥梁。&#xA;1. Domain Adaption&#xA;为了让一个网络能够对某一类物体高鲁棒，即target domain adaption，用一类主题目标数据集来整定(fine-tune)整个网络。fc8的节点数由目标类别数决定，H的节点数在文中有两种尝试：48和128。这两个层在fine-tune时，是随机初始化的，其中H的初始化参考了LSH[1]的方法，即通过随机映射来构造哈希位。通过这样训练，得到的网络能够产生对特定物体的描述子以及对应的哈希编码。&#xA;2. Image Retrieval&#xA;主要提出了一种从粗糙到细致的检索方案（coarse-to-fine）。H层首先被二值化：&#xA;粗糙检索是用H层的二分哈希码，相似性用hamming距离衡量。待检索图像设为I，将I和所有的图像的对应H层编码进行比对后，选择出hamming距离小于一个阈值的m个构成一个池，其中包含了这m个比较相似的图像。&#xA;细致检索则用到的是fc7层的特征，相似性用欧氏距离衡量。距离越小，则越相似。从粗糙检索得到的m个图像池中选出最相似的前k个图像作为最后的检索结果。每两张图128维的H层哈希码距离计算速度是0.113ms，4096维的fc7层特征的距离计算需要109.767ms，因此可见二值化哈希码检索的速度优势。&#xA;3. 实验结果&#xA;作者在MINIST，CIFAR-10，YAHOO-1M三个数据集上做了实验，并且在分类和检索上都做了实验，结果都很不错，特别是在CIFAR-10上图像检索的精度有30%的提升。&#xA;（1）MINIST&#xA;左边第一列是待检索图像，右边是48和128位H层节点分别得到的结果。可以看到检索出的数字都是正确的，并且在这个数据集上48位的效果更好，128位的太高，容易引起过拟合。&#xA;（2）CIFAR-10&#xA;在这个数据集上128位的H层节点比48位的效果更好，比如128检索出更多的马头，而48位的更多的全身的马。&#xA;（3）YAHOO-1M&#xA;作者在这个数据集上比较了只用fc7,只用H和同时用两者（粗糙到细致）的结果，实验结果表明是两者都用的效果更好。&#xA;可以看到如果只用alexnet而不进行fine-tune的话，检索出的结果精度很低。&#xA;4. 总结&#xA;这个方法整篇文章看下来给人的感觉比较工程，全篇讲理论和方法的部分很少，几乎没有什么数学公式，但是效果好，这个最重要。想法很简单，但是很巧妙，值得学习。代码已经开源，准备尝试。&#xA;[1] Gionis A, Indyk P, Motwani R. Similarity search in high dimensions via hashing[C]//VLDB. 1999, 99(6): 518-529.</description>
    </item>
    <item>
      <title>我和我的小伙伴们</title>
      <link>https://anwangtanmi.github.io/posts/799b613371ac556bdfc58538c31682fb/</link>
      <pubDate>Fri, 27 May 2016 20:29:54 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/799b613371ac556bdfc58538c31682fb/</guid>
      <description>Who am I 我只是一个普普通通的OI.&#xA;我的学校：Sun Yat-Sen Memorial Middle School&#xA;年级：G2&#xA;My Partners DDX&#xA;LJF&#xA;LYZ（这人非常牛！)&#xA;XHM（大学霸）&#xA;YYK&#xA;HHN&#xA;HXY&#xA;CTY（这个人非常的牛%%%%%%）&#xA;ZZ&#xA;LH&#xA;CH&#xA;HZJ&#xA;LYD&#xA;LZH&#xA;Infleaking&#xA;DH&#xA;LYH&#xA;WYT</description>
    </item>
    <item>
      <title>Deep Reinforcement Learning 深度增强学习资源</title>
      <link>https://anwangtanmi.github.io/posts/3bfcee145c23d6c947c1cd9afaf19504/</link>
      <pubDate>Sun, 24 Jan 2016 10:35:12 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3bfcee145c23d6c947c1cd9afaf19504/</guid>
      <description>1 学习资料 增强学习课程 David Silver （有视频和ppt）:&#xA;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&#xA;最好的增强学习教材：&#xA;Reinforcement Learning: An Introduction&#xA;https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html&#xA;深度学习课程 （有视频有ppt有作业）&#xA;https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/&#xA;深度增强学习的讲座都是David Silver的：&#xA;ICLR 2015 part 1 https://www.youtube.com/watch?v=EX1CIVVkWdE&#xA;ICLR 2015 part 2 https://www.youtube.com/watch?v=zXa6UFLQCtg&#xA;UAI 2015 https://www.youtube.com/watch?v=qLaDWKd61Ig&#xA;RLDM 2015 http://videolectures.net/rldm2015_silver_reinforcement_learning/&#xA;其他课程：&#xA;增强学习&#xA;Michael Littman:&#xA;https://www.udacity.com/course/reinforcement-learning–ud600&#xA;AI（包含增强学习，使用Pacman实验）&#xA;Pieter Abbeel：&#xA;https://www.edx.org/course/artificial-intelligence-uc-berkeleyx-cs188-1x-0#.VKuKQmTF_og&#xA;Deep reinforcement Learning:&#xA;Pieter Abbeel&#xA;http://rll.berkeley.edu/deeprlcourse/&#xA;高级机器人技术（Advanced Robotics）：&#xA;Pieter Abbeel：&#xA;http://www.cs.berkeley.edu/~pabbeel/cs287-fa15/&#xA;深度学习相关课程：&#xA;用于视觉识别的卷积神经网络（Convolutional Neural Network for visual network)&#xA;http://cs231n.github.io/&#xA;机器学习 Machine Learning&#xA;Andrew Ng&#xA;https://www.coursera.org/learn/machine-learning/&#xA;http://cs229.stanford.edu/</description>
    </item>
    <item>
      <title>基于深度学习的图像去噪（论文总结）</title>
      <link>https://anwangtanmi.github.io/posts/4c99d94ed29bc9dcccc1de28ae0e37b3/</link>
      <pubDate>Sun, 24 Jan 2016 00:24:47 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4c99d94ed29bc9dcccc1de28ae0e37b3/</guid>
      <description>2015&#xA;深度学习、自编码器、低照度图像增强&#xA;Lore, Kin Gwn, Adedotun Akintayo, and Soumik Sarkar. “LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement.” arXiv preprint arXiv:1511.03995 (2015).&#xA;利用深度学习的自编码器方法训练不同低照度图像信号的特征来实现自适应变亮和去噪，主要是通过非线性暗化和添加高斯噪声的方法来模拟低照度环境，进行图像对比度增强和去噪。&#xA;2014&#xA;深度学习、深度卷积神经网络、图像去卷积&#xA;Xu, Li, et al. “Deep convolutional neural network for image deconvolution.”Advances in Neural Information Processing Systems. 2014.&#xA;利用深度卷积神经网络进行图像去卷积，实现图像复原，优点：相比于当前其他方法，有更好的PSNR值和视觉效果。&#xA;2014&#xA;深度学习、稀疏编码、自编码器、图像去噪&#xA;Li, HuiMing. “Deep Learning for Image Denoising.” International Journal of Signal Processing, Image Processing and Pattern Recognition 7.3 (2014): 171-180.&#xA;利用稀疏编码（sparsecoding）与自编码器（Auto-encoder）两种方法结合来实现图像去噪，不足之处是只对图像进行处理，没有涉及视频。&#xA;2014</description>
    </item>
    <item>
      <title>Deep Learning Face Attributes in the Wild</title>
      <link>https://anwangtanmi.github.io/posts/d4f32ca6487e3e881c6e8395c8b21b72/</link>
      <pubDate>Fri, 09 Jan 2015 11:43:28 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d4f32ca6487e3e881c6e8395c8b21b72/</guid>
      <description>文章要解决的问题&#xA;Predicting face attributes from web images&#xA;方法的主要想法&#xA;It cascades two CNNs (LNet and ANet) forface localization and attribute prediction respectively.&#xA;贡献（吹牛逼）&#xA;(1) It shows how LNet and ANet can be improved by different pre-trainingstrategies.&#xA;(2) It reveals that although filters of LNet are fine-tuned by attributelabels, their response maps over the entire image have strong indication offace’s location.&#xA;(3)It also demonstrates that the high-level hidden neurons of ANetautomatically discover semantic concepts after pretraining, and such concepts aresignificantly enriched after fine-tuning.</description>
    </item>
    <item>
      <title>deep learning 源代码集锦</title>
      <link>https://anwangtanmi.github.io/posts/861b819aaebdf86f9866971cab5e3f3b/</link>
      <pubDate>Mon, 21 Apr 2014 16:58:55 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/861b819aaebdf86f9866971cab5e3f3b/</guid>
      <description>首先推荐几个deep learning工具箱 1. Hinton DBN code 这个不用说了，开山鼻祖Hinton的代码，下载链接：Hinton DBN coce。代码可读性一般，后面我会把我修改后的代码公布出来。&#xA;2. Deep learning toolbox Mathworks上的代码，纯正Matlab代码，CNN部分写的不错，下载链接：DL Toolbox matlab。&#xA;3. Covolutional DBN Github项目主页：CDBN 科研机构 1. 香港中文大学 http://mmlab.ie.cuhk.edu.hk/ Honglak Lee Ph.D. in Computer Science, Stanford University, 2010 1. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. [code]&#xA;2. Unsupervised feature learning for audio classification using convolutional deep belief networks. [code]&#xA;工具包：&#xA;1.Matlab Environment for Deep Architecture Learning (MEDAL) [code]&#xA;包含：</description>
    </item>
    <item>
      <title>deepnet: deep learning toolkit in R</title>
      <link>https://anwangtanmi.github.io/posts/6e8675d35fb5b92e5b13ce97c3735130/</link>
      <pubDate>Thu, 20 Mar 2014 23:00:25 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/6e8675d35fb5b92e5b13ce97c3735130/</guid>
      <description>前段农闲时间，用R实现了些neural network和deep learning相关的算法，把这些代码做了个package叫deepnet,而且已经上传到CRAN上了。有兴趣的同学可以安装玩玩（R中执行命令:install.packages(“deepnet”)）.package介绍文档：http://cran.r-project.org/web/packages/deepnet/index.html&#xA;​&#xA;已经​实现的算法包括bp, rbm训练，deep belief net, deep auto-encoder。后续有时间的话打算接着实现cnn和rnn。&#xA;​&#xA;​本来打算做第一个deep learning的R package, 结果被一个叫Martin Drees的哥们捷足先登了，比我早两天发布了一个叫darch的package.各种不服啊。</description>
    </item>
    <item>
      <title>Deep Learning源代码收集-持续更新…</title>
      <link>https://anwangtanmi.github.io/posts/97ee17eabbc06ab6a9e1a85b850e4cc1/</link>
      <pubDate>Sun, 22 Sep 2013 23:25:37 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/97ee17eabbc06ab6a9e1a85b850e4cc1/</guid>
      <description>Deep Learning源代码收集-持续更新…&#xA;[email protected]&#xA;http://blog.csdn.net/zouxy09&#xA;收集了一些Deep Learning的源代码。主要是Matlab和C++的，当然也有python的。放在这里，后续遇到新的会持续更新。下表没有的也欢迎大家提供，以便大家使用和交流。谢谢。&#xA;最近一次更新：2013-9-22&#xA;Theano&#xA;http://deeplearning.net/software/theano/&#xA;code from: http://deeplearning.net/&#xA;Deep Learning Tutorial notes and code&#xA;https://github.com/lisa-lab/DeepLearningTutorials&#xA;code from: lisa-lab&#xA;A Matlab toolbox for Deep Learning&#xA;https://github.com/rasmusbergpalm/DeepLearnToolbox&#xA;code from: RasmusBerg Palm&#xA;deepmat&#xA;Matlab Code for Restricted/Deep BoltzmannMachines and Autoencoder&#xA;https://github.com/kyunghyuncho/deepmat&#xA;code from: KyungHyun Cho http://users.ics.aalto.fi/kcho/&#xA;Training a deep autoencoder or a classifieron MNIST digits&#xA;http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html&#xA;code from: Ruslan Salakhutdinov and GeoffHinton&#xA;CNN – Convolutional neural network class</description>
    </item>
    <item>
      <title>Tor源码分析十一 — 客户端执行流程（网络信息的下载续）</title>
      <link>https://anwangtanmi.github.io/posts/2de66a9b262792478ecbe11ea3b138f3/</link>
      <pubDate>Tue, 25 Jun 2013 21:31:26 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2de66a9b262792478ecbe11ea3b138f3/</guid>
      <description>通过上一节中我们对连接和链路的重新描述，我们可以继续进行源码的分析了。在本节中，我们会开始着重讲述链路的建立，以及链路所基于的OR连接的建立，同时还有部分Libevent调度的再度分析。大家会明白，进行到此处之时，我们已经开始接触Tor系统最底层，最深藏着的连接机制以及调度机制。这个部分，是整个系统的精髓。后期几乎所有的应用请求连接处理等，都是重复地使用该部分的代码。&#xA;1. 链路建立以及OR连接的开始，circuit_establish_circuit 我们在此处重新分析下链路建立的函数：&#xA;/** Build a new circuit for purpose. If exit * is defined, then use that as your exit router, else choose a suitable * exit node. * * Also launch a connection to the first OR in the chosen path, if * it&#39;s not open already. */ origin_circuit_t * circuit_establish_circuit(uint8_t purpose, extend_info_t *exit, int flags) { origin_circuit_t *circ; int err_reason = 0; circ = origin_circuit_init(purpose, flags); //链路初始化； if (onion_pick_cpath_exit(circ, exit) &amp;lt; 0 || //选取链路出口结点； onion_populate_cpath(circ) &amp;lt; 0) { //选取链路入口结点及中间结点； .</description>
    </item>
    <item>
      <title>Tor源码分析十 — 连接和链路</title>
      <link>https://anwangtanmi.github.io/posts/b4625b290a2fd620f10686ef483485a8/</link>
      <pubDate>Fri, 21 Jun 2013 22:01:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b4625b290a2fd620f10686ef483485a8/</guid>
      <description>源码分析到这个部分，为了让大家明白源码中的编码逻辑，不得不开始从头梳理程序内部的复杂连接和链接组织形式。否则大家后期会更加一头雾水。笔者开始分析源码之时，没有这些宏观的概念，只能死嚼代码，硬猜硬想，再加以检查代码进行验证，才得以明白程序的主要框架逻辑。如果再以猜测验证的模式向大家讲述源码，必定会越来越混乱。所以，在本节之中，我们会将系统中所有的连接类型，链路类型和他们之间的关系和代码之中的关联方式尽量讲明。若大家遇到不明晰的部分，可以参照代码进行查阅。&#xA;在我们进行详细分析之前，先再次给出连接和链接的框架位置图。这个简单的层次图帮助我们理解不同连接和链接所处层次的位置关系，其实已经在我们分析OR连接源码之时给出，之时当时没有进行过多的深入介绍。&#xA;DIR连接，LISTENER连接　|&#xA;－－－－－－－－－－－－－－－－－　|&#xA;AP连接，EXIT连接……　Tor协议上层 |&#xA;－－－－－－－－－－－－－－－－－－－－－－－－－－－－ |　应用层&#xA;Circuit链路……　Tor协议中层 |&#xA;－－－－－－－－－－－－－－－－－－－－－－－－－－－－ |&#xA;OR连接……　Tor协议下层 |&#xA;－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－&#xA;TLS连接　传输层&#xA;1. 连接 很明显，系统中的连接类型多种多样，各有各自的不同职能。在此处，我们先罗列出所有系统中存在的连接，其后再对一些我们比较重视的连接类型进行相关说明。&#xA;//OR监听连接：本地用于监听远端传来的OR请求，为每个新请求建立一个OR连接；OR监听连接本地全局只有一个； #define CONN_TYPE_OR_LISTENER 3 //OR连接：基于TLS连接的，主要负责Tor系统内主机之间相互通信的连接；OR连接数量表征本地与多少台主机建立了互联关系，因为他们是一一对应的； #define CONN_TYPE_OR 4 //EXIT连接：翻译为出口连接是因为该连接会与远端服务器建立socket连接并传递数据； #define CONN_TYPE_EXIT 5 //AP监听连接：本地用于监听本地应用程序的服务请求而设立的监听连接，为每个服务请求建立一个AP连接；AP监听连接本地全局只有一个； #define CONN_TYPE_AP_LISTENER 6 //AP连接：基于链路的，主要负责为客户端请求寻找合适链路和传递数据；AP连接数量表征本地应用程序发出的连接请求数量，因为他们是一一对应的； #define CONN_TYPE_AP 7 //DIR监听连接：Tor系统目录服务器用于监听Tor系统内主机发出的目录相关请求，也就是说该类型连接只在目录服务器上存在； #define CONN_TYPE_DIR_LISTENER 8 //DIR连接：Tor客户端向Tor目录服务器发送目录请求时需要新建的连接，该连接需要通过AP连接为其转发请求，也就是说类似于普通应用程序的请求连接； #define CONN_TYPE_DIR 9 //CPUWORKER连接：用于在程序开启多进程解密服务的时候提供进程间通信，详细过程可以参看前面分析过的cpuworker部分文章；正常情况下未被使用； #define CONN_TYPE_CPUWORKER 10 //CONTROL监听连接：本地用于监听本地应用程序传递的控制指令或请求；（后期专门讲述，此处暂略） #define CONN_TYPE_CONTROL_LISTENER 11 //CONTROL连接：本地用于控制消息处理的连接；（后期专门讲述，此处暂略） #define CONN_TYPE_CONTROL 12 //以下三种连接由于非常少见，此处我们暂时略去； #define CONN_TYPE_AP_TRANS_LISTENER 13 #define CONN_TYPE_AP_NATD_LISTENER 14 #define CONN_TYPE_AP_DNS_LISTENER 15 很明显地，通过分析上面这些连接类型我们发现，对于客户端而言，最重要的连接类型无非就是DIR,AP,OR三种连接。这些连接之间的相互关联和作用帮助客户端应用程序将应用请求送入Tor系统，并再通过Tor系统的封装等操作将数据成功送出到远端目的地。下面我们来简要描述下整个系统运转的过程。 1.</description>
    </item>
    <item>
      <title>Tor源码分析九 — 客户端执行流程（网络信息的下载）</title>
      <link>https://anwangtanmi.github.io/posts/71f56c6ca8c9f50ee1bf2bce976f4026/</link>
      <pubDate>Tue, 28 May 2013 14:58:24 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/71f56c6ca8c9f50ee1bf2bce976f4026/</guid>
      <description>源码分析到这里，大家应该已经大致了解到Tor系统的前期启动没有做任何的下载操作。前期启动的最关键环节，就是正常开启Libevent的调度机制，从而有条不紊地进行系统内所有子模块的维护等。我们需要再次强调的是，系统的主进程内，是没有做任何直接的获取网络状态，获取路由描述符，获取额外路由信息的操作。系统将这些操作视为需要实时维护的工作，因为所有这些网络信息都有其时限。所以，对这些网络信息的获取，全部置于秒回调函数之后进行。秒回调函数能够控制对这些信息的检测和获取，这些在前一节中已经有部分描述。&#xA;我们将在本文中再次重申秒回调函数中最重要的执行线路，以弄清Tor系统是如何获取网络信息，从而开启链路以至于完成整个系统的成功建立。由于本文中所提到的许多函数嵌套层次非常之多，我们只针对最重要的函数加以说明；同时，我们会略去许多不被执行的函数。这些不被执行的函数，大多情况下是因为执行条件不满足而未能够通过函数内部执行前检测，或更甚者未能通过函数外部的判断语句检测。关于这些情况，请大家自行查看函数细节，此处就不再一个一个详细解释。&#xA;1. update_networkstatus_downloads 前面介绍的秒回调函数之中最重要的维护函数即为事件调度函数。事件调度函数中，大部分的内容由于与时间相关而不会在短期内执行。具体的执行间隔可以参照之前章节中源码的注释。在简要的对该函数进行分析之后，我们发现，客户端系统刚刚启动之时，最终重点调用的代码如下：&#xA;static void run_scheduled_events(time_t now) { ...... /* 2b. Once per minute, regenerate and upload the descriptor if the old * one is inaccurate. */ ...... if (time_to_check_descriptor &amp;lt; now &amp;amp;&amp;amp; !options-&amp;gt;DisableNetwork) { //每一分钟进行一次网络状态的下载检测 ...... time_to_check_descriptor = now + CHECK_DESCRIPTOR_INTERVAL; ...... /* Also, once per minute, check whether we want to download any * networkstatus documents. */ update_networkstatus_downloads(now); //开启网络状态的下载 } ...... } 以下为网络状态下载函数的函数体：&#xA;/** Launch requests for networkstatus documents and authority certificates as * appropriate.</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Cpuworker</title>
      <link>https://anwangtanmi.github.io/posts/82be7d2dea5ed17ad5cf868ea403ef5a/</link>
      <pubDate>Fri, 24 May 2013 12:12:28 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/82be7d2dea5ed17ad5cf868ea403ef5a/</guid>
      <description>我们知道，对于Tor的服务器来说，有的时候因为其访问量巨大，不得不采取一些相应机制来保证服务的正常提供。在服务器编程里，我们经常可以用到的技术，例如线程池，多路复用等。Tor程序，在大多数情况下，都是单进程运行的，几乎没有哪里用到多线程的操作。正因为如此，Tor的主进程才绝对不允许出现阻塞式的操作。但是，唯独在一处，Tor为了提高自身效率，利用了线程池类似的机制。这个部分就是Cpuworker。本文就主要介绍该模块的作用和实现机制。&#xA;简单的说，Cpuworker存在的目的，是为了利用线程池的机制分担Tor主进程的压力，帮助其在接收到CREATE请求时计算对称密钥。下面进行具体的过程描述：&#xA;1. 系统启动时根据主机CPU数量，初始化cpuworker线程池；（linux中线程和进程基本无差别）&#xA;cpuworkers_rotate()&#xA;1.1 根据配置文件配置选项，自适应地检测CPU数量或固定设置CPU数量；&#xA;spawn_enough_cpuworkers()&#xA;1.2 根据CPU数量，开启cpuworker线程；（最大数目为16，最小数目为1）&#xA;spawn_cpuworker()&#xA;1.3 开启线程之前，创建sockpair，创建cpuworker连接，关联cpuworker连接与sockpair[0]；&#xA;1.4 开启线程之时，设置线程执行函数cpuworker_main，关联线程与sockpair[1]；&#xA;1.5 开启线程之后，将cpuworker连接加入系统连接池，同时从此线程与Tor主进程之间的通信方式为sockpair，类似socket的读写操作；&#xA;2. 在成功完成以上操作之后，系统达到如下效果：&#xA;2.1 当Tor主进程收到某主机发来的CREATE包时，检查连接池内是否有空闲的cpuworker连接；&#xA;assign_onionskin_to_cpuworker()&#xA;2.2 若有空闲cpuworker连接，则将CREATE包内容写入该连接；&#xA;connection_write_to_buf()&#xA;2.2.1 写入该连接则会激活该连接，使其将数据进一步写到sockpair[0]内，亦即传递给对应的线程，线程利用sockpair[1]来读取数据；&#xA;2.2.2 线程的主函数是阻塞式得等待数据，一旦数据到达，则开始处理，并在处理完毕之后，将结果写回sockpair[1]，亦即传递回主进程cpuworker连接；&#xA;cpuworker_main()&#xA;2.2.3 主进程处理cpuworker读事件就是根据对应的链路返回对应的CREATED包，其中包括DH密钥交换协议第二部分密钥和生成的对称密钥的摘要等；&#xA;2.3 若无空闲cpuworker连接，则将CREATE包挂起，在适当时候再写入空闲连接；&#xA;onion_pending_add()&#xA;上述整个过程，省略了很多细节部分，大家各自参照原函数进行进一步分析和理解。此处对几点再进行强调：Tor系统的连接多种多样，我们前面提过AP连接等内部连接，实际上Cpuworker连接也是内部连接，虽然他是用sockpair来完成的；sockpair是一种进程间通信机制，在众多的通信机制中，这种机制对Tor系统最为合适，所以选用了这种方式；Cpuworker线程的主要工作内容是对称密钥的生成。&#xA;这里我们再对称密钥的生成进行说明：&#xA;1. 在链路建立的过程中，Tor服务器应该首先接收到CREATE包。CREATE包的负载部分具有如下格式：（针对TAP握手方式）&#xA;Payload := PK(Padding || Symmetric key || First part of g^x) || SK(Second part of g^x)&#xA;PK：利用服务器的Onion Key进行公钥加密；&#xA;SK：利用Symmetric Key进行对称加密；&#xA;Padding：填充字节，长度一般为42B&#xA;Symmetric Key：用于加密第二部分内容的对称密钥；&#xA;g^x：DH密钥交换协议第一部分密钥材料。&#xA;2. 服务器接收到CREATE包之后，就要决定DH密钥交换协议的第二部分密钥材料g^y，从而计算出对称密钥。就是因为这个部分的密钥操作过程稍微会耗上一部分的时间，不适合在Tor主进程中进行操作，所以Tor程序利用cpuworker机制，开启线程专门为这种操作提供服务。在处理玩这些操作之后，Tor主进程生成CREATED包，返回给指定的链路。CREATED包的结构如下：&#xA;Payload := DH_key || Digest</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Log</title>
      <link>https://anwangtanmi.github.io/posts/1b65896c1e213bc78f7cd105eb5077d7/</link>
      <pubDate>Wed, 22 May 2013 20:29:11 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1b65896c1e213bc78f7cd105eb5077d7/</guid>
      <description>日志模块是Tor系统中一个非常重要的部件。它将Tor系统中的所有事件，分成不同的严重级别，分成不同的系统域，进行统一的日志处理。同时它还维护着一个日志记录链表。日志记录链表内存储的是所有日志需要输出的目标日志文件或目标日志输出位置。下文中我们会详细地对日志模块进行分析，并简要说明源文件中的各函数的简单作用。&#xA;1. 严重等级和域 日志模块内定义了5个严重等级，其具体的设定如下：&#xA;/** Debug-level severity: for hyper-verbose messages of no interest to * anybody but developers. */ #define LOG_DEBUG 7 /** Info-level severity: for messages that appear frequently during normal * operation. */ #define LOG_INFO 6 /** Notice-level severity: for messages that appear infrequently * during normal operation; that the user will probably care about; * and that are not errors. */ #define LOG_NOTICE 5 /** Warn-level severity: for messages that only appear when something has gone * wrong.</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Control</title>
      <link>https://anwangtanmi.github.io/posts/43f30972e8e6e824cf64e838765f996d/</link>
      <pubDate>Wed, 22 May 2013 16:13:03 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/43f30972e8e6e824cf64e838765f996d/</guid>
      <description>传统上我们所指的系统信号，就是系统传递给进程的控制信号。例如，SIGINT，SIGTERM等等。这些信号的传递，通常都是指示相关进程完成相应的操作，或暂停或终止，或其他一些操作。在Tor系统中，信号分为两种：传统信号，控制器信号。下面代码段是这些信号的简单罗列： /* These signals are defined to help handle_control_signal work. */ #ifndef SIGHUP #define SIGHUP 1 #endif #ifndef SIGINT #define SIGINT 2 #endif #ifndef SIGUSR1 #define SIGUSR1 10 #endif #ifndef SIGUSR2 #define SIGUSR2 12 #endif #ifndef SIGTERM #define SIGTERM 15 #endif /* Controller signals start at a high number so we don&#39;t * conflict with system-defined signals. */ #define SIGNEWNYM 129 #define SIGCLEARDNSCACHE 130 这些信号，是程序最主要关心的信号。他们都有相应的响应处理，其处理函数均为信号回调函数中调用的process_signal函数。在前面的小节中，我们已经提到过了关于信号事件的处理机制，这里就不再多说。但是，值得注意的是，当时的信号事件注册，并没有注册SIGNEWNYM信号和SIGCLEARDNSCACHE信号。也就是说，这两个信号并非由信号处理事件激活的。那他们是出现在何处的呢？答案是控制连接信号传递。&#xA;我们暂且将这个部分的内容搁置在这里，标题改成Control。在后期分析完成控制连接之后，将这个系统控制连接的主要作用和用法在这里进行详细描述。&#xA;未完待续……</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Hibernation</title>
      <link>https://anwangtanmi.github.io/posts/afce98dc8c1575d91f00b0a6da79b466/</link>
      <pubDate>Tue, 21 May 2013 18:32:36 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/afce98dc8c1575d91f00b0a6da79b466/</guid>
      <description>本篇会介绍Tor系统的休眠模块。休眠模块的代码处于源文件Hibernation.c之中。简单的说，其主要作用就是在适当的时机将系统进入休眠状态以保护系统资源被过度消耗；或者在适当的时机重新唤醒系统以达到重新为全局服务的目的。在默认的系统配置下，客户端的休眠模块是被关闭的，也就是说客户端永远不会进入休眠态。而Tor系统中的工作路由服务器则并非如此。他们很多时候需要设置一些为网络服务的策略和带宽，那么他们就需要对自己为Tor系统做出的贡献做一定的限制。单纯从流量角度来说，或许有一些OR服务器不愿意在一段时间内，允许流过自身的数据量超过一个非常大的范围。所以，OR服务器运行其自身的Tor程序时，进行了相关配置，要求其检查固定时间内流过的数据量。如果该数据量较小，则服务器可以忍受；若数据量非常大，则服务器会让其Tor程序进入休眠状态，提供少量服务，甚至最后不提供服务。&#xA;在讲休眠机制之前，需要关注配置文件中的两个配置选项：AccountingMax，AccountingStart。服务器就是通过这两个配置项，来配置他们所能接受的流量范围。我们以默认情况举例。一台Tor路由服务器，一般情况下统计其流量的周期为一个月，一个月内允许通过其自身的Tor网络的流量为1G，则上述两个配置项可以根据周期为月，流量为1G这些需求进行配置。具体在配置文件中如何具体配置，请参照Tor Manual中的选项说明。&#xA;这里我们先来分析休眠模块的源文件的全局变量及部分函数。&#xA;0. 概述 概述部分我们只简单地贴出来源文件头部对文件的说明，因为其已经相对清晰地描述了文件所要处理的事件和主要工作内容。&#xA;/** * \file hibernate.c * \brief Functions to close listeners, stop allowing new circuits, * etc in preparation for closing down or going dormant; and to track * bandwidth and time intervals to know when to hibernate and when to * stop hibernating. **/ /* hibernating, phase 1: // soft limit 将要耗尽流量之时产生的反应 - send destroy in response to create cells - send end (policy failed) in response to begin cells - close an OR conn when it has no circuits hibernating, phase 2: // hard limit 流量耗尽之时产生的反应 (entered when bandwidth hard limit reached) - close all OR/AP/exit conns) */ 简而言之，系统是否休眠取决于在指定的时间段内，系统的流量多少。源文件中的注释中给出了如下的关于休眠和统计流量的简要解释：</description>
    </item>
    <item>
      <title>Tor源码分析八 — 客户端执行流程（second_elapsed_callback函数）</title>
      <link>https://anwangtanmi.github.io/posts/a9bd8f98fc2a3141855463a592e0474d/</link>
      <pubDate>Mon, 13 May 2013 15:16:42 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a9bd8f98fc2a3141855463a592e0474d/</guid>
      <description>在之前的客户端源码分析中，我们讲述了整个客户端的事件集和相关调度规则。每一类事件的激活都有相应的条件，要么是socket可读写，要么是收到信号，要么是定时事件到达，还有手动的事件激活。总而言之，系统中添加的所有事件经过Libevent的调度，使得整个系统有条不紊的运行起来。同时，每个事件均有其对应的事件处理函数，在系统运行起来之后，一旦事件被激活，就会调用相应的回调函数进行处理。&#xA;本文我们着重介绍秒回调事件的事件处理函数second_elapsed_callback。该函数所做的工作非常之多，但是总的说来，是对系统的正常运行的维护和保障。之所以要最先介绍这个函数，是因为新安装的Tor系统在正常启动之后，没有任何事件会先于该秒回调事件被激活，也就是说，系统最先执行的回调函数是秒回调函数。在这个函数中，运行Tor系统的许多必要信息被获取或维护，保障了系统能够在正常地启动，最终为用户提供服务。下面，我们采取代码中加注释的方法，介绍函数的主要过程。英文注释已经描述地很清楚的部分就不再多说了。&#xA;1. second_elapsed_callback() /** Libevent callback: invoked once every second. */ static void second_elapsed_callback(periodic_timer_t *timer, void *arg) // 本函数的两个参数没有任何用处 { static time_t current_second = 0; // 静态的当前时间，每次进入该函数时，该值就是上次执行该函数时的时间； time_t now; // 真正的当前时间； size_t bytes_written; size_t bytes_read; int seconds_elapsed; const or_options_t *options = get_options(); (void)timer; (void)arg; n_libevent_errors = 0; /* log_notice(LD_GENERAL, &#34;Tick.&#34;); */ now = time(NULL); update_approx_time(now); /* the second has rolled over. check more stuff. */ seconds_elapsed = current_second ?</description>
    </item>
    <item>
      <title>Tor源码分析七 — 握手协议</title>
      <link>https://anwangtanmi.github.io/posts/ea9ecccf72a3064c68ca92f7a09f603e/</link>
      <pubDate>Sat, 11 May 2013 15:33:09 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ea9ecccf72a3064c68ca92f7a09f603e/</guid>
      <description>本节主要讲述Tor系统中所用到的握手协议。握手协议分三层：TCP握手；TLS握手；Tor握手。其中Tor握手又分为三个层次：OR握手；链路建立；流建立。&#xA;TCP的三次握手我想应该学计算机方向的朋友无人不知了，所以此处就略去。而TLS是SSL的升级版本，其握手过程与SSLv3几乎一致。同时由于TLS根据客户端的不同握手选择，会有些许握手过程中的差别，我们希望大家能够找到TLS相关的书籍翻阅。此处就不再过多的叙述TLS的握手过程。&#xA;本文的重心着眼于描述Tor握手全过程，更甚者是描述整个Tor协议。协议的主要内容包括协议使用的主要数据结构，连接的建立和初始化，连接的协商，链路管理，流管理以及流量控制等部分。在本文描述完全之后，大家会有个对Tor系统中结点之间如何进行交流的一个比较完整的印象。当然，可以说本文中几乎全部内容都来自tor-spec.txt，也就是Tor Protocol Specification。有兴趣的朋友可以参看原文，原文必定比笔者讲的要详细清楚。&#xA;1. 系统概述&#xA;Tor是一个用以保证用户匿名性的分布式网络。其主要的服务对象是用户所使用的基于TCP连接的应用。在使用Tor系统之时，客户端会选择一系列的结点建立Tor链路。链路中的结点，只知道其前继结点和后继结点，不知道链路中的其他结点信息。这样从某种程度上保证了坏结点存在对链路匿名性的破坏。链路中的数据流是以Cell数据包的形式进行传输的。Cell数据包的负载符合洋葱路由的特性，即由原点出发的负载被多层加密，传输过程中被层层解密；而由出口结点返回的数据在传输过程中层层加密，到原点后由原点一次性解密。而在这个加解密过程中所使用到的密钥，则是由Tor协议提供机制来协商生成的。下面先简要描述系统中所使用到的最常见的密钥：&#xA;1）Identity Key（非对称密钥的公钥）：系统中的结点均有其自身所对应的ID密钥，该密钥的作用是用以证书签名或标识路由身份。实际上，在前面的文章中我们也提到过，Tor结点根据自身的身份不同，会使用不同的Identity Key。但是，要说明的是，在目录服务器已注册的结点，不可以随意更改其Identity Key，否则就无法正常提供服务。&#xA;2）Onion Key（非对称密钥的公钥）：系统中提供中继服务的结点，即OR结点，均有一个比ID密钥更短期的一周一换的洋葱密钥。该密钥的主要作用是用来保护OP与OR之间进行DH密钥交换协议的前半部分信息（g^x）的安全性。利用Onion Key，OP与OR之间可以教安全的实行DH密钥交换协议从而协商出洋葱对称密钥。&#xA;3）Connection Key（对称密钥）：洋葱对称密钥，是OP与各个OR之间协商的临时对称密钥，用来完成上文中叙述的层层加解密功能。&#xA;另外，由于结点之间的通信都是由TLS连接来保障的，所以必定会在TLS那个层次中有一对保障TLS通信安全的对称密钥。这并非Tor系统所关心的密钥，所以略去。&#xA;注：这个部分的说明和Tor Protocol Specification之中的说明或许有所不同，大家请自行参照更多其它材料来进行分析。最终目标就是理解整个Tor系统的密钥组织结构及其使用。下面会再对整个系统的密码学相关过程做更详细的分析。&#xA;上图中给出了Tor系统建立链路的整个过程，下面我们对它进行深入的分析。&#xA;1. Alice(OP)开启Tor系统之后，需要建立第一条可用链路，她选择了OR1作为链路第一跳，并与OR1建立起了TLS连接；（TLS的建立与上层系统关系极小）&#xA;2. Alice成功与OR1建立TLS连接之后，他们之间的通信就是被TLS连接保障的加密通信。此时，Alice开始执行Tor握手协议第一步：OR握手；（OR握手的过程图中并未给出，而此过程与TLS握手密切相关，我们可以暂且认为OR握手就包含于TLS握手之中）&#xA;3. Alice成功与OR1完成OR握手之后，他们相互之间交换了彼此的信息。此时，Alice开始执行Tor握手协议第二步：链路建立；&#xA;1）Tor协议中的协议数据交换使用的是固定长度的Cell，以下为Cell的简单框图：&#xA;实际上现有版本的Cell结构已经发生了些许改变，但是此处我们用最原始的Cell结构进行说明。下面我们用稍微形式化的方式来描述上述结构：&#xA;Cell ::= Control Cell | Relay Cell&#xA;Control Cell ::= Control Header || Control Payload Length = 2B + 1B + 509B = 512B&#xA;Control Header ::= CircID || Control CMD Length = 2B + 1B = 3B&#xA;Control Payload ::= Control DATA Length = 509B</description>
    </item>
    <item>
      <title>Tor源码分析六 — 总体框架图</title>
      <link>https://anwangtanmi.github.io/posts/aac3a7cd471335b937ce00bad46a81f3/</link>
      <pubDate>Fri, 10 May 2013 11:36:46 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/aac3a7cd471335b937ce00bad46a81f3/</guid>
      <description> 1. 客户端的总体框图 由于画框图的过程稍微繁琐，所以框图给出的稍慢了些，请大家见谅。同时，框图画的比较急促，一定会有错误的地方，请大家指正。&#xA;框图中给出了客户端Tor系统所使用到的最重要的结构体以及其成员变量，有兴趣的朋友可以自行比对，这里就不便多说了。&#xA;2. 服务器端的总体框图（2合1） 服务器端的框图将两种身份的服务器所使用到的主要结构体均画了出来。也就是说该框图中展现服务器的两种功能：Relay OR，EXIT OR.&#xA;3. 一些额外的说明 </description>
    </item>
    <item>
      <title>Tor源码分析五 — 客户端执行流程（libevent调度）</title>
      <link>https://anwangtanmi.github.io/posts/8c2fe584dbc876f7d92eebef3abc6408/</link>
      <pubDate>Thu, 09 May 2013 21:46:40 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/8c2fe584dbc876f7d92eebef3abc6408/</guid>
      <description>上一源码分析的小节中，已经将系统的主要流程启动过程介绍完毕。实际上，系统在如此启动之后，就可以通过正确的流程将Tor系统完整的启动起来，为用户提供应用程序代理服务，以达到匿名通信的目标。本节的目标，不是为了详细介绍系统初始化后的正确启动流程，而是介绍系统是如何通过Libevent的调度实现成功启动的。也就是说，本节的重点在于说明Libevent调度机制在Tor系统之中的应用。&#xA;上节当中也简要介绍了Libevent的三种应用方式：信号处理；定时处理；网络套接字处理。这三种处理内容对于Libevent来说均是事件，只是事件的属性与回调函数不同而已。Libevent根据自身的调度机制，优先级队列等，来进行事件的调度。事件大多在被创建之时就被加入到Libevent事件集之中，Libevent一旦发现事件被激活，就会执行事件相应的回调函数。而值得说明的是，激活事件的方式根据事件类别的不同而有所区别：&#xA;1）signal – 信号处理事件：挂起的信号处理事件在进程接收到相应信号时被激活，信号的发送者可以是用户，也可以是另外一个进程等；&#xA;2）time – 定时处理事件：挂起的定时处理事件在定时器到时时被激活，定时器的时间设定会在定时处理事件生成时就指定；&#xA;3）socket – 网络套接字处理事件：挂起的网络套接字处理事件主要分两种，分别为读事件和写事件；读事件在相应套接字有数据可读时被激活；写事件比较特殊，它总是处于激活状态的，也就是说，只要套接字是正常可写的，写事件就会不断被激活；因为写事件的情况特殊，所以Tor系统对其的处理是动态地增删，而不是像读事件一样一直放置于事件池内部。利用Libevent的事件池输出调试函数event_base_dump_events可以看到事件池内事件的变动和简单状态，是调试Tor系统的重要手段。（我们讨论非阻塞的读写事件，若读写事件是阻塞的，则Tor系统的单线程执行很容易就会被阻塞，系统就无法正常运行）&#xA;0. 系统中的普通事件 此处暂且介绍客户端系统的事件集，服务器端的事件集可以由此作为依据进行猜测。&#xA;0.1. 监听事件(socket) — 系统初始化阶段 tor_init()&#xA;客户端系统中的监听事件主要分为两种：socket监听；control监听。&#xA;以上两种监听事件的目的是不同的：socket监听是监听Tor提供给用户程序的应用端口，接收的是应用请求；control监听是监听Tor提供给用户的控制端口，通过向控制端口传送相应的命令，可以有效控制Tor系统的执行，该部分的控制规则可以在文件control-spec.txt中找到详细的说明。&#xA;监听事件的添加是在监听连接创建之后马上执行的，主要处理代码在初始化阶段，入口函数为options_act_reversible。其后主要是通过connection_listener_new函数中的处理来完成事件的新建和添加。connection_start_reading函数完成了事件向事件集中添加的具体操作。在完成了添加之后，一旦进入到主循环的Libevent循环，就将控制权交给Libevent，通过Libevent的机制进行调度，执行提供的回调函数。&#xA;此处值得先一提的是，对于所有连接的读事件和写事件，执行的回调函数分别相同：conn_read_callback，conn_write_callback。在后期的分析之中，会着重介绍这两个函数对网络套接字事件所执行的具体处理。&#xA;0.2. 信号事件(signal) — 系统无限循环开启前 do_main_loop()&#xA;与大多数的Linux软件编程类似，Tor系统进程需要准确地控制进程以接收信号完成相应操作。信号的处理是件相对繁琐的事情，我们此处略去信号的具体处理细节，而把注意力集中在Tor系统是如何利用Libevent来进行信号控制。其实其过程也相对简单，只利用了一个函数：&#xA;/** Set up the signal handlers for either parent or child. */ void handle_signals(int is_parent) { #ifndef _WIN32 /* do signal stuff only on Unix */ int i; // 定义需要处理的信号 static const int signals[] = { SIGINT, /* do a controlled slow shutdown */ SIGTERM, /* to terminate now */ SIGPIPE, /* otherwise SIGPIPE kills us */ SIGUSR1, /* dump stats */ SIGUSR2, /* go to loglevel debug */ SIGHUP, /* to reload config, retry conns, etc */ #ifdef SIGXFSZ SIGXFSZ, /* handle file-too-big resource exhaustion */ #endif SIGCHLD, /* handle dns/cpu workers that exit */ -1 }; //信号事件集 static struct event *signal_events[16]; /* bigger than it has to be.</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Circuits全局变量</title>
      <link>https://anwangtanmi.github.io/posts/17bafb97d1259455dad3d9bffebb6223/</link>
      <pubDate>Thu, 09 May 2013 10:39:12 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/17bafb97d1259455dad3d9bffebb6223/</guid>
      <description>Tor系统源码中用于控制链路circuit的文件主要有三个，分别是：circuitlist.c，circuituse.c，circuitbuild.c。这三个文件分别主要针对的处理功能与他们的名字相类似，即分别处理链路的组织，使用和建立。在本篇中，我们只介绍主要用于链路部分的全局变量，即存在于这三个文件之中的全局变量的使用。此处不再罗列链路函数，因为链路部分的函数着实不少，尤其是链路建立部分。或许在本文之后会花专门的篇幅描述链路建立以及对链路建立的函数加以说明，不过这不是本篇的主要内容。&#xA;0. 全局变量 circuitlist.c circuitlist.c文件主要用于处理链路列表的组织，整理，计数等相关的操作。文件中前两个全局变量如下：&#xA;/** A global list of all circuits at this hop. */ circuit_t *global_circuitlist=NULL; /** A list of all the circuits in CIRCUIT_STATE_OR_WAIT. */ static smartlist_t *circuits_pending_or_conns=NULL; 第一个全局变量用于维护系统的链路列表。系统链路列表是每个Tor主机上维护的全局链路列表，也就是说所有的链路都挂接在这个链路列表之中。第二个全局变量用于维护系统等待链路的列表。系统等待链路的列表就是系统中正在等待OR连接完成的链路形成的列表。 这两个列表的操作很简单，就是链表的插入和删除。其代码也可以很容易的用sourceinsight进行查找和分析，此处略去。在仔细研究代码之前我们就可以推测，全局链路列表必定是在链路被建立之时被使用到，一旦链路结构体被创建，链路设置完开始状态，就被加入全局链路列表。当然，若链路开始状态为等待OR连接，则也会被加入等待链路列表。这里的前后关系应该并不是那么重要，只是要保证两个列表的一致性：全局链路列表中状态为等待的链路一定会在等待列表中；等待链路列表中的所有链路都应该存在在全局链路列表之中。&#xA;文件中后两个全局变量如下：（哈希表的使用请详细查看源代码）&#xA;/** Map from [orconn,circid] to circuit. */ static HT_HEAD(orconn_circid_map, orconn_circid_circuit_map_t) orconn_circid_circuit_map = HT_INITIALIZER(); HT_PROTOTYPE(orconn_circid_map, orconn_circid_circuit_map_t, node, _orconn_circid_entry_hash, _orconn_circid_entries_eq) HT_GENERATE(orconn_circid_map, orconn_circid_circuit_map_t, node, _orconn_circid_entry_hash, _orconn_circid_entries_eq, 0.6, malloc, realloc, free) /** The most recently returned entry from circuit_get_by_circid_orconn; * used to improve performance when many cells arrive in a row from the * same circuit.</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Connetion_Edge</title>
      <link>https://anwangtanmi.github.io/posts/49652d2d63441169cf4915cc6a8d1df2/</link>
      <pubDate>Wed, 08 May 2013 10:33:17 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/49652d2d63441169cf4915cc6a8d1df2/</guid>
      <description>所谓的Edge连接主要是形容在链路两端所使用的连接，包括AP连接与EXIT连接两种。Edge连接的存在，主要是为了个应用程序连接提供服务，所以才将两种Edge连接命名为Application Proxy应用代理连接和EXIT出口连接。这两种连接主要出现于链路两端，即链路第一跳与链路最后一跳，分别用于接受应用请求和处理应用请求的发出。因为Edge连接在Tor系统中也具有重要的作用，所以本篇就简单分析关于Edge连接的源码文件connection_edge.c。&#xA;因为Edge连接位于链路两端，需要处理的事务比中间结点单纯的传递和记录要繁琐出许多，有很多部分笔者也没有很详细的看过或找出其应用需要，此处必定有很多讲的含糊其辞和不准确的地方，请大家包涵。&#xA;0. 全局变量 /** A client-side struct to remember requests to rewrite addresses * to new addresses. These structs are stored in the hash table * &#34;addressmap&#34; below. * 由此处可见，addressmap全局变量的使用，只存在于客户端，其他身份的TOR成员并不使用此全局变量； * * There are 5 ways to set an address mapping: * - A MapAddress command from the controller [permanent] * - An AddressMap directive in the torrc [permanent] * - When a TrackHostExits torrc directive is triggered [temporary] * - When a DNS resolve succeeds [temporary] * - When a DNS resolve fails [temporary] * 由此处可见，除去在配置文件中进行地址映射之外，主要还是靠DNS解析来修改address mapping； * * When an addressmap request is made but one is already registered, * the new one is replaced only if the currently registered one has * no &#34;</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Connection_OR</title>
      <link>https://anwangtanmi.github.io/posts/c18a620b97ec1a439ffcfa26a6097cc2/</link>
      <pubDate>Tue, 07 May 2013 12:12:06 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c18a620b97ec1a439ffcfa26a6097cc2/</guid>
      <description>作为连接类型中最重要的一种连接，OR连接对OP与OR之间，OR与OR之间的通信负全责。也就是说，OR连接的存在，是解决底层两机之间通信的必要条件。所以，有必要对OR连接的源码文件进行深入分析，并对OR连接先做简要介绍。后期，在介绍Tor系统的全部连接之时，会更加详细地介绍各种连接及他们的作用。&#xA;OR连接，即Onion Router连接，是用以连接Tor系统内两结点的连接。该连接的主要功能包括：管理结点间TLS连接；管理活动链接；管理CELL的发送和接收等。OR连接处于Tor系统的最底层，其下层即为TLS层。所以，OR连接负责的主要部分就是可靠的CELL传递以及与其上层链接层的交互。&#xA;DIR连接……　应用层&#xA;－－－－－－－－－－－－－－－－－－－－－－－－&#xA;AP连接，EXIT连接……&#xA;－－－－－－－－－－－－－－－－－－&#xA;Circuit链路……　Tor协议层&#xA;－－－－－－－－－－－－－－－－－－&#xA;OR连接……　－－－－－－－－－－－－－－－－－－－－－－－－&#xA;TLS连接　传输层&#xA;同时，在此处值得一提的是，Tor系统的应用连接AP，链路Circuit，OR连接三者之间的复用情况：（多AP连接复用Circuit，多Circuit复用OR连接）&#xA;AP Stream 1 –&amp;gt;&#xA;AP Stream 2 –&amp;gt; Circuit 1 -&amp;gt;&#xA;… …　3 –&amp;gt;&#xA;Circuit 2 -&amp;gt; OR Connection 1&#xA;Circuit 3 -&amp;gt;&#xA;0. 全局变量 1）orconn_identity_map /** Map from identity digest of connected OR or desired OR to a connection_t * with that identity digest. If there is more than one such connection_t, * they form a linked list, with next_with_same_id as the next pointer.</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Connection</title>
      <link>https://anwangtanmi.github.io/posts/9a3d7a506d842f88e6cbacca6ae84952/</link>
      <pubDate>Sat, 27 Apr 2013 17:24:47 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9a3d7a506d842f88e6cbacca6ae84952/</guid>
      <description>Tor协议的层次结果经过简要分析可以大致概括为如下框图：&#xA;DIR连接……　应用层&#xA;－－－－－－－－－－－－－－－－－－－－－－－－&#xA;AP连接，EXIT连接……&#xA;－－－－－－－－－－－－－－－－－－&#xA;Circuit链路……　Tor协议层&#xA;－－－－－－－－－－－－－－－－－－&#xA;OR连接……　－－－－－－－－－－－－－－－－－－－－－－－－&#xA;TLS连接　传输层&#xA;而其中的DIR,AP,EXIT,OR等连接均是对理解Tor协议非常重要的部分，所以有必要简要的介绍一下通用连接的源码文件connection.c。&#xA;0. 全局变量 // 用于检测IP地址变化的全局变量，引用处极少 // 该部分的全局变量只用于函数 client_check_address_changed /** The last addresses that our network interface seemed to have been * binding to. We use this as one way to detect when our IP changes. * * XXX024 We should really use the entire list of interfaces here. **/ static tor_addr_t *last_interface_ipv4 = NULL; /* DOCDOC last_interface_ipv6 */ static tor_addr_t *last_interface_ipv6 = NULL; /** A list of tor_addr_t for addresses we&#39;ve used in outgoing connections.</description>
    </item>
    <item>
      <title>Tor源码文件分析 — Main</title>
      <link>https://anwangtanmi.github.io/posts/ba3edb7d88656c10da47da9c75c0aa29/</link>
      <pubDate>Sat, 27 Apr 2013 16:50:32 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ba3edb7d88656c10da47da9c75c0aa29/</guid>
      <description>Main文件是Tor系统的主要执行函数所处文件，内容偏多，但是较于其他底层处理函数所在的文件，也算较少。所以这里做简要的分析，其中删去很多不常见的，不重要的函数，大家可以自行在源文件内部查看。&#xA;1. 全局变量 全局变量在文件头部定义，每个全局变量都已经有比较详尽的英文分析和解释，这里再做简单的罗列。&#xA;1）系统（先前）已读与已写字节数以及用于流量控制的令牌桶全局变量；（先前与当前已读写字节数的区别与refill回调函数相关）&#xA;令牌桶用于控制整个系统的流量，经过初始化，减少，增加等操作（当读写相应的令牌数不够时，系统或连接会停止读或写）：&#xA;// 初始化令牌桶令牌数 /** Initialize the global read bucket to options-\&amp;gt;BandwidthBurst. */ void connection_bucket_init(void) { const or_options_t *options = get_options(); /* start it at max traffic */ global_read_bucket = (int)options-&amp;gt;BandwidthBurst; global_write_bucket = (int)options-&amp;gt;BandwidthBurst; if (options-&amp;gt;RelayBandwidthRate) { global_relayed_read_bucket = (int)options-&amp;gt;RelayBandwidthBurst; global_relayed_write_bucket = (int)options-&amp;gt;RelayBandwidthBurst; } else { global_relayed_read_bucket = (int)options-&amp;gt;BandwidthBurst; global_relayed_write_bucket = (int)options-&amp;gt;BandwidthBurst; } } // 减少令牌桶令牌数 /** We just read num_read and wrote num_written bytes * onto conn.</description>
    </item>
    <item>
      <title>Tor源码分析二 — 目录结构</title>
      <link>https://anwangtanmi.github.io/posts/9f3b4f1178ecd7fbb1b88ff0c75e797a/</link>
      <pubDate>Thu, 25 Apr 2013 12:20:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9f3b4f1178ecd7fbb1b88ff0c75e797a/</guid>
      <description>Tor由于经过了长年的开发，版本到现在已经有很多。笔者选用的是tor-0.2.3.25版本。关于版本变迁和更新说明，请大家自行查看：&#xA;https://gitweb.torproject.org/tor.git (需翻墙)&#xA;1. 源码目录关系图 Tor的源码目录中包括doc, contrib, src等一些列很莫名的文件。其实，在这些所有的文件中，除了src文件夹，其他都可以在初期暂时不去理会。所以，这里我们就谈src文件夹下的目录结构，也是Tor源码核心的目录结构。&#xA;上面两图已经基本给出了各个子目录之间的关系和引用次数。总的来说，src目录下包括下列这样的目录：&#xA;1）win32：用于Windows的目录，内部只含有一个orconfig.h文件。主要用于Tor源码的跨平台编译。&#xA;2）common：Tor源码中基本通用函数的封装目录。包括：OpenSSL,Libevent的封装，TLS的封装，链表和哈希表的实现等。&#xA;3）tools：Tor使用工具目录。包括：密钥验证，证书生成，域名解析和防火墙辅助的实现。&#xA;4）or：Tor核心程序逻辑目录。（Tor系统所有核心协议的实现目录）包括：客户端身份核心代码，路由身份核心代码，目录服务器身份核心代码等。&#xA;5）test：Tor源码各功能模块的测试代码目录。&#xA;6）config：配置文件模板目录&#xA;2. or目录下的各文件 or目录是Tor源代码的核心，有必要简要介绍下该目录下的文件的主要作用：（细节部分后期会逐个文件分别进行分析）&#xA;以下部分同时用于OP与OR：(Onion Proxy, Onion Router)&#xA;1）Buffers：缓冲区相关结构体及处理函数；&#xA;a generic interface buffer. Buffers are fairly opaque string holders that can read to or flush from: memory, file descriptors, or TLS connections.&#xA;2）Circuitbuild：链路建立相关函数；&#xA;The actual details of building circuits.&#xA;3）Circuitlist：链路列表及链路控制相关函数；&#xA;Manage the global circuit list.&#xA;4）Circuituse：链路获取或使用相关函数；&#xA;Launch the right sort of circuits and attach streams to them.</description>
    </item>
    <item>
      <title>Tor源码分析一 — 使用工具集</title>
      <link>https://anwangtanmi.github.io/posts/132307c2744c312ab24b439ca1046248/</link>
      <pubDate>Thu, 25 Apr 2013 10:51:22 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/132307c2744c312ab24b439ca1046248/</guid>
      <description>Tor系统是用于匿名通信的一个系统，源代码的维护到今天已经快10年。笔者从今年3月份开始陆陆续续研究Tor系统的源码，由于源码量大，也有很多需要总结的地方，所以特地在这里开辟个Tor源码分析的系列文章，供自己总结，也供大家共同探讨。&#xA;1. 源码查看工具 — SourceInsight 本来Tor源码在Linux下分析和调试都可以比在Windows下更专业些，但是因为笔者本身大部分时间需要在Windows下工作，所以选择了Windows下的源码分析软件。SourceInsight是个非常好的源码分析软件，用于查看各种开源代码工程都非常方便。这里选用SourceInsight作为Tor源码的基本浏览和修改工具。&#xA;2. 文档生成工具 — Doxygen 关于Doxygen的基本介绍大家可以在网上找到很多。简单的说，Doxygen就是将源码内的信息进行总结，生成包括函数调用关系，函数介绍等信息。这对于宏观查看代码非常重要。Tor源码内很多函数都连续嵌套非常多层，靠人为记住是非常困难的。利用Doxygen的函数调用图，能帮助我们更好的查看函数之间的调用关系。这里不介绍Doxygen如何使用，百度上有许多。注：使用Doxygen时定要打开函数调用图的生成功能。&#xA;3. 编译工具 — MinGW MSYS msyDTK 为了在Windows上完成对Tor的编译，需要使用到下面一些列套件。好在这些部分在Tor的FAQ中已经很详细地说明如何使用，这里就仅仅将Tor中关于如何在WIndows下编译Tor源码的部分复制于此，请大家自行查看。&#xA;## ## Instructions for building Tor with MinGW (http://www.mingw.org/) ## Stage One: Download and Install MinGW. --------------------------------------- Download mingw: http://prdownloads.sf.net/mingw/MinGW-5.1.6.exe?download Download msys: http://prdownloads.sf.net/ming/MSYS-1.0.11.exe?download Download msysDTK: http://sourceforge.net/projects/mingw/files/MSYS%20Supplementary%20Tools/msysDTK-1.0.1/msysDTK-1.0.1.exe/download Install MinGW, msysDTK, and MSYS in that order. Make sure your PATH includes C:\MinGW\bin. You can verify this by right clicking on &#34;My Computer&#34;, choose &#34;</description>
    </item>
  </channel>
</rss>
