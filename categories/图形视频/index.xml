<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>图形视频 on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/categories/%E5%9B%BE%E5%BD%A2%E8%A7%86%E9%A2%91/</link>
    <description>Recent content in 图形视频 on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 11 Jun 2019 14:44:14 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/categories/%E5%9B%BE%E5%BD%A2%E8%A7%86%E9%A2%91/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>图像分割算法的优缺点比较</title>
      <link>https://anwangtanmi.github.io/posts/840d4013e26e8b1fa97cf14903f8c77b/</link>
      <pubDate>Tue, 11 Jun 2019 14:44:14 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/840d4013e26e8b1fa97cf14903f8c77b/</guid>
      <description>本文章只做简单介绍各个分割方法，后续会对各个方法一一做详细介绍 数字图像处理常用的图像分割算法有：&#xA;基于阀值的分割方法。 基于边缘的分割方法。 基于区域的分割方法。 基于聚类分析的图像分割方法。 基于小波变换的分割方法。 基于数学形态学方法。 基于人工神经网络的方法。 基于遗传算法的方法。 阈值分割 1、原理&#xA;用一个或几个阈值将图像的灰度直方图分成几个类， 认为图像中灰度 值在同一类中的像素属于同一物体。（常用的二值分割）&#xA;2、优点&#xA;直接利用图像的灰度特性，所以计算简单、运算效率较高、速度快。&#xA;3、适用范围&#xA;相差很大的不同目标和背景能进行有效的分割。&#xA;4、缺点&#xA;对噪声敏感，对灰度差异不明显以及不同目标灰度值有重叠分割不明显，所以需要与其他方法进行结合。合适的阈值查找。&#xA;边缘分割 1、原理&#xA;通常不同的区域之间的边缘上像素灰度值的变化往往比较剧烈， 这是边缘检测方法得以实现的主要假设之一。常用灰度的一阶或二阶微分算子进行边缘检测。&#xA;2、优点&#xA;搜索检测的速度快，对边缘检测好。&#xA;3、适用范围&#xA;低噪声干扰，区域之间的性质差别很大（或则说边缘变化大）。&#xA;4、缺点&#xA;不能得到较好的区域结构，边缘检测时抗噪性和检测精度之间的矛盾。精度提高，则会牺牲抗噪性，反之。我们可以设置一个熵，取一个折中的办法，求取熵最大的时候的精度和抗噪性。&#xA;区域分割 1、原理&#xA;把具有某种相似性质的像索连通，从而构成最终的分割区域。它采用两种方法：分裂和合并&#xA;2、优点&#xA;有效地克服其他方法存在的图像分割空间小连续的缺点，有较好的区域特征。&#xA;3、适用范围&#xA;需得到具有区域结构的分割图。&#xA;4、缺点&#xA;容易造成图像的过度分割，将边缘检测与区域分割结合，可以得到良好的分割效果。&#xA;聚类分析的图像分割 1、原理&#xA;将图像空间中的像素用对应的特征空间点表示，根据它们在特征空间的聚集对特征空间进行分割，然后将它们映射回原图像空间，得到分割结果。它采用两种方法：K 均值、模糊 C 均值聚类(FCM)算法&#xA;2、优点&#xA;且 FCM 算法对初始参数极为敏感，有时需要人工干预参数的初始化以接近全局最优解，提高分割速度。&#xA;3、适用范围&#xA;适合图像中存在不确定性和模糊性。&#xA;4、缺点&#xA;传统 FCM 算法没有考虑空间信息，对噪声和灰度不均匀敏感。&#xA;聚类分析所要做的工作。&#xA;(1)、聚类的类数如何确定。&#xA;(2)、怎样确定聚类的有效性准则。&#xA;(3)、聚类中心的位置和特性事先不清楚时， 如何设置初始值。&#xA;(4)、运算的开销。&#xA;小波变换的分割 1、原理&#xA;将基于小波变换的阈值图像分割方法的基本思想是，首先由二进小波变换将图像的直方图分解为不同层次的小波系数，然后依据给定的分割准则和小波系数选择阈值门限，最后利用阈值标出图像分割的区域。&#xA;2、优点&#xA;空域和频域的局域变换，因而能有效地从信号中提取信息，通过伸缩和平移等运算功能对函数或信号进行多尺度分析，解决了傅立叶变换不能解决的许多问题。由于是频域操作，所以对噪声不敏感。&#xA;3、适用范围&#xA;用于边缘检测，可提取多尺度边缘，并可通过对图像奇异度的计算和估计来区分一些边缘的类型。</description>
    </item>
    <item>
      <title>ps高低频磨皮详细教学</title>
      <link>https://anwangtanmi.github.io/posts/ac5a245e978d4939a45c495b8f559064/</link>
      <pubDate>Sun, 26 May 2019 23:18:25 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ac5a245e978d4939a45c495b8f559064/</guid>
      <description> 很多同学都知道高低频磨皮，其实高低频不仅可以用来磨皮肤，还可以用来磨头发，磨衣物上的褶皱等。操作方法有多种，但原理都一样，都是通过融合颜色及明暗对比再提取纹理达到一个降低反差并保留纹理的磨皮效果。所谓高低频，就是使用两个图层以不同的方式提取图像中的信息，这两个图层一个叫低频层一个叫高频层。这里给大家演示一下常用的三种操作方法。&#xA;第一种：&#xA;复制两个原图-下方图层进行表面模糊（菜单栏滤镜-模糊-表面模糊，也可以用高斯模糊）-根据需要的融合效果设置半径与阈值（半径可以理解为力度，阈值可以理解为范围）-此层作为低频层。 上方图层进行反相（Ctrl+I）-将不透明度改为50％。 3.合并可见图层作为高频层-关闭下方两个图层的可见性或直接删除-将高频层的混合模式改为线性光-进行表面模糊（参数需小于低频层的2~5倍）&#xA;好了，放大看下对比效果：&#xA;下面演示一下第二种操作方法，与第一种的原理及效果相同，只是操作方式有点区别。这次我们用高斯模糊代替表面模糊，两者只是模糊方式不同，不在所包括的操作方法内。&#xA;第二种：&#xA;还是复制两个图层-下方图层进行高斯模糊（菜单栏滤镜-模糊-高斯模糊，也可以用表面模糊）-半径设置到看不清瑕疵确仍有轮廓为止-此层为低频层。 上方图层应用低频层混合模式减去缩放2补偿值128（菜单栏图像-应用图像）-此层作为高频层。 3.高频层进行高斯模糊（理论上模糊的参数需要与低频层相同以最大程度的保留纹理，但也可以根据效果稍微小一点以增加力度）-混合模式改为线性光。&#xA;好了，这是两种操作方法，这两种比较繁琐，磨皮的力度较小，所保留的纹理较多，适合磨皮肤。&#xA;下面我们演示第三种较快捷的方法，这种磨皮的力度较大，所保留的纹理较少，适合磨头发、褶皱等。这次我们用头发来做演示，当然如果对皮肤质感要求不高的话也可以用于皮肤。&#xA;第三种：&#xA;先看下原图：&#xA;复制一个图层反相-混合模式线性光。 高反差保留（滤镜-其它-高反差保留）-参数设置到反差较明显的光影平整了即可。 高斯模糊（理论上模糊的参数需要与高反差保留的参数相同，但这里稍微小一点以增加力度）。注：磨头发及褶皱时不能用表面模糊。 好了，第三种就完成了。放大看下对比效果：&#xA;当然，磨完以后要建个蒙版擦一下不需要的部分，这个就不多说了。&#xA;但可能有些同学会觉得这个头发看上去不够光滑，下面我们稍微调整一下，在不影响头发磨皮效果的前提下增强下对比。&#xA;调整步骤：&#xA;建个曲线蒙版应用高光-调整蒙版增强对比（选中蒙版-菜单栏图像-调整-曲线）-蒙版进行高斯模糊（参数宁小不大）-回到曲线提亮高光。 再建一个曲线蒙版应用上个曲线的反相蒙版（选中上个曲线的蒙版后按住Alt键拖移至这个曲线的蒙版上，然后Ctrl+I反相。当然你也可以重新应用阴影再进行一次高斯模糊）-回到曲线降暗阴影。 最后别忘了将两个曲线建个组加个蒙版擦一下。 欢迎大家一起学习交流 Q：673187073 </description>
    </item>
    <item>
      <title>食品网页色彩和风格的讲解</title>
      <link>https://anwangtanmi.github.io/posts/bfc24969e7eb291c8c9acbf2073c2437/</link>
      <pubDate>Wed, 08 May 2019 18:45:18 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/bfc24969e7eb291c8c9acbf2073c2437/</guid>
      <description>食品网页色彩和风格的讲解&#xA;（撰写时间：5月6日 作者：李梦熙）&#xA;首先来看看讲解的第一个网页：&#xA;这个网页的风格是利用插画来装饰整个页面，让整个页面丰富活跃起来，插画风格的网页会让内容更加的突出，让整个画面有种生动的感觉，就是内容过于太少，让用户无法了解更多的内容；色彩搭配它采用黑色作底色，用白色手绘图案作装饰和内容，还有就是用一些实体物品用插画的方式来点缀整个画面，这种小清新的设计很有新意。&#xA;下面的也是一个披萨网页，但和上面的插画风格完全不一样，它是采用图片做一些效果来当做背景，然后在图片的上方放一些介绍，比如这个食物的历史或由来和食物介绍、食物价格等。&#xA;每两个图片的中间都有白底隔开，在白底上放置一些菜品，和介绍，但是这个网页也用了插画，虽然不是用的很多，但是展现和别的网页有不一样的风格，黑白版块的插入突出里面的主要内容，让用户可以更好的了解本页面的内容。色彩搭配让人感觉很舒服，用黑白做主题色、利用酒红做点缀色。&#xA;下面的那个网页是有一半展示一半网页，网页采用的颜色搭配是黑绿搭配，这种颜色搭配很少见，但它搭配出来的效果还是挺好看的，上面做了个展示，把页面更好的展示出来，让用户直观的了解整个页面，网页的内容是以居中排版为主，用印花水墨嵌入背景来做背景的点缀，这个网页使用黑、绿色搭配，让整个画面都有一种清新的感觉。&#xA;风格有点像水墨画的风格。</description>
    </item>
    <item>
      <title>opengl学习笔记</title>
      <link>https://anwangtanmi.github.io/posts/d0dc42249d2b08c82cf341cab0ba4faa/</link>
      <pubDate>Wed, 24 Apr 2019 15:22:16 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d0dc42249d2b08c82cf341cab0ba4faa/</guid>
      <description>一、绘制方式。&#xA;绘制方式&#xA;方式 说明 网格线绘图方式（wireframe） 这种方式仅绘制三维物体的网格轮廓线 深度优先网格线绘图方式（depth_cued） 用网格线方式绘图，增加模拟人眼看物体一样，远处的物体比近处的物体要暗一些。 反走样网格线绘图方式（antialiased） 用网格线方式绘图，绘图时采用反走样技术以减少图形线条的参差不齐。&#xA;平面消隐绘图方式（flat_shade） 对模型的隐藏面进行消隐，对模型的平面单元按光照程度进行着色但不进行光滑处理。 光滑消隐绘图方式（smooth_shade） 对模型进行消隐按光照渲染着色的过程中再进行光滑处理，这种方式更接近于现实。 加阴影和纹理的绘图方式（shadows、textures） 在模型表面贴上纹理甚至于加上光照阴影，使得三维景观像照片一样。 运动模糊的绘图方式（motion_blured） 模拟物体运动时人眼观察所感觉的动感现象。 大气环境效果（atmosphere_effects） 在三维景观中加入如雾等大气环境效果。 深度域效果（depth_effects） 类似于照相机镜头效果，模型在聚焦点处清晰，反之则模糊。 二、OpenGL核心库&#xA;1、基本图元&#xA;几何图元类型和说明&#xA;类型 说明 GL_POINTS 单个顶点集 GL_LINES 多组双顶点线段 GL_POLYGON 单个简单填充凸多边形 GL_TRAINGLES 多组独立填充三角形 GL_QUADS 多组独立填充四边形 GL_LINE_STRIP 不闭合折线 GL_LINE_LOOP 闭合折线 GL_TRAINGLE_STRIP 线型连续填充三角形串 GL_TRAINGLE_FAN 扇形连续填充三角形串 GL_QUAD_STRIP 连续填充四边形串 2、调用函数&#xA;调用函数&#xA;函数 说明 glVertex*() 设置顶点坐标 glColor*() 设置当前颜色 glIndex*() 设置当前颜色表 glNormal*() 设置法向坐标 glEvalCoord*() 产生坐标 glCallList(),glCallLists() 执行显示列表 glTexCoord*() 设置纹理坐标 glEdgeFlag*() 控制边界绘制 glMaterial*() 设置材质 例如绘制一个三角形：</description>
    </item>
    <item>
      <title>cv2伪彩色applyColorMap()函数</title>
      <link>https://anwangtanmi.github.io/posts/dca6003c910c6e9b2402ff8c4972e123/</link>
      <pubDate>Sun, 09 Dec 2018 14:19:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/dca6003c910c6e9b2402ff8c4972e123/</guid>
      <description>本文主要介绍cv2模块中的伪彩色applyColorMap()函数。&#xA;引用自：https://blog.csdn.net/u013381011/article/details/78341861&#xA;colormap（色度图）&#xA;假设我们想在地图上显示美国不同地区的温度。我们可以把美国地图上的温度数据叠加为灰度图像——较暗的区域代表较冷的温度，更明亮的区域代表较热的区域。这样的表现不仅令人难以置信，而且代表了两个重要的原因。首先，人类视觉系统没有被优化来测量灰度强度的微小变化。我们能更好地感知颜色的变化。第二，我们用不同的颜色代表不同的意思。用蓝色和较温暖的温度用红色表示较冷的温度更有意义。&#xA;温度数据只是一个例子，但还有其他几个数据是单值（灰度）的情况，但将其转换为彩色数据以实现可视化是有意义的。用伪彩色更好地显示数据的其他例子是高度、压力、密度、湿度等等。&#xA;在OpenCV中使用applycolormap（伪彩色函数）&#xA;OpenCV的定义12种colormap（色度图），可以应用于灰度图像，使用函数applycolormap产生伪彩色图像。让我们很快看到如何将色度图的一种模式colormap_jet应用到一幅图像中。&#xA;下面是示例代码：&#xA;import cv2 import numpy as np def colormap_name(id): switcher = { 0 : &#34;COLORMAP_AUTUMN&#34;, 1 : &#34;COLORMAP_BONE&#34;, 2 : &#34;COLORMAP_JET&#34;, 3 : &#34;COLORMAP_WINTER&#34;, 4 : &#34;COLORMAP_RAINBOW&#34;, 5 : &#34;COLORMAP_OCEAN&#34;, 6 : &#34;COLORMAP_SUMMER&#34;, 7 : &#34;COLORMAP_SPRING&#34;, 8 : &#34;COLORMAP_COOL&#34;, 9 : &#34;COLORMAP_HSV&#34;, 10: &#34;COLORMAP_PINK&#34;, 11: &#34;COLORMAP_HOT&#34; } return switcher.get(id, &#39;NONE&#39;) img = cv2.imread(&#39;./pluto.jpg&#39;, cv2.IMREAD_GRAYSCALE) im_out = np.zeros((600, 800, 3), np.uint8) for i in range(0, 4): for j in range(0, 3): k = i + j * 4 im_color = cv2.</description>
    </item>
    <item>
      <title>opencv 两路视频 保存avi 输出yuv</title>
      <link>https://anwangtanmi.github.io/posts/83f5fef159b26d4b44d0c33f2120cb0a/</link>
      <pubDate>Mon, 03 Dec 2018 16:32:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/83f5fef159b26d4b44d0c33f2120cb0a/</guid>
      <description>opencv版本：3.4.1&#xA;编译工具：vs2015&#xA;说明：&#xA;两路视频：一路为网络摄像头，一路为PC usb摄像头&#xA;保存avi：根据条件切换不同视频源保存到同一个avi文件&#xA;输出yun:根据条件切换不同视频源，输出yuv作为后续使用。&#xA;#include #include #include using namespace cv; using namespace std; int main(int argc, char *argv[]) { VideoCapture videoInput(&#34;rtsp://X.X.X.X:554&#34;); VideoCapture videoInput2(0); if (!videoInput.isOpened() || !videoInput2.isOpened()) { return -1; } float fpsInput = 25; //获取帧率 float pauseInput = 1000 / fpsInput; //设置帧间隔 Mat frame; int w = videoInput.get(CV_CAP_PROP_FRAME_WIDTH) &amp;gt;= videoInput2.get(CV_CAP_PROP_FRAME_WIDTH) ? videoInput2.get(CV_CAP_PROP_FRAME_WIDTH) : videoInput.get(CV_CAP_PROP_FRAME_WIDTH); int h = videoInput.get(CV_CAP_PROP_FRAME_HEIGHT) &amp;gt;= videoInput2.get(CV_CAP_PROP_FRAME_HEIGHT) ? videoInput2.get(CV_CAP_PROP_FRAME_HEIGHT) : videoInput.get(CV_CAP_PROP_FRAME_HEIGHT); //Size videoSize = Size(videoInput.</description>
    </item>
    <item>
      <title>OpenCV—如何将彩色图像分通道输出（4）</title>
      <link>https://anwangtanmi.github.io/posts/c9b0ed11a469c923e0fe456c7376dc8b/</link>
      <pubDate>Mon, 03 Dec 2018 15:25:34 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c9b0ed11a469c923e0fe456c7376dc8b/</guid>
      <description> 附代码如下：&#xA;import cv2 as cv import numpy as np def ch(): src = cv.imread(&#34;D:/matplotlib/0.jpg&#34;) h,w,ch = np.shape(src) bgr = cv.split(src) cv.imshow(&#34;blue&#34;,bgr[0]) cv.imshow(&#34;green&#34;,bgr[1]) cv.imshow(&#34;red&#34;,bgr[2]) print(h,w,ch) cv.waitKey(0) cv.destroyAllWindows() ch() 运行效果：&#xA;代码解释：&#xA;import cv2 as cv import numpy as np def ch(): src = cv.imread(&#34;D:/matplotlib/0.jpg&#34;) h,w,ch = np.shape(src) bgr = cv.split(src) #将彩色图像拆分成单个通道 cv.imshow(&#34;blue&#34;,bgr[0]) cv.imshow(&#34;green&#34;,bgr[1]) cv.imshow(&#34;red&#34;,bgr[2]) #分别显示每个通道的图像 print(h,w,ch) cv.waitKey(0) cv.destroyAllWindows() ch() </description>
    </item>
    <item>
      <title>【图像处理】一种低光照图像的亮度提升方法（Adaptive Local Tone Mapping Based on Retinex for High Dynamic Range Images）</title>
      <link>https://anwangtanmi.github.io/posts/381d993c888c1ce5ce1dfd9b9f1c7536/</link>
      <pubDate>Thu, 15 Nov 2018 21:25:11 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/381d993c888c1ce5ce1dfd9b9f1c7536/</guid>
      <description>【fishing-pan：https://blog.csdn.net/u013921430 转载请注明出处】 前言 在实际的拍照过程中，常常会遇到，光线不足的情况。这时候单反用户一般会调大感光度，调大光圈，以让照片整体更清晰，更亮。那么如果照片已经被拍的很暗了，怎么办呢？这时候我们可以利用算法来提升图像整体的光照情况，让图像更清晰。&#xA;2013年这篇《Adaptive Local Tone Mapping Based on Retinex for High Dynamic Range Images》发表在了IEEE上，如题目所说，文章提到将高动态图像在低动态范围显示设备上进行显式时，会面临信息丢失的问题。因此结合传统的CENTER/SURROUND RETINEX 技术提出了全局自适应和局部自适应的HDR实现过程，对HDR image 进行色调映射。而文中的全局自适应方法对于低照度图像具有很好的照度提升效果。作者将他的Matlab脚本上传到了Github，有兴趣的可以点击这里去查看。&#xA;全局自适应原理 全局自适应方法的原理很简单，就是两个公式；&#xA;L&#xA;g&#xA;(&#xA;x&#xA;,&#xA;y&#xA;)&#xA;=&#xA;l&#xA;o&#xA;g&#xA;(&#xA;L&#xA;w&#xA;(&#xA;x&#xA;,&#xA;y&#xA;)&#xA;/&#xA;L&#xA;w&#xA;ˉ&#xA;+&#xA;1&#xA;)&#xA;l&#xA;o&#xA;g&#xA;(&#xA;L&#xA;w&#xA;m&#xA;a&#xA;x&#xA;/&#xA;L&#xA;w&#xA;ˉ&#xA;+&#xA;1&#xA;)&#xA;L_{g}(x,y)=\frac{log(L_{w}(x,y)/\bar{L_{w}}+1)}{log(L_{wmax}/\bar{L_{w}}+1)}&#xA;Lg​(x,y)=log(Lwmax​/Lw​ˉ​+1)log(Lw​(x,y)/Lw​ˉ​+1)​&#xA;上述式子中，&#xA;L&#xA;g</description>
    </item>
    <item>
      <title>关于opencv更改摄像头参数（帧率，分辨率，曝光度……）的几个问题</title>
      <link>https://anwangtanmi.github.io/posts/3104bf37f79ba45e74548afd28b7d431/</link>
      <pubDate>Mon, 24 Sep 2018 10:17:34 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3104bf37f79ba45e74548afd28b7d431/</guid>
      <description>1，适用于VideoCapture打开的摄像头 VideoCapture capture(0); 设置摄像头参数 不要随意修改&#xA;capture.set(CV_CAP_PROP_FRAME_WIDTH, 1080);//宽度&#xA;capture.set(CV_CAP_PROP_FRAME_HEIGHT, 960);//高度&#xA;capture.set(CV_CAP_PROP_FPS, 30);//帧率 帧/秒&#xA;capture.set(CV_CAP_PROP_BRIGHTNESS, 1);//亮度 capture.set(CV_CAP_PROP_CONTRAST,40);//对比度 40&#xA;capture.set(CV_CAP_PROP_SATURATION, 50);//饱和度 50&#xA;capture.set(CV_CAP_PROP_HUE, 50);//色调 50&#xA;capture.set(CV_CAP_PROP_EXPOSURE, 50);//曝光 50 获取摄像头参数&#xA;得到摄像头的参数&#xA;capture.get(CV_CAP_PROP_FRAME_WIDTH);&#xA;capture.get(CV_CAP_PROP_FRAME_HEIGHT);&#xA;capture.get(CV_CAP_PROP_FPS);&#xA;capture.get(CV_CAP_PROP_BRIGHTNESS);&#xA;capture.get(CV_CAP_PROP_CONTRAST);&#xA;capture.get(CV_CAP_PROP_SATURATION);&#xA;capture.get(CV_CAP_PROP_HUE);&#xA;capture.get(CV_CAP_PROP_EXPOSURE); 获取视频参数：&#xA;capture.get(CV_CAP_PROP_FRAME_COUNT);//视频帧数 然后你会发现除了个别参数你能更改之外（如曝光度），大分布你是不能更改的，甚至都没办法得到，这种并不适用&#xA;2，不做开发，只是单纯的更改 那么推荐一个软件，amcap，百度网盘链接，https://pan.baidu.com/s/1pL8nq0V#list/path=%2F，很简单很容易上手。&#xA;补，现在突然想起来我的一个学长告诉我的，利用这个软件调节摄像头的曝光度，可以改变帧率，且摄像头会记住曝光度的设置（其他特性就没有这个特点）。-2019.3.12&#xA;3，修改opencv的文件，不过效果可能和第一个差不多 大概是在opencv的这个位置，找一下，modules/highgui/src/cap_v4l.cpp，里面有关于参数的设置，位置比较靠前，可以搜索，也可以直接找到&#xA;大致在200多行&#xA;4，v4l2 下面是我找到的一篇参考，可以突破帧率的限制，当然前提是摄像头支持&#xA;https://blog.csdn.net/c406495762/article/details/72732135&#xA;目前只适用于Linux系统，本人试验过，120帧的摄像头在只打开摄像头时可以达到100帧左右，设置的图片分辨率越小，能达到的帧率越高&#xA;#include #include #include #include #include #include #include #include #include #include #include #include #include #include #include #include &#34;opencv2/highgui/highgui.hpp&#34; #include &#34;opencv2/imgproc/imgproc.hpp&#34; #include #include #include #include using namespace std; using namespace cv; #define CLEAR(x) memset(&amp;amp;(x), 0, sizeof(x)) #define IMAGEWIDTH 3264 #define IMAGEHEIGHT 2448 #define WINDOW_NAME1 &#34;</description>
    </item>
    <item>
      <title>opencv图像处理初步（二）：实现色彩还原—（白平衡）</title>
      <link>https://anwangtanmi.github.io/posts/83ba81b35a6389fab98f30b3da89b668/</link>
      <pubDate>Fri, 07 Sep 2018 17:51:04 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/83ba81b35a6389fab98f30b3da89b668/</guid>
      <description>0、说明 目前很多摄像头特别是网络摄像头对色彩的处理情况存在色差，比如一个橙子（是黄色的），但是拍出来的效果会泛白，有点像梨子的颜色，因此要用到色彩校正。&#xA;一般色彩校正使用白平衡，白平衡一般又分为：灰世界、完美反射、等，这里不做具体陈述。&#xA;此处提供了一种方法，总体原理为（对每个通道而言）：&#xA;1）对偏暗和偏亮的颜色进行特定的处理：指定一个特定的像素值；&#xA;2）对其他像素值进行小幅度拉伸；&#xA;3）最后三个通道合并即为最终结果。&#xA;注意：偏暗和偏亮得界限是不对称的，可以同通过改变代码中的s实现。&#xA;具体计算方式可以看源码（c语言实现，非c++）。&#xA;1、效果 2、代码实现 2.1我的实现（调整参数s以符合自己效果） /* 对偏暗和偏亮的的像素进行处理，对其他像素进行拉伸 偏暗和偏亮的判定可以通过调整下面的s进行调整，注意偏暗和偏亮是不对称的 */ #include #include using namespace cv; using namespace std; void color_balance(IplImage *img) { int histo[256] = { 0 };//直方图统计每个像素值的数目 int num_of_pixels = img-&amp;gt;width*img-&amp;gt;height; //统计每个像素值的数目 for (int y = 0; y &amp;lt; img-&amp;gt;height; ++y) { uchar *data = (uchar*)(img-&amp;gt;imageData + y*img-&amp;gt;widthStep);//定义的大小和图像尺寸一致 for (int x = 0; x &amp;lt; img-&amp;gt;width; ++x) { histo[data[x]] += 1; } } //统计当前像素值和之前像素值的总数 for (int i = 1; i &amp;lt; 256; ++i) histo[i] = histo[i] + histo[i - 1]; double s = 0.</description>
    </item>
    <item>
      <title>180709 利用Python与OpenCV裁剪图像做数据增强</title>
      <link>https://anwangtanmi.github.io/posts/ca34ccff47f3f513fa5eda42b6feee1b/</link>
      <pubDate>Mon, 09 Jul 2018 14:33:47 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ca34ccff47f3f513fa5eda42b6feee1b/</guid>
      <description>原始图像 裁剪图像 # -*- coding: utf-8 -*- &#34;&#34;&#34; Created on Mon Jul 9 11:38:19 2018 @author: guokai_liu &#34;&#34;&#34; import numpy as np import matplotlib.pyplot as plt import cv2 filename = &#39;Yuna2.jpg&#39; def crop_figure(fn,kw=100,kh=100,sx=50,sy=50): # assgn saving name sn = fn.split(&#39;.&#39;)[0] # read image img = cv2.imread(fn) # set parameters f_h, f_w,f_c = img.shape k_w = kw k_h = kh s_x = sx s_y = sy # get output numbers of rows and columns n_y = (f_h-k_w)//s_y n_x = (f_w-k_h)//s_x # begin points for rows and columns c_x = [i+s_x*i for i in range(n_x)] c_y = [i+s_y*i for i in range(n_y)] # crop images for idx_y, y in enumerate(c_y): for idx_x,x in enumerate(c_x): crop_img = img[y:y+k_h,x:x+k_w] cv2.</description>
    </item>
    <item>
      <title>Ubuntu16.04&#43;cuda8.0&#43;cudnn5.1&#43;anaconda&#43;tensorflow0.12.1暗影精灵三GTX1080ti</title>
      <link>https://anwangtanmi.github.io/posts/471dee9e5028129552513c002855b571/</link>
      <pubDate>Tue, 08 May 2018 20:47:12 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/471dee9e5028129552513c002855b571/</guid>
      <description>电脑配置：i7-8700K 16G 256GSSD＋2T GTX1080Ti 由于要用tf==0.12.1，所以配置的深度学习环境并不是最新的。想安装最新的ＴＦ可以参考此教程&#xA;1、win10+Ubuntu16.04双系统，安装教程、补充教程。&#xA;2、cuda8.0+cudnn5.1+anaconda 安装教程，建议从作者提供的百度云链接下载，英伟达官网链接有点慢。&#xA;注：安装英伟达的驱动，桌面&amp;gt;&amp;gt;系统设置&amp;gt;&amp;gt;软件和更新&amp;gt;&amp;gt;附加驱动&amp;gt;&amp;gt;选择英伟达的驱动。&#xA;注：ｃｕｄａ的例子没有下载，所以教程中没有ｍａｋｅ例子。&#xA;3、用anaconda安装TF，参照tensorflow官网&#xA;conda create -n tensorflow ##后面加 pip python=2.7 (or 3.X) #Activate the conda environment by issuing the following command: source activate tensorflow (tensorflow)$ # Your prompt should change (tensorflow)$ pip install --ignore-installed --upgrade \ tfBinaryURL ##轮子地址 也可以直接 pip install tensorflow-gpu==0.12.1&#xA;激活conda环境后，进入python&#xA;检验TF和cuda是否安装成功&#xA;# Python import tensorflow as tf hello = tf.constant(&#39;Hello, TensorFlow!&#39;) sess = tf.Session() print(sess.run(hello)) 我的一开始报错，&#xA;I tensorflow/stream_executor/dso_loader.cc:119]Could’t open CUDA library libcudnn.</description>
    </item>
    <item>
      <title>OpenCV实验系列之修改图片对比度与亮度</title>
      <link>https://anwangtanmi.github.io/posts/299886bfc61dd11597c3c2662512e4f0/</link>
      <pubDate>Sat, 17 Feb 2018 19:56:08 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/299886bfc61dd11597c3c2662512e4f0/</guid>
      <description>OpenCV实验系列之修改图片对比度与明亮度 注意：以下内容根据opencv官网提供的教程结合个人理解所得，仅是个人学习笔记，可能存在错误或偏差，欢迎指正。&#xA;OpenCV实验系列之修改图片对比度与明亮度 对比度与亮度的理解个人臆测 实现方法 对比度与亮度的理解（个人臆测） 以下对于对比度的解释来自百度百科&#xA;对比度指的是一幅图像中明暗区域最亮的白和最暗的黑之间不同亮度层级的测量，差异范围越大代表对比越大，差异范围越小代表对比越小，好的对比率120:1就可容易地显示生动、丰富的色彩，当对比率高达300:1时，便可支持各阶的颜色。但对比率遭受和亮度相同的困境，现今尚无一套有效又公正的标准来衡量对比率，所以最好的辨识方式还是依靠使用者眼睛。&#xA;以灰度图来举例，个人理解就是图片中亮与暗点间的灰度差值，比如255与200的对比度小于255与100的对比度，对于灰度图来说亮度就是整体的白的程度。由此可以得出对对比度和亮度进行调节的公式： α用来控制对比度，β用来控制亮度。可以想象当α&amp;gt;1是图片的对比度将扩大（扩大了像素点间的差异），β&amp;gt;0时亮度将提高，当0&amp;lt;α&amp;lt;1是图片的对比度将j减小，β&amp;lt;0时亮度将减小。&#xA;实现方法 可以使用LUT() http://blog.csdn.net/Nrush/article/details/79330077 进行处理在下例中使用一般遍历的方法进行处理。&#xA;#include #include #include #include using namespace std; using namespace cv; void main() { double alpha = 1.5; double beta = 0; Mat src; src = imread( &#34;timg.jpg&#34; ); imshow(&#34;src&#34;,src); Mat dst = Mat::zeros( src.size(), src.type()); for( int y = 0; y &amp;lt; src.rows; y++ ) { for( int x = 0; x &amp;lt; src.cols; x++ ) { for( int c = 0; c &amp;lt; 3; c++ ) { dst.</description>
    </item>
    <item>
      <title>图像增强:多尺度的图像细节提升(multi-scale detail boosting)实现方法</title>
      <link>https://anwangtanmi.github.io/posts/4719a3d36e83dd0ac5be01ea9aefdbc0/</link>
      <pubDate>Tue, 23 Jan 2018 11:49:42 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4719a3d36e83dd0ac5be01ea9aefdbc0/</guid>
      <description>图像增强:多尺度的图像细节提升(multi-scale detail boosting)实现方法 看到一篇博客介绍基于多尺度的图像的细节提升算法，其参考论文《Dark image enhancement based onpairwise target contrast and multi-scale detail boosting》，下图是该论文的核心算法过程。然后自己在Matlab和OpenCV实现了该算法，最终实现的效果还是不错的，可以增强图像的细节部分。 论文的核心算法过程：&#xA;论文的核心思想类似于Retinex，使用了三个尺度的高斯模糊，再和原图做减法，获得不同程度的细节信息，然后通过一定的组合方式把这些细节信息融合到原图中，从而得到加强原图信息的能力：请参考这一篇博客介绍：http://www.cnblogs.com/Imageshop/p/7895008.html，该博客给出了SSE的实现过程。 算法实现很容易，下面，我给出本人的OpenCV和Matlab实现方法： OpenCV实现方法： #include #include #include using namespace std; using namespace cv; cv::Mat multiScaleSharpen(cv::Mat Src, int Radius) { int rows = Src.rows; int cols = Src.cols; int cha = Src.channels(); cv::Mat B1, B2, B3; GaussianBlur(Src, B1, Size(Radius, Radius), 1.0, 1.0);//高斯模糊 GaussianBlur(Src, B2, Size(Radius*2-1, Radius*2-1), 2.0, 2.0); GaussianBlur(Src, B3, Size(Radius*4-1, Radius*4-1), 4.0, 4.0); double w1 = 0.</description>
    </item>
    <item>
      <title>openCV学习笔记（七）：傅里叶变换</title>
      <link>https://anwangtanmi.github.io/posts/1b724a8c45999e71f70bd4c6c090e3ee/</link>
      <pubDate>Fri, 22 Dec 2017 13:51:32 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1b724a8c45999e71f70bd4c6c090e3ee/</guid>
      <description>之前熟悉了openCV怎么调整图片对比度和亮度（通过线性和非线性的方法），都是很基础的图形操作，这边开始介绍openCV图像处理很重要的工具方法：傅里叶变换。&#xA;这部分内容涉及到复杂的傅里叶公式、也涉及到很多数学上的原理，单单通过官网是根本不会理解这部分内容的，我也是翻阅了很多的资料才对这部分内容有了比较清晰的了解，这边我通过用openCV的傅里叶变换方法实现图片校正的功能，里面涉及到的方法和原理我尽量详细的介绍。&#xA;上一篇内容：http://blog.csdn.net/jbl20078/article/details/78854660&#xA;openCV 傅里叶变换实际应用：图像旋转校正 第一部分：什么是傅里叶变换 光是学习傅里叶变换就花了我多半天的时间，后悔大学没好好学呀，这部分内容如果想深入了解的话看这个神人写的吧，已经非常用心并且很耐看： http://blog.jobbole.com/70549/ 如果对傅里叶变换还模糊或者看不下去的话也没关系，我们了解它在图形处理方面的意义和能处理什么问题就可以了。 第二部分：傅里叶变换在图形处理方面的实际意义和应用 如果第一部分详读了之后，应该会清楚傅里叶变换后其实是描述频率的二维数据，有了可以描述图像的频率（或者波频）我们能分辨出图像的高频部分和低频部分，高频部分就是图像对比度很大的区域（高频点趋近白色），低频部分是图像对比度比较小的部分（低频趋近于暗色），图像的边缘通常就是高频，因为这个区域色差普遍很大，所以傅里叶变换后的图像数据会比较方便的处理边缘（为我们边缘检测和图片方向检测提供了可能），如果你了解图像噪音的话（由于图像的传输等原因，图像中会有噪点）傅里叶通过对频率值的分析可以去除图像噪音，达到增强图像的效果。 当然，傅里叶应用的地方还有很多，比如图像特征值提取等，正是由于它的用途这么大，我们才一定要花大力气搞懂它的使用方法和原理，对我们后面图形的处理会有极大的帮助。 下面是傅里叶变换得到的波谱图（或者叫频率图，幅度图，无所谓，我们知道它是描述图形变化频率的数据就ok了）。 后面我会介绍怎么通过傅里叶变换后的波频图来给上面的图片扶正。 第三部分：实现图片的傅里叶变换 傅里叶变换需要经历很多步骤，我们分解一步步来实现： 1、读取原始图形数据,并将图像转化成灰度图（因为彩色图很多数据对我们没用，灰度值就够了，而且计算量小） Mat mat = [[CVUtil sharedHelper]cvMatFromUIImage:_buildImg.image]; Mat grayImg; cvtColor(mat, grayImg, COLOR_BGR2GRAY); 我读取图形的方式跟官网不同，因为我是在IOS系统上模拟测试的，这个根据自己测试的平台来修改 2、扩充灰度图的尺寸并且填充像素（图形尺寸是2，3，5倍数的时候傅里叶变换计算速度最快），这些接口openCV都提供了 Mat padded; int m = getOptimalDFTSize(grayImg.rows); int n = getOptimalDFTSize(grayImg.cols); //通过copyMakeBorder进行填充像素和颜色(多出的像素点全部用0填充) copyMakeBorder(grayImg, padded, 0, m-grayImg.rows, 0, n-grayImg.cols, BORDER_CONSTANT, Scalar::all(0)); 这部分涉及到的接口都很简单，看参数都能明白是做什么，如果不清楚，调到代码中看方法描述，或者去下载源码阅读。 3、扩充图形的通道，准备调用离散傅里叶变换方法 看了第一部分内容或者了解傅里叶变换公式的同学知道，傅里叶变换得到的结果是一个复数，实数和虚数部分为了都能保存到我们傅里叶变换后的结果，我们要给上面的灰度图 扩充一个值全部为0的通道，并且把数值转化成float类型。最后得到的傅里叶变换结果我们会将实数部分和虚数部分拿出来计算幅度值（就是我们想要得到的频率图）做后面的校正 图形，下面会说。 //傅里叶变化后的结果是一个复数，也就是转化到频域中会有两个图像值，我们将图像转化成float类型 并且添加一个额外通道来存储复数部分 Mat planes[] = {Mat_(padded),Mat::zeros(padded.size(), CV_32F)}; //复制了一个padded，然后添加了一个全是0的通道 Mat complex; //进行通道的混合 //merge函数是合并多个array 成为一个多通道的array，比方说array1 array2 合并成array3 那么array3[1][0] = array1[1] //array3[1][2] = array2[1] 它的逆向操作是split方法 merge(planes,2,complex); //合并好了 相当于给傅里叶变换的结果预先分配了存储空间（因为结果是复数）所以下面就可以进行傅里叶变换了 //dft函数（离散傅里叶变换） //输入输出支持同一个图像 dft(complex,complex); 4、计算幅度图，得到频域图谱 傅里叶计算结束，我们得到了一个双通道的Mat数据，第一个通道是傅里叶变换数据的实数，第二个通道是傅里叶变换数据的虚数，这两者具体几何意义是什么，第一部分的连接 文章都有讲，不清楚的小伙伴可以回头看下，或者我们清楚下面的概念： 我们一般用幅度图像来标识图像傅里叶的变换结果，幅度的计算公式： Re(DFT)是实数，IM(DFT)是虚数</description>
    </item>
    <item>
      <title>openCV学习笔记 (六) : 改变图像的对比度和亮度</title>
      <link>https://anwangtanmi.github.io/posts/5e8b0dd582fb46fa29dde935f4db5679/</link>
      <pubDate>Wed, 20 Dec 2017 16:27:27 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/5e8b0dd582fb46fa29dde935f4db5679/</guid>
      <description>上一篇熟悉了filter2D函数，通过掩码矩阵实现滤波功能（锐化图片），这里继续熟悉其他处理图像的方式：改变图像对比度和亮度。&#xA;注意：&#xA;如果连续关注我笔记的同学会发现，这里我跳过了官网核心模块中的两部分内容（只是没有做总结）：一个是图形的基本操作（Load、write，以及像素点获取的方式），一个是图片的线性混合。希望跟我一样学习的同学最好按照步骤去熟悉这两部分内容，openCV官网的教程安排还是很合理的，基础部分经常会在后面章节中用到。&#xA;上一篇：openCV实现滤波功能：http://blog.csdn.net/jbl20078/article/details/78852194&#xA;官网地址：https://docs.opencv.org/master/d3/dc1/tutorial_basic_linear_transform.html&#xA;调整图像的对比度和亮度（通过线性方法） 一提到线性方法我们第一时间应该会联想到：二元一次方程 这里openCV介绍的一个函数方法就是利用二元一次方程线性修改图像中的像素值 这种算子能够调整图片亮度和对比度，当然也可以分区域的设置，这个后面用到再说，直接看源码的实现： void convertToImage(Mat&amp;amp; mat,Mat outputMat,double alpha,int beta){ //遍历这个mat for(int i = 0 ; i &amp;lt; mat.rows;i++){ for(int j = 0 ; j &amp;lt; mat.cols; j++){ for(int c = 0; c &amp;lt; 3; c++){ outputMat.at(i,j)[c] = saturate_cast(alpha*(mat.at(i,j)[c])+beta); } } } } 源码很简单，openCV提供给我们的实现函数式Mat.convertTo(OutputArray m , int rtype, double alpha = 1, double beta = 0); 参数说明下：&#xA;outputArray就是输出的目标Mat&#xA;rtype是深度，和之前用到的filter2D函数一样，-1代表使用跟源一样的depth，或者填写image.depth()&#xA;alpha就是α&#xA;beta就是β&#xA;（后面还会介绍γ方法）&#xA;使用方法：&#xA;//先创建一个outputmat Mat outputMat = Mat::zeros(oriMat.</description>
    </item>
    <item>
      <title>【PhotoShop】用自己的照片做个好看的星空头像PS</title>
      <link>https://anwangtanmi.github.io/posts/b52be374df67e1a840c2377665a5be80/</link>
      <pubDate>Wed, 16 Aug 2017 16:01:04 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b52be374df67e1a840c2377665a5be80/</guid>
      <description>图片素材均来自于网络。&#xA;1.打开图像，把图片直接拖入ps 2.选择 多边形套索 工具 3.圈出想要的部分 4.图像 》》调整 》》阈 (yu四声) 值 5.调整小三角，直到看清五官(我这个素材选择有点不太好，有点形成阴阳脸，就是说 明暗差距有点大，调出来也是一面黑一面白。只能看清楚一边。)&#xA;6.调整好之后，点击确定。然后按Ctrl+J 把这部分烤出来。 7.点击 选择》》色彩范围》》吸取头发黑色的部分颜色（这时候对话框里的脸会变黑。） 8.点击确定。Ctrl+N 新建一个页面。参数自定就行。我这里的参数有点大。分辨率尽量是72 9.这一步可以跳过，这一步的目的只是为了让之后的结果看得更加清楚。右击新建的图层前面的缩略图，选择图层样式，把图层样式改为如下参数。 10.点击回到刚才的选项卡，按“V”字母键，切换到移动工具，把我们的“脸”拖到第二个选项卡（新建的页面） 拖过来 是这样的 11.把之前准备好的星空素材拖进来。。星空素材的图层应该在 “脸”图层之上。 12.（这一步很关键）按住Alt，把鼠标移动到 星空和“脸”中间，会出现如图空白处绘制的形状，然后单击鼠标左键。 13.把可能多余的部分用左侧橡皮擦工具擦掉，如果嫌麻烦，那你前期就应该仔细吧脸部裁出来。&#xA;14.如果想在下面打上字。同样的操作方法。打完字以后，再拖进来一张“星空”，星空图层在字的上面，按住Alt并且单击鼠标左键（位置在星空图层和字图层的中间），就ok啦！ 效果如图： 15.最终出来的图片如图。 挚谢阅读。</description>
    </item>
    <item>
      <title>JavaFx&#43;openCv项目在win7系统部署异常（no opencv_java310 in java.library.path）</title>
      <link>https://anwangtanmi.github.io/posts/168415e43715e36f2c57bb43ec25ae26/</link>
      <pubDate>Sat, 01 Jul 2017 14:54:53 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/168415e43715e36f2c57bb43ec25ae26/</guid>
      <description> 【学习参考】 JavaFx+openCv项目代码参考官网教程点击跳转 JavaFx项目部署参考点击跳转 【问题解决】 以上是学习的参考，实际操作部署后，运行exe文件会出现两个错误弹窗： Error invoking method！ Failed to launch jvm！ .exe程序无法执行。&#xA;根据网上的找的半自动解决方法【直接执行jar包看异常】，出现以下异常内容。&#xA;java.lang.reflect.InvocationTargetException &amp;lt;省略部分异常&amp;gt; Caused by: java.lang.UnsatisfiedLinkError: no opencv_java310 in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at application.Main.main(Unknown Source) ... 11 more Exception running application application.Main 然后根据java.lang.UnsatisfiedLinkError: no opencv_java310 in java.library.path继续查找问题，看了多个解决方法后，明白问题的来源是程序中System.loadLibrary(Core.NATIVE_LIBRARY_NAME);加载不到opencv_java310.dll，而我用eclipse调试时因为按照教程对该参数进行设置过了，所以没有报错，但是部署的文件并不能获取到该参数，因此异常。所以对该加载opencv_java310.dll程序进行修改即可。修改如下：（两种方法，选其一即可） 相对路径方法&#xA;第一步: System.loadLibrary(Core.NATIVE_LIBRARY_NAME); 修改为: String relativelyPath=System.getProperty(&#34;user.dir&#34;); System.load(relativelyPath+&#34;\\opencv_java310.dll&#34;); 第二步: 部署安装，但还是会报错 第三步： 找到部署的文件安装位置，默认C:\Users\[用户名]\AppData\Local\[项目名]，然后打开其下的app文件夹，将opencv_java310.dll拷贝到这里。 执行上层文件夹中的exe文件即可正常运行，无报错。 绝对路径方法：&#xA;System.loadLibrary(Core.NATIVE_LIBRARY_NAME); 修改为: System.load(&#34;E:\\opencv\\build\\java\\x64\\opencv_java310.dll&#34;); 以上路径根据你的opencv_java310.dll实际位置修改。 重新打包部署，然后执行无报错。 </description>
    </item>
    <item>
      <title>Opencv 分水岭算法用于图像分割</title>
      <link>https://anwangtanmi.github.io/posts/cc8e7acc035031e1fddb61c9285a5487/</link>
      <pubDate>Fri, 26 May 2017 11:36:06 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/cc8e7acc035031e1fddb61c9285a5487/</guid>
      <description>目标 • 使用分水岭算法基于掩模的图像分割 • 学习函数： cv2.watershed() 原理 任何一幅灰度图像都可以被看成拓扑平面，灰度值高的区域可以被看成是山峰，灰度值低的区域可以被看成是山谷。我们向每一个山谷中灌不同颜色的水，随着水的位的升高，不同山谷的水就会相遇汇合，为了防止不同山谷的水汇合，我们需要在水汇合的地方构建起堤坝。不停的灌水，不停的构建堤坝直到所有的山峰都被水淹没。我们构建好的堤坝就是对图像的分割。这就是分水岭算法的背后哲理。 但是这种方法通常都会得到过度分割的结果，这是由噪声或者图像中其他不规律的因素造成的。为了减少这种影响， OpenCV 采用了基于掩模的分水岭算法，在这种算法中我们要设置哪些山谷点会汇合，哪些不会，这是一种交互式的图像分割。我们要做的就是给我们已知的对象打上不同的标签。如果某个 区域肯定是前景或对象，就使用某个颜色（或灰度值）标签标记它。如果某个区域肯定不是对象而是背景就使用另外一个颜色标签标记。而剩下的不能确定是前景还是背景的区域就用 0 标记。这就是我们的标签。然后实施分水岭算法。每一次灌水，我们的标签就会被更新，当两个不同颜色的标签相遇时就构建堤 坝，直到将所有山峰淹没，最后我们得到的边界对象（堤坝）的值为 -1。&#xA;代码 下面的例子中我们将就和距离变换和分水岭算法对紧挨在一起的对象进行分割。 如下图所示，这些硬币紧挨在一起。就算你使用阈值操作，它们任然是紧挨着的。 我们从找到这些硬币的近似估计值开始，我们使用Otsu’s二值化。 import cv2 import numpy as np from matplotlib import pyplot as plt img = cv2.imread(&#39;image/coins.png&#39;) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ret,thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU) 结果图： 现在我们要去除图像中的所有的白噪声,这就需要使用形态学中的开运算。为了去除对象上小的空洞我们需要使用形态学闭运算。所以我们现在知道靠近对象中心的区域肯定是前景，而远离对象中心的区域肯定是背景。而不能确定的区域就是硬币之间的边界。&#xA;所以我们要提取肯定是硬币的区域。腐蚀操作可以去除边缘像素。剩下就可以肯定是硬币了。当硬币之间没有接触时，这种操作是有效的。但是由于硬币之间是相互接触的，我们就有了另外一个更好的选择：距离变换再加上合适的阈值。接下来我们要找到肯定不是硬币的区域。这是就需要进行膨胀操作了。膨胀可以将对象的边界延伸到背景中去。这样由于边界区域被去处理，我们就可以知道那些区域肯定是前景，那些肯定是背景。&#xA;剩下的区域就是我们不知道该如何区分的了。这就是分水岭算法要做的。这些区域通常是前景与背景的交界处（或者两个前景的交界）。我们称之为边界。从肯定是不是背景的区域中减去肯定是前景的区域就得到了边界区域。 kernel = np.ones((3,3),np.uint8) opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel,iterations= 2) sure_bg = cv2.dilate(opening, kernel,iterations=3) dist_transform =cv2.distanceTransform(opening, 1, 5) ret,sure_fg = cv2.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0) sure_fg = np.</description>
    </item>
    <item>
      <title>OpenGL开启Gouraud明暗处理，减少马赫夫效应</title>
      <link>https://anwangtanmi.github.io/posts/2558b4dc5fd51aaae1b0f6e7c6311863/</link>
      <pubDate>Tue, 18 Apr 2017 10:23:41 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2558b4dc5fd51aaae1b0f6e7c6311863/</guid>
      <description>在我们建立三维模型的过程中，当用多边形网格或者是三角面片来近似的表示曲面的表面时，曲面被离散成许多的平面多边形，如果我们的网格较大，离散度较粗，在模型表面使用明暗处理后，两两相邻的多边形会出现凸起或者是凹陷的折痕，在连接处显得比周围处亮或者暗，这就是所谓的马赫夫效应，如下图所示&#xA;针对于出现的马赫夫效应，我们需要进行明暗处理，消除或者是减少三维模型的马赫夫效应，使其看上去更加的光滑美观，常用的明暗处理技术有双线性光强插值—Gouraud明暗处理技术和双线性法向插值-Phong明暗处理技术。以下展示经过明暗处理前后结果对比&#xA;OpenGL提供了两种着色模式void glShadeModel ( GLenum mode)，恒定着色GL_FLAT，光滑着色GL_SMOOTH，而GL_SMOOTH中则是使用了Gouraud明暗处理技术，对于Phong明暗处理技术可以参见http://blog.csdn.net/dalewzm/article/details/46291397&#xA;http://blog.csdn.net/silangquan/article/details/10011169&#xA;Gouraud明暗处理算法在处理亮度的不连续性方面很有效，但是在明暗强度函数的斜率急剧变化处仍然可以看到马赫夫效应，不能完全消除光强度的不连续性。而Phong明暗处理是对表面的法向量而不是亮度进行插值，大大改善了Gouraud模型对高亮度镜面反射光的处理，在每一点都是用法向量的一个近似值，所以一般法向量插值的结果要优于亮度插值，在很大程度上消除了马赫夫效应，但是会大大增加明暗处理的时间。&#xA;鉴于此，我在用MC算法建立三维模型的时候，由于我的网格设置较大，导致出现了马赫夫效应，即模型表面的可视化效果不光滑，如下：&#xA;于是使用OpenGL自带的Gouraud明暗处理技术，以一个MC算法生成的章鱼模型为例子，效果如下&#xA;相比未使用明暗处理的模型，使用了Gouraud处理的模型从可视化的角度上来看更加的光滑，效果更好。</description>
    </item>
    <item>
      <title>OpenCV实践（3）- 改变图像的对比度和亮度</title>
      <link>https://anwangtanmi.github.io/posts/d1d9f95f269dfc53a827cb91bed5ef94/</link>
      <pubDate>Wed, 01 Feb 2017 17:45:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d1d9f95f269dfc53a827cb91bed5ef94/</guid>
      <description>1 目标 （1）访问像素值； （2）初始化矩阵为0； （3）学习saturate_cast做什么和它为什么有用？ （4）Get some cool info about pixel transformations 2 理论 可以参考[计算机视觉：算法和应用](http://szeliski.org/Book/)一文。 3 图像处理 （1）图像处理运算就是一个函数把输入的一个或多个图像，转换为输出图像的过程； （2）图像变换可以被看成： a) 像素点的转换； b) 邻域的操作（基于域的概念）； 4 像素变换 在这类转换中，每一个输出像素点仅仅依赖于相对应的输入像素点（潜在地附加一些收集的全面信息和参数）。 这类操作的例子有：亮度和对比度的调整，以及色彩校正和转换。&#xA;4.1 亮度和对比度调整 （1） 理论公式为 （2） 在这里， 和 被称为增益和偏差参数，有时候，也被认为分别控制对比度和亮度。 （3）你可以认为f(x)是输入图像的像素值，g(x)是输出图像的像素值。那么为了方便我们就可以把上面的公式写为： 在这里，i和j分别代表像素点在行和列的位置。&#xA;4.2 代码实现 #include #include #include using namespace cv; double alpha; /**&amp;lt; Simple contrast control */ int beta; /**&amp;lt; Simple brightness control */ int main( int argc, char** argv ) { /// Read image given by user Mat image = imread( argv[1] ); Mat new_image = Mat::zeros( image.</description>
    </item>
    <item>
      <title>OpenGL学习脚印：伽马校正(Gamma Correction)</title>
      <link>https://anwangtanmi.github.io/posts/9a7e808f5c667c654514e973d626b66e/</link>
      <pubDate>Sun, 30 Oct 2016 21:35:29 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9a7e808f5c667c654514e973d626b66e/</guid>
      <description>写在前面 由于CRT,LED等显示设备显示颜色时并非按照线性方式工作，因此我们在程序中输出的颜色，最终输出到显示器上时会产生亮度减弱的现象，这种现象在计算光照和实时渲染时对图形质量有一定影响，需要我们加以处理。本节将熟悉Gamma校正的概念，并通过点光源的示例来表现Gamma校正对图形效果的影响。本节示例代码均可以在我的github下载。&#xA;为什么需要&#xA;γ&#xA;校正 我们在图形程序中认为(1.0, 0.0,0.0)颜色应该是(0.5,0.0, 0.0)的红色强度的2倍，但是实际上在显示设备上对于输入的原始值，并不按照线性的关系来反映这个亮度差别。显示设备对于输入的原始值，按照一种非线性的方式处理，也就是说上面原始值中红色分量提高2倍时，实际处理时得到的亮度并不是成这种2倍的比例。&#xA;事实上，CRT、LED这类显示设备，对于输入的原始值，会计算一个指数函数，即： Cout=C2.2in&#xA;。这里的2.2称为 伽马系数(Gamma factor)，范围一般在2.0到2.4之间，不同显示器这个系数有区别。这个关系如下图所示（来自Gamma Correction）：&#xA;上面的图中间的点线，是我们图形中工作的线性颜色，当显示器接受这个线性原始值后，进行了上面的指数计算，将会输出图中下方的实线所示的非线性的颜色，亮度会降低。对于0。0和1.0两个值显示器总是原样输出，两个值之间的颜色亮度将会降低。&#xA;因此，要想最终显示器按照我们预期的输出中间的线性颜色，我们就需要执行上述指数运算的逆运算：&#xA;Ccorrected=C1.0/2.2out&#xA;，这样最终输出颜色，才会是我们在图形应用中指定的颜色。 执行这个逆运算的过程，称之为伽马校正(Gamma Correction)，一般的取伽马系数2.2进行校正产生的效果在大多数显示器上都比较理想，实际还可以根据显示器不同进行查询。例如上面的颜色(0.5,0.0,0.0)执行校正后的值为：&#xA;(0.5,0.0,0.0)1/2.2=(0.5,0.0,0.0)0.45=(0.73,0.0,0.0)&#xA;则实际显示为： (0.73,0.0,0.0)2.2=(0.5,0.0,0.0)&#xA;也就是说要让显示器显示我们预期的&#xA;(0.5,0.0,0.0)&#xA;，我们的着色器需要输出&#xA;(0.73,0.0,0.0)&#xA;。&#xA;另外一个需要知道的事实时，大多数的图片在显示器已经被预处理过了，因此存储的颜色信息已经是伽马校正了的，我们在处理纹理时需要工作在线性的颜色空间下，因此需要对输入的图片，例如JPEG纹理进行处理，保证它在线性颜色空间下。关于实际操作，后面会介绍。&#xA;有了上面的基础，我们直观感受下，有无伽马校正，下面是绘制人面皮肤的效果图，图形对比如下(来自The Importance of Being Linear)：&#xA;上面的左图中，将输入的纹理进行了线性处理，同时最终输出时进行了伽马校正；而右边的图形两个处理都没有进行，因而面部的镜面光加上漫反射光后变成了黄色，同时面部的阴影变得太暗了。&#xA;处理输入的纹理 对于输入的纹理图片，由于大多数情况下已经进行了伽马校正，因此我们要得到他们线性的颜色，必须去除这个校正，在OpenGL中提供了两种方式。&#xA;方式一是通过在构造纹理对象时，指明纹理的内部格式为sRGB或者GL_SRGB_ALPHA，SRGB就是我们所说的已经校正后的非线性颜色，当按上述方式指明时OpenGL将会自动将其转换到线性空间。&#xA;glTexImage2D(GL_TEXTURE_2D, 0, GL_SRGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, image); 方式二是在着色器中开始使用纹理时，手动执行转换：&#xA;float gamma = 2.2; vec3 diffuseColor = pow(texture(diffuse, texCoords).rgb, vec3(gamma)); 这种方式是在着色器中执行伽马变换的逆操作，相比上一种方式，显得麻烦些。&#xA;需要注意的是，对于某些类型的纹理，例如diffuse map，通常已经工作在线性空间了，这个时候就不需要指明为sRGB格式了。在指明为sRGB格式时也需要根据不同情况加以区别。&#xA;处理着色器的输出 对于着色器的输出，我们有两种方式执行伽马校正。第一种方式，即采用OpenGL提供的FBO的选项，当开启 GL_FRAMEBUFFER_SRGB选项后，随后的所有FBO绘制将自动执行校正，包括默认的FBO。&#xA;glEnable(GL_FRAMEBUFFER_SRGB); 当开启这个选项后，执行的伽马校正，通常是2.2的校正。需要注意的是，如果你使用多个FBO来完成某项操作，当需要在中间步骤中使用线性颜色空间时，就需要关闭这个选项，否则将会在错误的颜色空间上工作。一般是在最后一个FBO上操作时开启这个选项。&#xA;第二种方式是在着色器的输出中，手动进行伽马校正：&#xA;vec3 result; result = pow(result, vec3(1.</description>
    </item>
    <item>
      <title>OpenGL学习脚印：Blinn-Phong光照模型</title>
      <link>https://anwangtanmi.github.io/posts/3dfbc8de0d42f9f2bea209f88f3bf82c/</link>
      <pubDate>Sat, 29 Oct 2016 16:01:56 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3dfbc8de0d42f9f2bea209f88f3bf82c/</guid>
      <description>写在前面 在前面基础光照部分，我们学习了Phong Shading模型，Blinn-Phong模型对Phong模型的镜面光成分进行了改进，虽然在物理上解释没有Phong好，但是能更好地模拟光照。本节代码可以在我的github下载。&#xA;本节内容整理自： 1.www.learnopengl.com 2.Blinn-Phong Model&#xA;Phong不能处理的情况 我们知道，Phong模型在计算镜面光系数为：&#xA;float specFactor = pow(max(dot(reflectDir, viewDir), 0.0), 32); // 32为镜面高光系数 这里的计算由反射向量和观察向量决定，当两者的夹角&#xA;θ&#xA;超过90时，截断为0.0，则没有了镜面光成分。因此Phong模型能处理的是下面的左图中(&#xA;θ≤90&#xA;)的情况，而对于右图中(&#xA;θ&amp;gt;90&#xA;)的情况则镜面光成分计算为0(来自Advanced-Lighting)。 而右图的这种情况实际上是存在的，将镜面光成分取为0，没有很好地体现实际光照情况。例如下面的图表示的是，镜面光系数为1.0，法向量为(0.0,1.0,0.0)的平面位置在-0.5,光源在原点时，观察者在(0,0,4.0)位置时，光照展示的情形：&#xA;这里我们看到，Phong的镜面光成分，在边缘时立马变暗，这种对比太明显，不符合实际情形。&#xA;为什么会产生这样一个光线明暗分明的情形? 我尝试这样推导，对此不感兴趣地可以跳过。&#xA;首先记表面位置为&#xA;fragPos=(x,−0.5,z)&#xA;, 光源位置为&#xA;lightPos=(0.0,0.0,0.0)&#xA;，则光照向量为：&#xA;L=−(lightPos−fragPos)=(x,−0.5,z)(light direction)&#xA;法向量为：&#xA;N=(0.0,1.0,0.0)(surface normal)&#xA;根据&#xA;reflect函数的计算原理，得到反射向量为：&#xA;R=L−2.0∗dot(N,L)∗N=(x,−0.5,z)−2.0∗(−0.5)∗(0.0,1.0,0.0)=(x,0.5,z)(surface reflection)&#xA;设观察点位置为&#xA;(x′,y′,z′)&#xA;，则观察向量为： V=(x′,y′,z′)−fragPos=(x′−x,y′+0.5,z′−z)(viewer direction)&#xA;那么反射向量和观察向量的点积为：&#xA;dot(R,V)=(xx′−x2+0.5y′+0.25+zz′−z2)=−[(x−x′2)2+(z−z′2)2−0.25(x′2+z′2+1+2y′)]&#xA;记&#xA;δ=0.25(x′2+z′2+1+2y′)&#xA;令&#xA;0≤dot(R,V)≤1&#xA;,得到：&#xA;δ−1≤(x−x′2)2+(z−z′2)2≤δ&#xA;由此可以看出，位置在平面y=-0.5上的点，以适当位置观察时，会形成两个同心圆，在两个同心圆之间的部分则满足&#xA;0≤dot(R,V)≤1&#xA;，这部分有镜面光，其余部分截断为0.0，立马变暗，因此有这种明暗对比。 也就是说当观察向量和反射向量超过90度，这种截断引起了明显的明暗对比，这种情形在Blinn-Phong中得到改善。&#xA;Blinn-Phong Blinn-Phong模型镜面光的计算，采用了半角向量(half-angle vector)，这个向量是光照向量L和观察向量V的取中向量，如下图所示(来自Blinn-Phong Model)：&#xA;计算为: H=L+V||L+V||&#xA;当观察向量与反射向量越接近，那么半角向量与法向量N越接近，观察者看到的镜面光成分越强。&#xA;对比Phong和Blinn-Phong计算镜面光系数为：</description>
    </item>
    <item>
      <title>Opencv学习之图像边缘检测</title>
      <link>https://anwangtanmi.github.io/posts/d0bd1b2ff36387c0194a9439c3d258a6/</link>
      <pubDate>Tue, 16 Aug 2016 13:48:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d0bd1b2ff36387c0194a9439c3d258a6/</guid>
      <description>该博客只为记录点滴所学，若有误导，还请大家原谅，并不吝赐教。&#xA;一直不太明白应用函数求导的方法能求出图像强度变化的边缘，其实就是图像边缘检测。现在终于想通了，解释如下。比如一张如下的图片：&#xA;图1&#xA;检测边缘可以检测到狗和兔子的轮廓。因为它们的轮廓跟其周围的像素值强度差是很明显的。那怎样理解这个呢？我们先假设有一个一维的图片，一维的图片就是一条线。它可以如下表示：&#xA;图2&#xA;横轴表示像素坐标，纵轴表示像素值。(如果不太好理解的话，我们可以对应二维图片来理解，像素坐标就是诸如(x,y)这样的值，只不过一维图片只需一个值来表示像素坐标，这里只需用t表示。像素值是一样理解的。)&#xA;再说回上面图2那张图片，可以看到在红色圆圈处像素值跳跃比较大。即在同样的delta t区间里，该处的像素值变化最大。而此处的导数值也越大，趋于无穷。于是我们就可以将其看成是一维图像的边缘。将其延伸到二维图片，即我们所认知的看到的普通的图片，也是一样的道理。我们可以将其拆分成两个轴向的一维图片，然后组合起来。&#xA;综上，就解释了为什么求导可以得出一张图片的边缘。比如我们假设当导数超过一个限度之后，就认为它是边缘，我们就把它的像素值设为255，否则设为0.</description>
    </item>
    <item>
      <title>地球实时卫星图片资源-与当前时间只相差30分钟</title>
      <link>https://anwangtanmi.github.io/posts/0798b6ead83f3e5ed7e8d2dd0466610d/</link>
      <pubDate>Mon, 18 Jul 2016 22:10:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0798b6ead83f3e5ed7e8d2dd0466610d/</guid>
      <description> 地球实时卫星图片资源-与当前时间只相差30分钟 在这里跟大家分享一个有趣的项目。这个项目提供一个实时地球照片源，通过向其服务器发送请求，能抓取到当前地球的照片。对于图片壁纸类的应用来说是一个不错的图片源选择。有550*550、1100*1100、2200*2200分辨率图片资源，您可以将它们集成在您的应用当中，让它成为您的应用的一个绝对亮点。&#xA;该项目的图片来自卫星实时拍摄的地球照片，更新频率为每十分钟一次，如下所示图样：&#xA;该项目不仅提供这些资源，而且还提供配套的基于HTTP网络协议的接口，同时提供全套主流语言 SDK ，如 C、C++、java、object-c 等，用户可以根据自己的开发语言，方便快捷的接入服务。同时也提供原始的HTTP协议与相关的文档和辅助开发的测试工具。&#xA;该项目的图片云服务端从性能、安全性、可扩展性为不同的应用需求提供了全套可靠的服务。&#xA;在可扩展性方面针对实时图片，该项目可以提供不同分辨率的图片，550 * 500、1100 * 1100、2200 * 2200等，并且提供裁剪、压缩、缩放、格式转换、图片效果（调整亮度、对比度）等等功能服务。 该项目提供原始无损的PNG图片，同时也支持在服务端将图片转换成JPG、BMP、webp等格式的图片。 在性能上面，该项目提供了BGP高速的云服务器，同时提供全网的CDN加速分发的功能，保证APP在任何网络下面都能够快速的获取图片。 在安全性方面，该项目以HTTP协议为基础，提供了一套安全体系，保证生成的图片URL有安全验证、有过期时间，确保不被盗链。 更多的信息请查看:&#xA;官方网站 www.runimg.com&#xA;演示图片 GMT+0 2016年4月28日00时10分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日00时20分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日00时30分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日00时40分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日12时00分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日12时10分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日12时20分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日12时30分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png GMT+0 2016年4月28日12时40分00秒 http://www.runimg.com/demo_img/2016_04_28_00_10_00_4d.png 文档工具 协议文档 http://www.kancloud.cn/khvysofq/runimg/101183 辅助测试工具 http://www.runimg.com/signature.html SDK JAVA SDK https://github.com/jinjiangztc/image_download_java.git C++ SDK https://git.oschina.net/guangleihe/runimg-sdk.git Object-C SDK https://github.com/itayou/RunimgService.git </description>
    </item>
    <item>
      <title>OpenGL(三)图形变换之裁剪变化</title>
      <link>https://anwangtanmi.github.io/posts/8face7149546764efc4adb36f326b9ce/</link>
      <pubDate>Fri, 08 Jul 2016 12:37:12 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/8face7149546764efc4adb36f326b9ce/</guid>
      <description>通过裁剪平面程序例子学习裁剪相关知识&#xA;#include #include #include static GLfloat xRot =0.0f; static GLfloat yRot = 0.0f; void SetupRC(void) { glClearColor(1.0f, 1.0f, 1.0f, 1.0f); glShadeModel(GL_FLAT); //明暗模式 以最后的所填颜色填充 } void ChangeSize(int w, int h) { if (h == 0) h = 1; //设置视区尺寸 glViewport(0, 0, w, h); glMatrixMode(GL_PROJECTION); glLoadIdentity(); //设置修剪空间 GLfloat fAspect = (float)w / (float)h; gluPerspective(60, fAspect, 1.0, 20.0f); glMatrixMode(GL_MODELVIEW); } //在窗口中绘制图形 void RenderScene(void) { //定义裁剪平面方程系数，这里平面方程为x=0; GLdouble equ[4] = { 1.0, 0.0, 0.0 ,0.</description>
    </item>
    <item>
      <title>【机器视觉】光源照明方式简介</title>
      <link>https://anwangtanmi.github.io/posts/7bcb31916b09cb646f2de2376bfca738/</link>
      <pubDate>Sun, 12 Jun 2016 13:41:52 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/7bcb31916b09cb646f2de2376bfca738/</guid>
      <description>原文地址：机器视觉光源照明方式简介&#xA;机器视觉光源照明方式简介&#xA;光源是影响机器视觉系统输入的重要因素，其直接影响输入数据的质量和至少30%的应用效果。由于没有通用的机器视觉照明设备，所以针对每个特定的应用实例，要选择相应的照明装置，以达到最佳效果。下面介绍几种常见的照明方式。&#xA;一般目的的照明（直接照明）：光直接射向物体，得到清楚的影像。当需要得到高对比度物体图像的时候，这种类型的光很有效。但是当用它照在光亮或反射的材料上时，会引起像镜面的反光。通用照明一般采用环状或点状照明。环灯是一种常用的通用照明方式，其很容易安装在镜头上，可给漫反射表面提供足够的照明。&#xA;暗场（Dark Field）照明：暗场照明是相对于物体表面提供低角度照明。使用相机拍摄镜子使其在其视野内，如果在视野内能看见光源就认为使亮场照明，相反的在视野中看不到光源就是暗场照明。因此光源是亮场照明还是暗场照明与光源的位置有关。典型的，暗场照明应用于对表面部分有突起的部分的照明或表面纹理变化的照明。&#xA;背光照明：从物体背面射过来均匀视场的光。通过相机可以看到物面的侧面轮廓。背光照明常用于测量物休的尺寸和定物体的方向。背光照明产生了很强的对比度。应用背光技术时候，物体表面特征可能会丢失。例如，可以应用背光技术测量硬币的直径，但是却无法判断硬币的正反面。&#xA;同轴照明：同轴光的形成即通过垂直墙壁出来的变化发散光，射到一个使光向下的分光镜上，相机从上面通过分光镜看物体。这种类型的光源对检测高反射的物体特别有帮助，还适合受周围环境产生阴影的影响，检测面积不明显的物体。&#xA;漫射照明：连续漫反射照明应用于物体表面的反射性或者表面有复杂的角度。连续漫反射照明应用半球形的均匀照明，以减小影子及镜面反射。这种照明方式对于完全组装的电路板照明非常有用。这种光源可以达到170立体角范围的均匀照明。&#xA;除了以上介绍的几种常用照明技术，还有些特殊场合所使用的照明技术，比如在线阵相机中需要亮度集中的条形光照明；比如在精密尺寸测量中与远心镜头配合使用的平行光照明技术；比如在高速在线测量中减小被测物模糊的频闪光照明技术；又比如可以主动测量相机到光源的距离结构光照明技术和减少杂光干扰的偏振照明技术等。&#xA;此外，很多复杂的被测环境需要两种或两种以上照明技术共同配合完成。因而丰富的照明技术可以解决视觉系统中图像获取的很多问题，光源照明技术的选择可能对一个视觉系统的成功与否至关重要。</description>
    </item>
    <item>
      <title>全黑图像去除</title>
      <link>https://anwangtanmi.github.io/posts/93a0945ae8f06188e63b59d41f64f02f/</link>
      <pubDate>Wed, 16 Mar 2016 14:37:40 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/93a0945ae8f06188e63b59d41f64f02f/</guid>
      <description>本系列文章由&#xA;@yhl_leo 出品，转载请注明出处。&#xA;文章链接：&#xA;http://blog.csdn.net/yhl_leo/article/details/50904358&#xA;昨天一个朋友，向我求助，帮忙去除掉一组遥感影像数据中，因为裁剪时产生的全黑图像，实现起来也很简单，就将工程代码提供给大家：&#xA;GitHub链接：yhlleo/ImageFormatConversion&#xA;图像格式为单通道16-bit的tif，可以使用OpenCV库读取：&#xA;cv::Mat curImage = cv::imread(fileName, IMREAD_UNCHANGED); 因为图像是16-bit的，所指使用图片查看器，浏览的话，肯定是全黑的。但是可以在Photoshop中，调整灰度曲线，显示图片内容（将曲线调成近似直角折线）：&#xA;那些黑色图像（真的是全黑，像素值为0），判断方法比较简单，获取图像灰度最大最小值，区间范围是0，必然是全黑图像：&#xA;double pMax = 0, pMin = 0; cv::minMaxIdx(curImage, &amp;amp;pMin, &amp;amp;pMax); int pRange = pMax - pMin; if ( pRange &amp;gt; 0 ) { // ... } else { // Black image // ... } 如果想把16-bit图像转为常见的8-bit图像，我使用的方法是这样的：&#xA;if( pRange &amp;gt; 0 ) { for ( int i=0; iint pValue = ( static_cast&amp;lt;int&amp;gt;(*++data_cur &amp;amp; 0xffff) - ipMin ) * 255 / pRange; data_trans[i] = static_cast(pValue); } } 即，使用简单的线性拉伸方法：</description>
    </item>
    <item>
      <title>PS图层样式中的“图案叠加”技巧</title>
      <link>https://anwangtanmi.github.io/posts/40f9679a161127db001ee8be87e9b782/</link>
      <pubDate>Fri, 19 Feb 2016 17:51:29 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/40f9679a161127db001ee8be87e9b782/</guid>
      <description>在我们的Photoshop中，图层样式是被应用最多的设计手段，那么在图层样式中，图案叠加相信有不少的朋友已经用过了，下面我们先来看看这图案叠加可以用来做什么~&#xA;首先在Photoshop中随意新建个画布~填充一个自己喜欢的颜色~&#xA;然后我们再新建个画布（参数如下）&#xA;然后我们得到一个50*50px的新画布，在该画布中我们需要隐藏掉背景图层，在这个画布中绘制一个小爱心~（摆放位置如下图所示）&#xA;然后在我的编辑菜单中找到选项卡“定义图案”~&#xA;然后在新画布中会弹出个窗口，如下图所示，输入好名字然后点击确定~&#xA;然后回到我们的第一块画布中，双击该画布中的黄色背景图层~&#xA;弹出图层样式窗口-&amp;gt;选择-&amp;gt;“图案叠加”选项卡，得到如下图所示画面~&#xA;可以看到我们刚刚确定保存的小爱心图案出现在这里面了，选择它，然后点击确定~&#xA;你会发现你的画面会变成这样~&#xA;是不是多了很多小爱心，这就如同图案的平铺效果一样，可以使用它做特殊的壁纸效果哟，&#xA;那么问题就来了！！！能不能单独把做的这个图案叠加的平铺效果拿出来？（有的童鞋可能只需要这个平铺效果，不需要背景/底色）&#xA;答案是，可以的，有两种方法&#xA;先说第一种：&#xA;选中做了图案叠加效果的图层，然后在菜单栏中的“图层”选择-&amp;gt;“图层样式”选项卡-&amp;gt;在该选项卡中再选择“创建图层”~&#xA;然后你会发现你的图层~&#xA;之前是这样&#xA;现在是这样&#xA;是不是变成了一个剪切蒙版的样子，SO…&#xA;你就可以把它单独拿出来慢慢“编辑”了；&#xA;然后，现在说第二种，也是最实用~最简单的。&#xA;要想得到独立的图案叠加背景效果，我们先新建一个图层取名“图案层”~&#xA;然后给他填充一个颜色（随意填充）~&#xA;然后再双击它给他添加图层样式的“图案叠加”效果~&#xA;然后你的画布上就变成这样了~&#xA;重点来了，接下来怎么做？先看图~&#xA;把这个图层的填充色变为0%，像这样~&#xA;然后见证奇迹的时刻~&#xA;黑底没有了~&#xA;现在总结说说第二种的好处，你可以随意更换/调节图案叠加的的图案效果（包括自己做的花纹图案，都行）~~~~妈妈再也不担心我没各种背景图案用了……</description>
    </item>
    <item>
      <title>泊松融合</title>
      <link>https://anwangtanmi.github.io/posts/d766612d6bd61db7d33abcb5e0ed0301/</link>
      <pubDate>Mon, 16 Nov 2015 16:02:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d766612d6bd61db7d33abcb5e0ed0301/</guid>
      <description>拖拖拉拉快一个月了 这是探索出来的 但感觉效果不是特别好 我目前只能做到这样子了：想把左图的人放进右图的水池中 用泊松融合想实现无缝自然的效果：&#xA;左图是直接做mask镶嵌的结果 右图是解了泊松方程后的最终的结果 我所能得到的：&#xA;感觉效果不够好 因为这个理想结果是这样：&#xA;看这个就比我的自然很多很多 我的应该是有问题的 但我是按照步骤来的 目前不知道错在哪里 在找：[今天经过网友的提醒，找到了，原来是迭代次数问题，我这个效果是只迭代一次的，再迭代几次就能达到理想效果，那篇论文中没有提过这个迭代！害死我了]&#xA;A=imread(‘F:\fisheye\pond.jpg’);&#xA;src=A;&#xA;[ma,na,ka]=size(A);&#xA;dst=imread(‘F:\fisheye\swim.jpg’);&#xA;se=strel(‘diamond’,10);&#xA;B_erode=imerode(dst,se);&#xA;Berodelogical=im2bw(B_erode);&#xA;imshow(Berodelogical)&#xA;B=dst;&#xA;[mb,nb,kb]=size(B);&#xA;&amp;gt;&amp;gt; for i=1:mb&#xA;for j=1:nb&#xA;if(Berodelogical(i,j)==1)&#xA;Berodelogical(i,j)=0;&#xA;else&#xA;Berodelogical(i,j)=1;&#xA;end&#xA;end&#xA;end&#xA;dstX=100;&#xA;dstY=100;&#xA;for i=dstY:dstY+mb-1&#xA;for j=dstX:dstX+nb-1&#xA;ii=i-(dstY-1);&#xA;jj=j-(dstX-1);&#xA;if(Berodelogical(ii,jj)==1)&#xA;A(i,j,1)=B(ii,jj,1);&#xA;A(i,j,2)=B(ii,jj,2);&#xA;A(i,j,3)=B(ii,jj,3);&#xA;end&#xA;end&#xA;end&#xA;ROI=uint8(zeros(mb,nb,3));&#xA;for i=dstY:dstY+mb-1&#xA;for j=dstX:dstX+nb-1&#xA;ii=i-(dstY-1);&#xA;jj=j-(dstX-1);&#xA;ROI(ii,jj,1)=A(i,j,1);&#xA;ROI(ii,jj,2)=A(i,j,2);&#xA;ROI(ii,jj,3)=A(i,j,3);&#xA;end&#xA;end&#xA;w=[0,-1,1];&#xA;ROIgradienty=imfilter(double(ROI),w,’conv’);&#xA;ROIgradientx=imfilter(double(ROI),w’,’conv’);&#xA;%接下来对梯度求偏导得到融合图像的散度 lap</description>
    </item>
    <item>
      <title>如何从Lytro 相机中获取图像阵列</title>
      <link>https://anwangtanmi.github.io/posts/e9131cdea763032e7129adf10159d82f/</link>
      <pubDate>Mon, 23 Mar 2015 17:10:58 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/e9131cdea763032e7129adf10159d82f/</guid>
      <description>1. 推荐阅读http://www.mathworks.com/matlabcentral/fileexchange/48405-deprecated-light-field-toolbox-v0-3-v0-4-now-available)来学习如何破解rawdata；&#xA;2. 下面是我精炼的一些方法：&#xA;1) 通过拍照获取LFR 文件；&#xA;2) 新建文件夹与Cameras/A000424242平行，文件的名字就是你相机的序列号；&#xA;3) 在序列号文件夹内创建文件夹命名为WhiteImages；&#xA;4) 标定相机。参考https://support.lytro.com/hc/en-us/articles/202587194-Desktop-4-Calibration-Pairing-Data&#xA;5) 讲SD卡中大约2GB 的数据放入 WhiteImages 文件夹并解压；运行命令LFUtilUnpackLytroArchive；&#xA;6) 在LFToolBox 文件夹里执行LFMatlabPathSetup 来添加路径；&#xA;7) 进入Samples文件夹，在改路径下执行RunLFUtilProcessWhiteImages&#xA;8) LFUtiDecodeLytroFolder，可以将LFR转换为mat格式；这个过程是很慢的，如果数据比较多，建议使用服务器；&#xA;9) Load mat数据，将mat写为图片并保存；&#xA;注：每个命令的具体功能请见推荐阅读链接中的帮助手册；&#xA;更多功能补充中。。。</description>
    </item>
    <item>
      <title>专业camera/isp术语中英文对照</title>
      <link>https://anwangtanmi.github.io/posts/c9b2fe7b619017fec200324fcaa7b8c2/</link>
      <pubDate>Wed, 04 Feb 2015 14:45:07 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c9b2fe7b619017fec200324fcaa7b8c2/</guid>
      <description>1. Automatic functions 自动功能:&#xA;ALC – Automatic Luminance Control 自动亮度控制&#xA;ABLC – Automatic Black Level Calibration 自动暗电流校正&#xA;AWB – Auto White Balance Control 自动白平衡控制&#xA;Programmable controls 编程控制:&#xA;Gain, exposure, frame rate and size 增益、曝光、帧速率和大小&#xA;Image mirror, flip, panning and cropping 图像镜像、翻转、平移和裁剪&#xA;Column and row sub-sampling or binning 行和列的二次抽样和合并&#xA;Image downsizing scalar图像缩小标量&#xA;2. Output formats输出格式:&#xA;DVP parallel interface DVP并行接口&#xA;MIPI CSI2 (single lane) MIPI CSI2 接口（单线）&#xA;3. Data formats数据格式:&#xA;10-bit RAW RGB 10位原始RGB</description>
    </item>
    <item>
      <title>颜色直方图均衡化</title>
      <link>https://anwangtanmi.github.io/posts/a94fac3a4e6780ac176ab6e8f970ad63/</link>
      <pubDate>Mon, 09 Dec 2013 17:30:48 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a94fac3a4e6780ac176ab6e8f970ad63/</guid>
      <description>直方图均衡化这种方法通常用来增加许多图像的全局对比度，尤其是当图像的有用数据的对比度相当接近的时候。通过这种方法，亮度可以更好地在直方图上分布。这样就可以用于增强局部的对比度而不影响整体的对比度，直方图均衡化通过有效地扩展常用的亮度来实现这种功能。&#xA;这种方法对于背景和前景都太亮或者太暗的图像非常有用，这种方法尤其是可以带来X光图像中更好的骨骼结构显示以及曝光过度或者曝光不足照片中更好的细节。这种方法的一个主要优势是它是一个相当直观的技术并且是可逆操作，如果已知均衡化函数，那么就可以恢复原始的直方图，并且计算量也不大。这种方法的一个缺点是它对处理的数据不加选择，它可能会增加背景噪声的对比度并且降低有用信号的对比度。&#xA;实现&#xA;例子&#xA;直方图均衡化算法分为三个步骤，第一步是统计直方图每个灰度级出现的次数，第二步是累计归一化的直方图，第三步是计算新的像素值。&#xA;第一步：&#xA;for(i=0;i for(j=0;j n[s[i][j]]++;&#xA;}&#xA;}&#xA;for(i=0;i p[i]=n[i]/(width*height);&#xA;}&#xA;这里，n[i]表示的是灰度级为i的像素的个数，L表示的是最大灰度级，width和height分别表示的是原始图像的宽度和高度，所以，p[i]表示的就是灰度级为i的像素在整幅图像中出现的概率(其实就是p[]这个数组存储的就是这幅图像的归一化之后的直方图)。&#xA;第二步：&#xA;for(i=0;i&amp;lt;=L;i++){&#xA;for(j=0;j&amp;lt;=i;j++){&#xA;c[i]+=p[j];&#xA;}&#xA;}&#xA;c[]这个数组存储的就是累计的归一化直方图。&#xA;第三步：&#xA;max=min=s[0][0];&#xA;for(i=0;i for(j=0;j if(max max=s[i][j];&#xA;}else if(min&amp;gt;s[i][j]){&#xA;min=s[i][j];&#xA;}&#xA;}&#xA;}&#xA;找出像素的最大值和最小值。&#xA;for(i=0;i for(j=0;j t[i][j]=c[s[i][j]]*(max-min)+min;&#xA;}&#xA;}&#xA;参考：&#xA;http://zh.wikipedia.org/wiki/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96&#xA;http://blog.csdn.net/zrongh/article/details/7302816</description>
    </item>
  </channel>
</rss>
