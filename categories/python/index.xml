<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/categories/python/</link>
    <description>Recent content in Python on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 05 Sep 2019 14:46:06 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/categories/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在scrapy使用Tor代理Ip的两种方法</title>
      <link>https://anwangtanmi.github.io/posts/d69e3b1dc0ed50f56ee06d1e5bb9633f/</link>
      <pubDate>Thu, 05 Sep 2019 14:46:06 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d69e3b1dc0ed50f56ee06d1e5bb9633f/</guid>
      <description>第一个：按照https://www.cnblogs.com/kylinlin/archive/2016/03/04/5242266.html 大佬的写法，实现了&#xA;1、首先将本地的代理服务器进行设置，，这一步是为了与polipo对接。。&#xA;2、因为我们在polipo的配置文件中写了 洋葱头代理的地址和端口（9050），我们使用socks请求端口9150发出每次请求。&#xA;socksParentProxy = “localhost:9050”&#xA;socksProxyType = socks5&#xA;diskCacheRoot = “”&#xA;http://zhihan.me/network/2017/09/24/socks5-protocol/ 这个是关于sockes5的介绍。。&#xA;3、然后在中间件中写入&#xA;4、在stttings中&#xA;我是在start_requests() 中加了一个检测ip的语句&#xA;这样是可以运行的。。但是他的IP不会换。。&#xA;第二种：&#xA;先把这个关掉。。&#xA;在spider中书写，，&#xA;这个IP地址会变化的。。更加稳定。。&#xA;这种方法，我暂时不会设置使用MySQL；会起冲突，我以为的是本地的localhost和本地ip4的地址配置问题</description>
    </item>
    <item>
      <title>Dark web爬虫</title>
      <link>https://anwangtanmi.github.io/posts/44a6eb4a2620e118f626fc16bf9c6352/</link>
      <pubDate>Fri, 30 Aug 2019 15:39:16 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/44a6eb4a2620e118f626fc16bf9c6352/</guid>
      <description> deep_web–python 如何进入 环境搭建 开始 demo 如何进入 url多以onion结尾，访问的方式与普通的域名访问方式也不相同，访问他们需要一款名叫Tor的浏览器。也叫洋葱浏览器&#xA;环境搭建 针对于win10系统:&#xA;tor浏览器。 Vidalia控制器。 Tor控制器。 cow 。 还要有一款支持socks5协议的工具 开始 首先打开tor浏览器，点击配置，点击选择内置网桥，选择中国可用的选项。出现以下界面说明你的tor连接成功了。&#xA;打开Vidalia控制面板，如图说明你的连接洋葱浏览器成功。&#xA;点击设定在该目录中找到解压的tor控制器中的tor.exe文件&#xA;3.网络设置&#xA;4.打开socks5软件，端口监听1080,好像ssr之类的默认就是1080&#xA;5.打开cow文件夹中的rc.txt文件 修改为&#xA;以后打开cow.exe&#xA;到此环境搭建完成&#xA;demo import requests proxies = {&#39;http&#39;: &#39;http://127.0.0.1:7777&#39;, &#39;https&#39;: &#39;http://127.0.0.1:7777&#39;} s = requests.Session() r = s.get(&#34;http://anonymzn3twqpxq5.onion/list.php?2&#34;, proxies = proxies) print(r.text) </description>
    </item>
    <item>
      <title>Python 语言介绍、IDE安装、新建第一个Python程序</title>
      <link>https://anwangtanmi.github.io/posts/ca508197af35256f2f304ab769b177d7/</link>
      <pubDate>Fri, 12 Jul 2019 22:59:27 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ca508197af35256f2f304ab769b177d7/</guid>
      <description>Python 能够用于开发哪些应用？&#xA;web开发 数据分析 machine learning algorithm deep learning 后台服务 数据可视化 data visualization Python’ s disadvantages:&#xA;运行速度慢 代码不能加密 Python开发环境安装：&#xA;网页版可以安装anaconda 使用 jupyter notebook进行开发 下载地址： https://www.anaconda.com/distribution/&#xA;本地版可以安装pyCharm IDE 集成环境 下载地址： http://www.jetbrains.com/pycharm/download/#section=windows&#xA;可以安装轻量级的Wing IDE 下载地址： wingware.com 创建一个Python程序：</description>
    </item>
    <item>
      <title>python3爬虫~~应对反爬策略</title>
      <link>https://anwangtanmi.github.io/posts/a6c1d3d7443eb3b9df32ee79a52713de/</link>
      <pubDate>Wed, 26 Jun 2019 23:05:52 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a6c1d3d7443eb3b9df32ee79a52713de/</guid>
      <description>通常防止爬虫被反主要有以下几个策略： 一.BAN IP 原因：某一个某一个时刻IP访问量特别特别大 ，或者是超出正常用户使用权限，导致服务器会偶尔把该IP放入黑名单 ，过一段时间再将其放出来&#xA;解决办法：分布式爬虫（分布式【分散url的手动分布式，以及框架分布式】）以及购买代理IP（Tor代理~~能买暗网代理），转化成app有的也有效&#xA;二.BAN USERAGENT 原因：爬虫请求头就是默认的 python-requests/2.18.4 等等类型在headers的数据包，会直接拒绝访问，返回403错误&#xA;解决办法：伪装浏览器头 ，添加其请求头，再发送请求&#xA;三.BAN COOKIES 原因：服务器对每一个访问网页的人都set-cookie，给其一个cookies，当该cookies访问超过某一个阀值（请求次数或者timeout值）时就BAN掉该COOKIE&#xA;解决办法：&#xA;1,控制访问速度 ,&#xA;2,在某宝上买多个账号，生成多个cookies，在每一次访问时带上cookies&#xA;3,手动获取页面返回cookies&#xA;4，解析js拿到其js生成的cookies再带入请求（超级稳）&#xA;5，再没有遇到逆天js加载情况下无所不能的selenuim&#xA;案例：亚马逊等大型电商平台，马蜂窝&#xA;四.验证码验证 原因：当某一用户访问次数过多后，就自动让请求跳转到一个验证码页面，只有在输入正确的验证码之后才能继续访问网站&#xA;解决办法：python可以通过一些第三方库如(pytesser,PIL)来对验证码进行处理，识别出正确的验证码，复杂的验证码可以通过机器学习让爬虫自动识别复杂验证码，让程序自动识别验证码并自动输入验证码继续抓取 ，还有最终手段打码平台（兔宝贝，超级鹰等）打码平台99%的能搞定，那1%也可以用打码平台的人工打码搞定。哈哈&#xA;短信验证：易码（专业手机短信验证好几年）&#xA;二维码验证：打码平台有支持扫码（神一样的验证，我没有遇到过，但听说过）&#xA;案例：淘宝，12306&#xA;五.javascript渲染 原因：网页开发者将重要信息放在网页中但不写入html标签中，而浏览器会自动渲染 Recent Posts 暗网网址导航大全（2021年12月更新） 暗网网址合集 暗网链接 Deep Web Link Director 全球十大暗网搜索引擎 最详细的暗网教程——tor洋葱浏览器的安装和使用方法 最新darkweb暗网搜索引擎——Tordex | The Uncensored Tor Search Engine 顶级保密暗网网址分享 如何进入暗网？教程+工具 Tor 洋葱浏览器 如何进入暗网详细步骤（暗网网桥获取方法） 什么是“暗网”？我们该如何访问? Deep Web / Dark Web 大全 Tagsandroid (10) application (5) google (7) hacker (5) internet (5) java (7) Linux (16) network (6) qq (4) Source Code Analysis (15) tor (44) ubuntu (14) web (21) windows (9) 互联网 (7) 产品 (6) 代理服务器 (5) 信息安全 (4) 其他 (7) 区块链 (4) 存储 (4) 安全 (11) 工作 (20) 工具 (10) 手机 (9) 搜索引擎 (5) 暗网 (16) 服务器 (10) 杂项 (17) 活动 (12) 测试 (5) 浏览器 (19) 深度学习 (5) 照片 (6) 爬虫 (7) 物理学猜想 (10) 环境搭建 (3) 生活 (23) 电信 (4) 电话 (5) 百度 (4) 算法 (5) 网络 (52) 网络安全 (18) 网络安全知识 (6) Categories AI (5) AltiumDesigner (7) BTC (6) darknet (25) Database (3) DeepLearning (78) Docker (1) GIS (9) Google (2) iOS (7) IT (7) java (37) Life (93) linux (86) LPC (11) macOS (6) mysql (5) Python (52) qt (5) SEO (2) threejs (5) Unity (19) 产品设计 (15) 人工智能 (12) 信息安全 (36) 前端 (128) 区块链 (14) 图像处理 (59) 图形视频 (36) 大数据 (15) 嵌入式 (7) 工具 (45) 开发 (57) 性能优化 (2) 技术 (18) 搜索 (10) 操作系统 (35) 教程知识 (1) 教育 (2) 数字图像处理 (3) 数据分析 (2) 数据库 (4) 数据结构 (3) 数论 (1) 显卡驱动 (1) 智能搜索技术 (3) 未分类 (496) 机器学习 (23) 模型压缩 (2) 模拟电路 (1) 模拟题 (1) 比特币 (3) 水下图像增强 (1) 测试 (4) 浏览器 (1) 深网 (2) 渗透测试 (4) 游戏 (2) 游戏开发 (16) 爬虫 (6) 环境搭建 (2) 生活 (5) 电子商务 (1) 硬件设计 (1) 社工库 (1) 神经机制 (1) 移动web (1) 移动开发 (76) 程序员 (6) 站长 (1) 算法 (9) 统计搜索 (9) 网络优化 (7) 网络安全 (80) 视觉 (4) 计算机 (31) 论文 (8) 设计 (6) 资源搜集 (1) 资讯 (34) 软件工具 (15) 软件开发 (26) 运维 (21) 逆向 (2) 项目管理 (4) Archives December 2021 (1) August 2021 (8) April 2020 (4) March 2020 (63) February 2020 (154) January 2020 (134) December 2019 (14) November 2019 (25) October 2019 (27) September 2019 (43) August 2019 (32) July 2019 (51) June 2019 (49) May 2019 (65) April 2019 (64) March 2019 (97) February 2019 (49) January 2019 (78) December 2018 (70) November 2018 (69) October 2018 (31) September 2018 (73) August 2018 (109) July 2018 (70) June 2018 (58) May 2018 (51) April 2018 (52) March 2018 (52) February 2018 (19) January 2018 (25) December 2017 (39) November 2017 (31) October 2017 (26) September 2017 (42) August 2017 (46) July 2017 (50) June 2017 (71) May 2017 (32) April 2017 (29) March 2017 (36) February 2017 (18) January 2017 (25) December 2016 (20) November 2016 (17) October 2016 (19) September 2016 (14) August 2016 (23) July 2016 (26) June 2016 (15) May 2016 (11) April 2016 (21) March 2016 (18) February 2016 (21) January 2016 (14) December 2015 (13) November 2015 (13) October 2015 (6) September 2015 (7) August 2015 (12) July 2015 (11) June 2015 (3) May 2015 (9) April 2015 (12) March 2015 (7) February 2015 (10) January 2015 (10) December 2014 (14) November 2014 (6) October 2014 (10) September 2014 (6) August 2014 (10) July 2014 (11) June 2014 (6) May 2014 (9) April 2014 (6) March 2014 (5) February 2014 (3) January 2014 (3) December 2013 (12) November 2013 (7) October 2013 (6) September 2013 (7) August 2013 (4) July 2013 (5) June 2013 (6) May 2013 (28) April 2013 (17) March 2013 (4) February 2013 (2) January 2013 (6) December 2012 (6) November 2012 (4) October 2012 (6) September 2012 (6) August 2012 (4) July 2012 (8) June 2012 (5) May 2012 (8) April 2012 (15) March 2012 (5) February 2012 (2) January 2012 (3) December 2011 (9) November 2011 (4) October 2011 (1) September 2011 (4) August 2011 (4) July 2011 (4) June 2011 (4) May 2011 (6) April 2011 (2) March 2011 (4) February 2011 (2) January 2011 (3) December 2010 (6) November 2010 (6) October 2010 (5) September 2010 (8) August 2010 (8) July 2010 (7) June 2010 (3) May 2010 (14) April 2010 (7) February 2010 (4) January 2010 (11) December 2009 (8) November 2009 (3) October 2009 (5) September 2009 (4) July 2009 (7) April 2009 (5) March 2009 (5) February 2009 (3) January 2009 (3) December 2008 (5) November 2008 (5) October 2008 (3) September 2008 (3) August 2008 (2) July 2008 (5) June 2008 (6) May 2008 (9) April 2008 (4) March 2008 (5) February 2008 (4) January 2008 (3) December 2007 (6) November 2007 (3) October 2007 (1) September 2007 (5) August 2007 (2) July 2007 (3) June 2007 (3) May 2007 (1) April 2007 (1) March 2007 (2) February 2007 (2) January 2007 (3) December 2006 (1) November 2006 (3) September 2006 (3) July 2006 (1) June 2006 (3) May 2006 (3) April 2006 (1) March 2006 (2) February 2006 (3) January 2006 (1) December 2005 (2) November 2005 (1) September 2005 (1) August 2005 (1) July 2005 (2) April 2005 (2) March 2005 (3) February 2005 (2) January 2005 (2) December 2004 (2) November 2004 (4) September 2004 (2) August 2004 (2) July 2004 (2) September 2003 (1) May 2002 (1) March 2002 (1) Copyright ©️uzzz.</description>
    </item>
    <item>
      <title>反反爬虫相关机制（面试必问，后续陆续添加）</title>
      <link>https://anwangtanmi.github.io/posts/af89eaa74d5351c9d4b0341cb300f716/</link>
      <pubDate>Wed, 26 Jun 2019 23:05:52 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/af89eaa74d5351c9d4b0341cb300f716/</guid>
      <description>通常防止爬虫被反主要有以下几个策略： 一.BAN IP 原因：某一个某一个时刻IP访问量特别特别大 ，或者是超出正常用户使用权限，导致服务器会偶尔把该IP放入黑名单 ，过一段时间再将其放出来&#xA;解决办法：分布式爬虫（分布式【分散url的手动分布式，以及框架分布式】）以及购买代理IP（Tor代理~~能买暗网代理），转化成app有的也有效&#xA;二.BAN USERAGENT 原因：爬虫请求头就是默认的 python-requests/2.18.4 等等类型在headers的数据包，会直接拒绝访问，返回403错误&#xA;解决办法：伪装浏览器头 ，添加其请求头，再发送请求&#xA;三.BAN COOKIES 原因：服务器对每一个访问网页的人都set-cookie，给其一个cookies，当该cookies访问超过某一个阀值（请求次数或者timeout值）时就BAN掉该COOKIE&#xA;解决办法：&#xA;1,控制访问速度 ,&#xA;2,在某宝上买多个账号，生成多个cookies，在每一次访问时带上cookies&#xA;3,手动获取页面返回cookies&#xA;4，解析js拿到其js生成的cookies再带入请求（超级稳）&#xA;5，再没有遇到逆天js加载情况下无所不能的selenuim&#xA;案例：亚马逊等大型电商平台，马蜂窝&#xA;四.验证码验证 原因：当某一用户访问次数过多后，就自动让请求跳转到一个验证码页面，只有在输入正确的验证码之后才能继续访问网站&#xA;解决办法：python可以通过一些第三方库如(pytesser,PIL)来对验证码进行处理，识别出正确的验证码，复杂的验证码可以通过机器学习让爬虫自动识别复杂验证码，让程序自动识别验证码并自动输入验证码继续抓取 ，还有最终手段打码平台（兔宝贝，超级鹰等）打码平台99%的能搞定，那1%也可以用打码平台的人工打码搞定。哈哈&#xA;短信验证：易码（专业手机短信验证好几年）&#xA;二维码验证：打码平台有支持扫码（神一样的验证，我没有遇到过，但听说过）&#xA;案例：淘宝，12306&#xA;五.javascript渲染 原因：网页开发者将重要信息放在网页中但不写入html标签中，而浏览器会自动渲染 Recent Posts 暗网网址导航大全（2021年12月更新） 暗网网址合集 暗网链接 Deep Web Link Director 全球十大暗网搜索引擎 最详细的暗网教程——tor洋葱浏览器的安装和使用方法 最新darkweb暗网搜索引擎——Tordex | The Uncensored Tor Search Engine 顶级保密暗网网址分享 如何进入暗网？教程+工具 Tor 洋葱浏览器 如何进入暗网详细步骤（暗网网桥获取方法） 什么是“暗网”？我们该如何访问? Deep Web / Dark Web 大全 Tagsandroid (10) application (5) google (7) hacker (5) internet (5) java (7) Linux (16) network (6) qq (4) Source Code Analysis (15) tor (44) ubuntu (14) web (21) windows (9) 互联网 (7) 产品 (6) 代理服务器 (5) 信息安全 (4) 其他 (7) 区块链 (4) 存储 (4) 安全 (11) 工作 (20) 工具 (10) 手机 (9) 搜索引擎 (5) 暗网 (16) 服务器 (10) 杂项 (17) 活动 (12) 测试 (5) 浏览器 (19) 深度学习 (5) 照片 (6) 爬虫 (7) 物理学猜想 (10) 环境搭建 (3) 生活 (23) 电信 (4) 电话 (5) 百度 (4) 算法 (5) 网络 (52) 网络安全 (18) 网络安全知识 (6) Categories AI (5) AltiumDesigner (7) BTC (6) darknet (25) Database (3) DeepLearning (78) Docker (1) GIS (9) Google (2) iOS (7) IT (7) java (37) Life (93) linux (86) LPC (11) macOS (6) mysql (5) Python (52) qt (5) SEO (2) threejs (5) Unity (19) 产品设计 (15) 人工智能 (12) 信息安全 (36) 前端 (128) 区块链 (14) 图像处理 (59) 图形视频 (36) 大数据 (15) 嵌入式 (7) 工具 (45) 开发 (57) 性能优化 (2) 技术 (18) 搜索 (10) 操作系统 (35) 教程知识 (1) 教育 (2) 数字图像处理 (3) 数据分析 (2) 数据库 (4) 数据结构 (3) 数论 (1) 显卡驱动 (1) 智能搜索技术 (3) 未分类 (496) 机器学习 (23) 模型压缩 (2) 模拟电路 (1) 模拟题 (1) 比特币 (3) 水下图像增强 (1) 测试 (4) 浏览器 (1) 深网 (2) 渗透测试 (4) 游戏 (2) 游戏开发 (16) 爬虫 (6) 环境搭建 (2) 生活 (5) 电子商务 (1) 硬件设计 (1) 社工库 (1) 神经机制 (1) 移动web (1) 移动开发 (76) 程序员 (6) 站长 (1) 算法 (9) 统计搜索 (9) 网络优化 (7) 网络安全 (80) 视觉 (4) 计算机 (31) 论文 (8) 设计 (6) 资源搜集 (1) 资讯 (34) 软件工具 (15) 软件开发 (26) 运维 (21) 逆向 (2) 项目管理 (4) Archives December 2021 (1) August 2021 (8) April 2020 (4) March 2020 (63) February 2020 (154) January 2020 (134) December 2019 (14) November 2019 (25) October 2019 (27) September 2019 (43) August 2019 (32) July 2019 (51) June 2019 (49) May 2019 (65) April 2019 (64) March 2019 (97) February 2019 (49) January 2019 (78) December 2018 (70) November 2018 (69) October 2018 (31) September 2018 (73) August 2018 (109) July 2018 (70) June 2018 (58) May 2018 (51) April 2018 (52) March 2018 (52) February 2018 (19) January 2018 (25) December 2017 (39) November 2017 (31) October 2017 (26) September 2017 (42) August 2017 (46) July 2017 (50) June 2017 (71) May 2017 (32) April 2017 (29) March 2017 (36) February 2017 (18) January 2017 (25) December 2016 (20) November 2016 (17) October 2016 (19) September 2016 (14) August 2016 (23) July 2016 (26) June 2016 (15) May 2016 (11) April 2016 (21) March 2016 (18) February 2016 (21) January 2016 (14) December 2015 (13) November 2015 (13) October 2015 (6) September 2015 (7) August 2015 (12) July 2015 (11) June 2015 (3) May 2015 (9) April 2015 (12) March 2015 (7) February 2015 (10) January 2015 (10) December 2014 (14) November 2014 (6) October 2014 (10) September 2014 (6) August 2014 (10) July 2014 (11) June 2014 (6) May 2014 (9) April 2014 (6) March 2014 (5) February 2014 (3) January 2014 (3) December 2013 (12) November 2013 (7) October 2013 (6) September 2013 (7) August 2013 (4) July 2013 (5) June 2013 (6) May 2013 (28) April 2013 (17) March 2013 (4) February 2013 (2) January 2013 (6) December 2012 (6) November 2012 (4) October 2012 (6) September 2012 (6) August 2012 (4) July 2012 (8) June 2012 (5) May 2012 (8) April 2012 (15) March 2012 (5) February 2012 (2) January 2012 (3) December 2011 (9) November 2011 (4) October 2011 (1) September 2011 (4) August 2011 (4) July 2011 (4) June 2011 (4) May 2011 (6) April 2011 (2) March 2011 (4) February 2011 (2) January 2011 (3) December 2010 (6) November 2010 (6) October 2010 (5) September 2010 (8) August 2010 (8) July 2010 (7) June 2010 (3) May 2010 (14) April 2010 (7) February 2010 (4) January 2010 (11) December 2009 (8) November 2009 (3) October 2009 (5) September 2009 (4) July 2009 (7) April 2009 (5) March 2009 (5) February 2009 (3) January 2009 (3) December 2008 (5) November 2008 (5) October 2008 (3) September 2008 (3) August 2008 (2) July 2008 (5) June 2008 (6) May 2008 (9) April 2008 (4) March 2008 (5) February 2008 (4) January 2008 (3) December 2007 (6) November 2007 (3) October 2007 (1) September 2007 (5) August 2007 (2) July 2007 (3) June 2007 (3) May 2007 (1) April 2007 (1) March 2007 (2) February 2007 (2) January 2007 (3) December 2006 (1) November 2006 (3) September 2006 (3) July 2006 (1) June 2006 (3) May 2006 (3) April 2006 (1) March 2006 (2) February 2006 (3) January 2006 (1) December 2005 (2) November 2005 (1) September 2005 (1) August 2005 (1) July 2005 (2) April 2005 (2) March 2005 (3) February 2005 (2) January 2005 (2) December 2004 (2) November 2004 (4) September 2004 (2) August 2004 (2) July 2004 (2) September 2003 (1) May 2002 (1) March 2002 (1) Copyright ©️uzzz.</description>
    </item>
    <item>
      <title>Anaconda安装完美避坑指南</title>
      <link>https://anwangtanmi.github.io/posts/87aed7876769f2afebf48e0181af6e7d/</link>
      <pubDate>Mon, 03 Jun 2019 19:23:51 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/87aed7876769f2afebf48e0181af6e7d/</guid>
      <description>简单介绍下anaconda：简单来说，anaconda就是集合了python及其对应的几百种科学包和依赖项，同时还安装了ipython和spyder IDE。也就是说，安装了anaconda之后，就不用再安装numpy,pandas等库，也不用再额外安装IDE了。&#xA;下面就介绍下安装时的注意事项，以免采坑。&#xA;从官网上下载安装包后双击安装即可，到如下图&#xA;在安装时第一项必须要打勾，虽然这里写了not recommended，但还是要勾选，否则后面会很痛苦。建议第二项也勾选，可以在安装其他第三方IDE时免去很多麻烦。&#xA;如果第一项没打勾，在cmd中运行python, ipython，numpy，pandas等库都会提示无法启动。弹出如下提示：</description>
    </item>
    <item>
      <title>Python PIL库处理图片常用操作，图像识别数据增强的方法</title>
      <link>https://anwangtanmi.github.io/posts/abce8af11f1416d23f06936d3112b470/</link>
      <pubDate>Sat, 11 May 2019 13:58:30 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/abce8af11f1416d23f06936d3112b470/</guid>
      <description>在博客AlexNet原理及tensorflow实现训练神经网络的时候，做了数据增强，对图片的处理采用的是PIL(Python Image Library), PIL是Python常用的图像处理库.&#xA;下面对PIL中常用到的操作进行整理：&#xA;1. 改变图片的大小&#xA;from PIL import Image, ImageFont, ImageDraw def image_resize(image, save, size=(100, 100)): &#34;&#34;&#34; :param image: 原图片 :param save: 保存地址 :param size: 大小 :return: &#34;&#34;&#34; image = Image.open(image) # 读取图片 image.convert(&#34;RGB&#34;) re_sized = image.resize(size, Image.BILINEAR) # 双线性法 re_sized.save(save) # 保存图片 return re_sized 2. 对图片进行旋转：&#xA;from PIL import Image, ImageFont, ImageDraw import matplotlib.pyplot as plt def image_rotate(image_path, save_path, angle): &#34;&#34;&#34; 对图像进行一定角度的旋转 :param image_path: 图像路径 :param save_path: 保存路径 :param angle: 旋转角度 :return: &#34;</description>
    </item>
    <item>
      <title>“深网” &amp;&amp; “暗网”</title>
      <link>https://anwangtanmi.github.io/posts/62aaa19375add8e1def47e76a5b2a67a/</link>
      <pubDate>Sat, 13 Apr 2019 16:40:36 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/62aaa19375add8e1def47e76a5b2a67a/</guid>
      <description>深网是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可以抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络都是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站，所以浅网的数量相对深网还是比较少的。&#xA;暗网，也被称为 Darknet 或 dark Internet，完全是另一种“怪兽”。它们也建立在已有的网络基础上，但是使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。</description>
    </item>
    <item>
      <title>python：透明背景图</title>
      <link>https://anwangtanmi.github.io/posts/ec9317f8cb59346ac40b1c19d70ab5a6/</link>
      <pubDate>Wed, 03 Apr 2019 00:28:47 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ec9317f8cb59346ac40b1c19d70ab5a6/</guid>
      <description>图形是白色(颜色、字体可调)，背景是透明的。适用于暗色调背景。&#xA;图1 折线统计图&#xA;代码：&#xA;#encoding=utf-8 import matplotlib.pyplot as plt import numpy as np import pandas as pd #输入因变量 y1 = pd.read_csv(&#39;11.csv&#39;) y1 = np.array(y1) y2 = pd.read_csv(&#39;12.csv&#39;) y2 = np.array(y2) #assert y1.shape[0]==y2.shape[0], &#39;两个因变量个数不相等！&#39; fig,ax=plt.subplots(figsize=(10,9)) #设置自变量的范围和个数 x = np.linspace(1, 10, y1.shape[0]) #画图 ax.plot(x,y1,&#39;r-&#39;,label=&#39;x&#39;, color=&#39;red&#39;, linestyle=&#39;-&#39;, marker=&#39;*&#39;, markerfacecolor=&#39;coral&#39;, markersize=&#39;10&#39;) ax.plot(x,y2,&#39;b-&#39;,label=&#39;y&#39;, color=&#39;blue&#39;, linestyle=&#39;--&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;deeppink&#39;, markersize=&#39;10&#39;) #设置坐标轴 ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.spines[&#39;left&#39;].set_color(&#39;white&#39;) ax.spines[&#39;left&#39;].set_linewidth(3) ax.spines[&#39;bottom&#39;].set_color(&#39;white&#39;) ax.spines[&#39;bottom&#39;].set_linewidth(3) #ax.set_xlim(0, 9.5) #ax.set_ylim(0, 1.4) ax.set_xlabel(&#39;time(s)&#39;, color=&#39;white&#39;, fontsize=15) ax.set_ylabel(&#39;distance(m)&#39;, color=&#39;white&#39;, fontsize=15) #设置刻度 ax.</description>
    </item>
    <item>
      <title>利用dlib库进行人脸识别</title>
      <link>https://anwangtanmi.github.io/posts/f2865535a143bbd2a8338a344a427dfe/</link>
      <pubDate>Wed, 27 Mar 2019 09:37:19 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f2865535a143bbd2a8338a344a427dfe/</guid>
      <description>现如今人脸识别的技术已经十分先进了，识别率很高，dlib也是人脸识别常用的一个库，可以检测出人脸上的68个点，并且进行标注，当我们准备自己的人脸数据时，常常用dlib进行数据提取。&#xA;首先需要在python中安装dlib&#xA;pip install dlib==19.6.1 如果提示无法编译的错误，则需要在python环境中安装cmake，之后再安装dlib就能够成功&#xA;dlib人脸检测使用分为两种，一种是对人脸检测后提取整个人脸区域，一种是在人脸上标注特征点&#xA;人脸区域&#xA;import cv2 import numpy as np import time import os from matplotlib import pyplot as plt import dlib detector = dlib.get_frontal_face_detector() img = cv2.imread(&#39;E:\\private\\deeplearning\\dlib\\timg.jpg&#39;) img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) faces = detector(img_gray, 1) for index, face in enumerate(faces): left = face.left() top = face.top() right = face.right() bottom = face.bottom() cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0)) cv2.imwrite(&#39;E:\\private\\deeplearning\\dlib\\timg1.jpg&#39;, img) 女神镇楼&#xA;import cv2 import numpy as np import time import os from matplotlib import pyplot as plt import dlib detector = dlib.</description>
    </item>
    <item>
      <title>使用darknet识别点选验证码详细过程（附带源码）</title>
      <link>https://anwangtanmi.github.io/posts/fa5d6293d855226797c6e18b8bce47cb/</link>
      <pubDate>Thu, 21 Mar 2019 23:06:33 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/fa5d6293d855226797c6e18b8bce47cb/</guid>
      <description>项目源码：https://github.com/nickliqian/darknet_captcha&#xA;darknet_captcha 项目基于darknet开发了一系列的快速启动脚本，旨在让图像识别新手或者开发人员能够快速的启动一个目标检测（定位）的项目。&#xA;如果有没有讲清楚的地方，欢迎提issue和PR，希望能和大家共同完善！&#xA;本项目分为两个部分：&#xA;提供两个目标检测（单分类和多分类点选验证码）的例子，你可以通过例子熟悉定位yolo3定位网络的使用方式 基于darknet提供一系列API，用于使用自己的数据进行目标检测模型的训练，并提供web server的代码&#xA;目录 项目结构 开始一个例子：单类型目标检测 第二个例子：多类型目标检测 训练自己的数据 Web服务 API文档 其他问题 使用阿里云OSS加速下载 GPU云推荐 CPU和GPU识别速度对比 报错解决办法 TODO 项目结构 项目分为darknet、extent、app三部分&#xA;darknet: 这部分是darknet项目源码，没有作任何改动。 extent: 扩展部分，包含生成配置、生成样本、训练、识别demo、api程序。 app: 每一个新的识别需求都以app区分，其中包含配置文件、样本和标签文件等。 开始一个例子：单类型目标检测 以点选验证码为例&#xA;darknet实际上给我们提供了一系列的深度学习算法，我们要做的就是使用比较简单的步骤来调用darknet训练我们的识别模型。&#xA;推荐使用的操作系统是ubuntu，遇到的坑会少很多。 如果使用windowns系统，需要先安装cygwin，便于编译darknet。（参考我的博客：安装cygwin） 下面的步骤都已经通过ubuntu16.04测试。&#xA;1.下载项目 git clone https://github.com/nickliqian/darknet_captcha.git 2.编译darknet 进入darknet_captcha目录，下载darknet项目，覆盖darknet目录：&#xA;cd darknet_captcha git clone https://github.com/pjreddie/darknet.git 进入darknet目录，修改darknet/Makefile配置文件&#xA;cd darknet vim Makefile 如果使用GPU训练则下面的GPU=1 使用CPU训练则下面的GPU=0 GPU=1 CUDNN=0 OPENCV=0 OPENMP=0 DEBUG=0 然后使用make编译darknet：&#xA;make 不建议使用CPU进行训练，因为使用CPU不管是训练还是预测，耗时都非常久。&#xA;如果你需要租用临时且价格低的GPU主机进行测试，后面介绍了一些推荐的GPU云服务。&#xA;如果在编译过程中会出错，可以在darknet的issue找一下解决办法，也可以发邮件找我要旧版本的darknet。&#xA;3.安装python3环境 使用pip执行下面的语句，并确保你的系统上已经安装了tk：&#xA;pip install -r requirement.txt sudo apt-get install python3-tk 4.</description>
    </item>
    <item>
      <title>Jetson TX2的各种坑.md</title>
      <link>https://anwangtanmi.github.io/posts/c4b5e24e064b154ea228622d149fab43/</link>
      <pubDate>Sun, 10 Mar 2019 15:49:40 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c4b5e24e064b154ea228622d149fab43/</guid>
      <description>最近在使用Jetson TX2 在跑实验，然后遇到下面问题，做笔记，记录一下。&#xA;内存出错无法，中断 # 出现下面那种错误 2019-01-11 19:41:46.959970: E tensorflow/stream_executor/cuda/cuda_driver.cc:1068] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED 2019-01-11 19:41:46.960033: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x367c800: CUDA_ERROR_LAUNCH_FAILED 2019-01-11 19:41:46.960059: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x367c800: CUDA_ERROR_LAUNCH_FAILED 2019-01-11 19:41:46.960185: F tensorflow/stream_executor/cuda/cuda_dnn.cc:2045] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED [1] 10332 abort (core dumped) python monodepth_simple.py --image_path ./data/training/image_2/000000_10.jpg 解决办法：&#xA;config = tf.ConfigProto(allow_soft_placement=True) config.gpu_options.allow_growth = True #加多这一行 sess = tf.</description>
    </item>
    <item>
      <title>图像色彩增强之python实现——MSR,MSRCR,MSRCP,autoMSRCR</title>
      <link>https://anwangtanmi.github.io/posts/491f0a807009ec7bf52fc85083cca200/</link>
      <pubDate>Mon, 04 Mar 2019 08:35:52 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/491f0a807009ec7bf52fc85083cca200/</guid>
      <description>转载请注明出处：https://blog.csdn.net/weixin_38285131/article/details/88097771&#xA;最近在做街景图象色彩校正方面的工作，对于过暗，过曝光，以及背光等现象，用过一些gamma校正以及其他的方法，最终选择基于Retinex原理的方法对这几种现象都有一定的增强效果。&#xA;Retinex理论基于一下假设：&#xA;1.真实世界是无颜色的，我们所感知的颜色是光与物质的相互作用的结果。我们见到的水是无色的，但是水膜—肥皂膜却是显现五彩缤纷，那是薄膜表面光干涉的结果。&#xA;2.每一颜色区域由给定波长的红、绿、蓝三原色构成的；&#xA;3.三原色决定了每个单位区域的颜色。&#xA;Retinex理论的基础理论是物体的颜色是由物体对长波（红色）、中波（绿色）、短波（蓝色）光线的反射能力来决定的，而不是由反射光强度的绝对值来决定的，物体的色彩不受光照非均匀性的影响，具有一致性，即retinex是以色感一致性（颜色恒常性）为基础的。不同于传统的线性、非线性的只能增强图像某一类特征的方法，Retinex可以在动态范围压缩、边缘增强和颜色恒常三个方面达到平衡，因此可以对各种不同类型的图像进行自适应的增强。&#xA;40多年来，研究人员模仿人类视觉系统发展了Retinex算法，从单尺度Retinex算法，MSR改进成多尺度加权平均的MSR算法，再发展成彩色恢复多尺度MSRCR算法和色彩增益加权的AutoMSRCR算法。&#xA;主要算法公式介绍可以参考如下博客：&#xA;https://blog.csdn.net/ajianyingxiaoqinghan/article/details/71435098&#xA;我再对这几种方法稍稍总结一下：&#xA;一丶单尺度的Retinex——SSR&#xA;可以理解为图像分解，一幅图像S(x,y)可以分为他的光照图象 I(x,y)和反射图像R(x,y)，反射图象是根据物体本身的反射特性，所以基本不会发生变化，光照图像是根据环境明暗来决定的。&#xA;只看公式的话感觉一下就看懵逼了，我感觉就三个步骤：&#xA;1）将图像进行log变换&#xA;2）然后将log图像进行高斯模糊&#xA;3）利用原图和模糊之后的log图像做差分&#xA;二丶多尺度的Retinex——MSR&#xA;通俗解释：就是再多个单尺度Retinex做平均，区别是在第二步高斯模糊是选择的sigma是不同的&#xA;原始图像进行三次SSR&#xA;高斯模糊选择15，80，200作为高斯模糊sigma参数&#xA;对三次的SSR结果做平均即为MSR图像&#xA;三丶彩色恢复多尺度Retinex——MSRCR，MSRCP等&#xA;对多尺度MSR结果做了色彩平衡，归一化，增益和偏差线性加权&#xA;四丶参数说明&#xA;{&#xA;“sigma_list”: [15, 80, 200],多尺度高斯模糊sigma值&#xA;“G” : 5.0,增益&#xA;“b” : 25.0,偏差&#xA;“alpha” : 125.0,&#xA;“beta” : 46.0,&#xA;“low_clip” : 0.01,&#xA;“high_clip” : 0.99&#xA;}&#xA;五丶图像增强结果：&#xA;六丶代码&#xA;retinex.py&#xA;import numpy as np import cv2 def singleScaleRetinex(img, sigma): retinex = np.log10(img) - np.log10(cv2.GaussianBlur(img, (0, 0), sigma)) return retinex def multiScaleRetinex(img, sigma_list): retinex = np.</description>
    </item>
    <item>
      <title>pyqt5练习——海康摄像头登录信息界面</title>
      <link>https://anwangtanmi.github.io/posts/381c260582d48694ee38401779cc839b/</link>
      <pubDate>Sun, 03 Mar 2019 16:17:22 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/381c260582d48694ee38401779cc839b/</guid>
      <description>环境 PyQt 5.11.2 Python 3.6 分析 海康摄像头开发，其中一部分需要用户手动输入IP、端口号、用户名、密码等信息，这些信息可以单独写成一个设置窗口，从窗口获取信息，并传入主窗口，方便后续开发。&#xA;窗口UI 实现功能 对于IP地址、端口号、用户名、密码，能够根据正则表达式控制用户的输入类型，设置输入长度； 对于密码，可以设置明文、暗文显示 当用户输入完成后，判断IP地址、端口号、用户名、密码信息是否按要求类型载入到系统变量，为后续传参做准备。 对于某一项输入类型不合要求，弹窗提示，不会载入系统变量 代码 from PyQt5.QtWidgets import * from PyQt5.QtCore import * from PyQt5.QtGui import * import sys import threading window_width = 1920 window_height = 1080 button_width = 48 class Setting(QWidget): valid_signal = pyqtSignal() def __init__(self): super(Setting, self).__init__() self.ip = &#39;&#39; self.port = &#39;&#39; self.user = &#39;&#39; self.passwd = &#39;&#39; self.widgets = [] self.ip_label = QLabel(self) self.ip_label.setFixedHeight(32) self.ip_label.setText(&#34;ip地址：&#34;) self.widgets.append(self.ip_label) self.</description>
    </item>
    <item>
      <title>暗文密码输入</title>
      <link>https://anwangtanmi.github.io/posts/c5aaf7465d0db3fdc3620ac2bda35523/</link>
      <pubDate>Sun, 24 Feb 2019 15:19:46 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c5aaf7465d0db3fdc3620ac2bda35523/</guid>
      <description> import getpass _usrname = &#34;Alex&#34; _password = &#34;123&#34; username = input(&#34;username:&#34;) password = getpass.getpass(&#34;password:&#34;) if _usrname == username and _password == password: print(&#34;Welcome user {name} login...&#34;.format(name=username)) else: print(&#34;Invalid username or password!&#34;) #在pycharm中getpass不好使，会卡机 </description>
    </item>
    <item>
      <title>4.python-爬虫的基础认知，爬虫的几大分类？</title>
      <link>https://anwangtanmi.github.io/posts/70ac6068e3837798502a093c16849d46/</link>
      <pubDate>Fri, 01 Feb 2019 14:21:56 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/70ac6068e3837798502a093c16849d46/</guid>
      <description>分类 来自：百度百科&#xA;网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫（General Purpose Web Crawler）、聚焦网络爬虫（Focused Web Crawler）、增量式网络爬虫（Incremental Web Crawler）、深层网络爬虫（Deep Web Crawler）。 实际的网络爬虫系统通常是几种爬虫技术相结合实现的 。&#xA;通用网络爬虫 通用网络爬虫又称全网爬虫（Scalable Web Crawler），爬行对象从一些种子 URL 扩充到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。 由于商业原因，它们的技术细节很少公布出来。 这类网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。 虽然存在一定缺陷，通用网络爬虫适用于为搜索引擎搜索广泛的主题，有较强的应用价值。&#xA;通用网络爬虫的结构大致可以分为页面爬行模块 、页面分析模块、链接过滤模块、页面数据库、URL 队列、初始 URL 集合几个部分。为提高工作效率，通用网络爬虫会采取一定的爬行策略。 常用的爬行策略有：深度优先策略、广度优先策略 。&#xA;1) 深度优先策略：其基本方法是按照深度由低到高的顺序，依次访问下一级网页链接，直到不能再深入为止。 爬虫在完成一个爬行分支后返回到上一链接节点进一步搜索其它链接。 当所有链接遍历完后，爬行任务结束。 这种策略比较适合垂直搜索或站内搜索， 但爬行页面内容层次较深的站点时会造成资源的巨大浪费 。&#xA;2) 广度优先策略：此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。 当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。 这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无需存储大量中间节点，不足之处在于需较长时间才能爬行到目录层次较深的页面 。&#xA;聚焦网络爬虫 聚焦网络爬虫（Focused Crawler），又称主题网络爬虫（Topical Crawler），是指选择性地爬行那些与预先定义好的主题相关页面的网络爬虫[8]。 和通用网络爬虫相比，聚焦爬虫只需要爬行与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求 。&#xA;聚焦网络爬虫和通用网络爬虫相比，增加了链接评价模块以及内容评价模块。聚焦爬虫爬行策略实现的关键是评价页面内容和链接的重要性，不同的方法计算出的重要性不同，由此导致链接的访问顺序也不同 。&#xA;1) 基于内容评价的爬行策略：DeBra将文本相似度的计算方法引入到网络爬虫中，提出了 Fish Search 算法，它将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关，其局限性在于无法评价页面与主题相关 度 的 高 低 。 Herseovic对 Fish Search 算 法 进 行 了 改 进 ，提 出 了 Sharksearch 算法，利用空间向量模型计算页面与主题的相关度大小 。</description>
    </item>
    <item>
      <title>cv2伪彩色applyColorMap()函数</title>
      <link>https://anwangtanmi.github.io/posts/dca6003c910c6e9b2402ff8c4972e123/</link>
      <pubDate>Sun, 09 Dec 2018 14:19:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/dca6003c910c6e9b2402ff8c4972e123/</guid>
      <description>本文主要介绍cv2模块中的伪彩色applyColorMap()函数。&#xA;引用自：https://blog.csdn.net/u013381011/article/details/78341861&#xA;colormap（色度图）&#xA;假设我们想在地图上显示美国不同地区的温度。我们可以把美国地图上的温度数据叠加为灰度图像——较暗的区域代表较冷的温度，更明亮的区域代表较热的区域。这样的表现不仅令人难以置信，而且代表了两个重要的原因。首先，人类视觉系统没有被优化来测量灰度强度的微小变化。我们能更好地感知颜色的变化。第二，我们用不同的颜色代表不同的意思。用蓝色和较温暖的温度用红色表示较冷的温度更有意义。&#xA;温度数据只是一个例子，但还有其他几个数据是单值（灰度）的情况，但将其转换为彩色数据以实现可视化是有意义的。用伪彩色更好地显示数据的其他例子是高度、压力、密度、湿度等等。&#xA;在OpenCV中使用applycolormap（伪彩色函数）&#xA;OpenCV的定义12种colormap（色度图），可以应用于灰度图像，使用函数applycolormap产生伪彩色图像。让我们很快看到如何将色度图的一种模式colormap_jet应用到一幅图像中。&#xA;下面是示例代码：&#xA;import cv2 import numpy as np def colormap_name(id): switcher = { 0 : &#34;COLORMAP_AUTUMN&#34;, 1 : &#34;COLORMAP_BONE&#34;, 2 : &#34;COLORMAP_JET&#34;, 3 : &#34;COLORMAP_WINTER&#34;, 4 : &#34;COLORMAP_RAINBOW&#34;, 5 : &#34;COLORMAP_OCEAN&#34;, 6 : &#34;COLORMAP_SUMMER&#34;, 7 : &#34;COLORMAP_SPRING&#34;, 8 : &#34;COLORMAP_COOL&#34;, 9 : &#34;COLORMAP_HSV&#34;, 10: &#34;COLORMAP_PINK&#34;, 11: &#34;COLORMAP_HOT&#34; } return switcher.get(id, &#39;NONE&#39;) img = cv2.imread(&#39;./pluto.jpg&#39;, cv2.IMREAD_GRAYSCALE) im_out = np.zeros((600, 800, 3), np.uint8) for i in range(0, 4): for j in range(0, 3): k = i + j * 4 im_color = cv2.</description>
    </item>
    <item>
      <title>pygame游戏编程03，走！到森林里去</title>
      <link>https://anwangtanmi.github.io/posts/82fe851d9173671b81ceb88f0df59eb3/</link>
      <pubDate>Wed, 05 Dec 2018 22:05:20 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/82fe851d9173671b81ceb88f0df59eb3/</guid>
      <description>前言 小乖在得到我们发送的手机定位后，终于找到了正确的方向，呼呼地赶了过来。但当她来到“金暗之森”跟前的时候，一扫一路上的疲态，立马就变得跃跃欲试，恨不得马上进入其中探险。&#xA;“小乖！小乖！”你可等等啊，这里面麻烦不断，得先做些准备工作呀。好吧，哪里还能等我们把话说完，附近早已经不见了小乖的影子啦。。。&#xA;出现在森林之中 1.Ship类 在本章，由于小乖的到来，我们需要建立一个新类来对小乖的行为作出管理约束。这便是我们的Ship类啦。Ship类目前的主要功能为在游戏窗口中绘制人物、通过键盘来控制人物的行动。来看以下代码：&#xA;import pygame class Ship: def __init__(self, screen, img_path=&#34;rc/pic/ic_actor.png&#34;, speed_factor=1): &#34;&#34;&#34; 构造方法，目前负责在指定位置绘制游戏人物，使人物对键盘作出响应 :param screen: 当前游戏窗口的Surface对象 :param img_path: 人物图片的路径 :param speed_factor: 控制人物移动的快慢 &#34;&#34;&#34; self.screen = screen self.img = pygame.image.load(img_path) # 获得当前人物图片的Surface对象 self.rect = self.img.get_rect() # 获得人物图片外接矩形的Rect对象 self.screen_rect = self.screen.get_rect() self.rect.centerx = self.screen_rect.centerx # 确定人物在游戏窗口中的X位置 self.rect.centery = self.screen_rect.bottom - 30 # 确定人物在游戏窗口中的Y位置 def blit_actor(self): &#34;&#34;&#34; 负责在游戏中绘制人物 :return: 无 &#34;&#34;&#34; self.screen.blit(self.img, self.rect) 以上代码没有什么出现什么新的知识点，都是对之前所讲的再熟悉利用，所以，这里就不再赘述了。相应的，game_frame.py中的代码也需要做一定的调整。&#xA;def run_game(self): # 当游戏开始时候调用此方法 if self.</description>
    </item>
    <item>
      <title>python离线安装库和添加多个安装源</title>
      <link>https://anwangtanmi.github.io/posts/3749c2e876228cc256170f86501cf3c6/</link>
      <pubDate>Wed, 05 Dec 2018 18:01:53 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3749c2e876228cc256170f86501cf3c6/</guid>
      <description>修改安装源(主要是国内源更方便)：&#xA;在C:\Users\用户名\建立文件夹pip，然后在里面建立文件pip.ini，文件内容如下（豆瓣源，其它源自己选）：&#xA;[global] index-url = http://pypi.douban.com/simple/ extra-index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=pypi.douban.com ps：http就需要添加trusted-host，https就不需要；extra-index-url只能存在一个，存在多个会报错；trusted-host存在多个好像是只有第一个会生效&#xA;国内源：&#xA;阿里云 https://mirrors.aliyun.com/pypi/simple/&#xA;中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/&#xA;豆瓣(douban) https://pypi.douban.com/simple/&#xA;清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/&#xA;中国科学技术大学 https://pypi.mirrors.ustc.edu.cn/simple/&#xA;离线下载安装包，用于公司内网安装：&#xA;pip download 模块名 -d 绝对路径 例如：&#xA;pip download scapy -d D:\Users\songqiu\Desktop 如果提示不信任源，就加上--trusted-host 源域名&#xA;例如：&#xA;pip download scapy -d D:\Users\songqiu\Desktop --trusted-host pypi.douban.com 下载到本地后，安装方法为：&#xA;解压，执行python setup.py install</description>
    </item>
    <item>
      <title>scrapy中间件 部署 日志</title>
      <link>https://anwangtanmi.github.io/posts/15aa1009965a87e8f3af43435056a119/</link>
      <pubDate>Sun, 25 Nov 2018 19:57:37 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/15aa1009965a87e8f3af43435056a119/</guid>
      <description>IP代理：&#xA;抓取网上免费代理，测试 代理供应商提供的代理（收费） ADSL拨号，每次重新拨号会更换本地IP，但是会有1~3秒延迟 VPN/VPS 虚拟主机（翻墙爬取国外网站） Tor网络（暗网） 洋葱浏览器 自动生成user-agent&#xA;pip install fake_useragent&#xA;导入:&#xA;from fake_useragent import UserAgent&#xA;ua_obj = UserAgent()&#xA;ua_obj.ie&#xA;ua_obj.chrome&#xA;ua_obj.random&#xA;如果有重复图片、文件，保存到本地只有一份，后续改名只能成功一次，后面再改名。&#xA;用商品名称做为图片名保存，如果图片名里有”/”，则保存时会当作路径结点使用。&#xA;file_name = “Huawei Mate20 Pro 8GB/128GB 月光灰”&#xA;if “/” in file_name:&#xA;file_name.replace(“/”, “-”)&#xA;模拟登陆： 直接发送账户密码的POST请求，记录cookie，再发送其他页面的请求 先发送登录页面的get请求，获取登录参数，再发送登录的post请求，提交账户密码和登录参数，并记录cookie，再发送其他页面的请求 直接将cookies保存在请求报头里，直接发送附带登录状态的请求，获取页面。 Scrapyd远程部署和执行爬虫、停止爬虫、监控爬虫运行状态 服务端:&#xA;安装客户端和服务器端的工具：&#xA;端口: 6800&#xA;客户端：pip install scrapyd-client&#xA;服务器端：pip install scrapyd&#xA;服务器端开启scrapyd服务（提供一个监听6800端口的web）&#xA;修改 default_scrapyd.conf 配置文件里的 bind_address 为 0.0.0.0&#xA;再开启服务&#xA;ubuntu: $ scrapyd&#xA;以下全部是客户端的操作：&#xA;修改scrapy项目的scrapy.cfg文件，添加 配置名称和url&#xA;[deploy:scrapyd_Tencent3]&#xA;url = http://192.</description>
    </item>
    <item>
      <title>TensorFlow学习笔记–Deep Dream模型</title>
      <link>https://anwangtanmi.github.io/posts/a23a786bfcefc82bfeb41712b829ebc3/</link>
      <pubDate>Thu, 18 Oct 2018 14:17:59 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a23a786bfcefc82bfeb41712b829ebc3/</guid>
      <description>零、目标 Deep Dream是谷歌推出的一个有意思的技术。在训练好的CNN上，设定几个参数就可以生成一张图象。具体目标是：&#xA;了解Deep Dream基本原理 掌握实现生成Deep Dream 模型 一、技术原理 在卷积网络中，通常输入的是一张图象，经过若干层的卷积运算，最终输出图像的类别。这期间使用到了图片计算梯度，网络根据梯度不断的调整和学习最佳的参数。但是卷积层究竟学习到了什么，卷积层的参数代表了什么，浅层卷积和深层卷积学习到的内容有哪些区别，这些问题Deep Dream可以解答。&#xA;假设输入网络的图像为X，网络输出的各个类别的概率为t（t是一个多维向量，代表了多种类别的概率）。设定t[N]为优化目标，不断的让神经网络去调整输入图像X的像素值，让输出t[N]尽可能的大，最后极大化第N类别的概率得到图片。&#xA;关于卷积层究竟学到了什么，只需要最大化卷积层的某一个通道数据就可以了。折输入的图像为X，中间某个卷积层的输出是Y，Y的形状是hwc，其中h为Y的高度，w为Y的宽度，c为通道数。卷积的一个通道就可以代表一种学习到的信息。以某一个通道的平均值作为优化目标，就可以弄清楚这个通道究竟学习到了什么，这也是Deep Dream的基本原理。&#xA;二、在TensorFlow中使用 导入Inception模型&#xA;原始的Deep Dream 模型只需要优化ImageNet 模型卷积层某个通道的激活值就可以。因此，应该先导入ImageNet图像识别模型，这里以 Inception 为例。创建 load_inception.py 文件，输入如下代码： # 导入基本模块 import numpy as np import tensorflow as tf # 创建图和会话 graph = tf.Graph() sess = tf.InteractiveSession(graph=graph) # 导入Inception模型 # tensorflow_inception_graph.pb 文件存储了inception的网络结构和对应的数据 model_fn = &#39;tensorflow_inception_graph.pb&#39; with tf.gfile.FastGFile(model_fn, &#39;rb&#39;) as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) # 导入的时候需要给网络制定一个输入图像，因此定义一个t_input作为占位符 # 需要注意的是，使用的图像数据通常的格式为：(height,width,channel)， # 其中height为图像的像素高度，width为图像的像素宽度，chaneel为图像的通道数，一般使用RGB图像，所以通道数为3 t_input = tf.placeholder(np.float32, name=&#39;input&#39;) imagenet_mean = 117.</description>
    </item>
    <item>
      <title>HTTP和HTTPS的区别</title>
      <link>https://anwangtanmi.github.io/posts/21c62986a54bf9a651bd3da1be6ea2e9/</link>
      <pubDate>Thu, 20 Sep 2018 15:36:57 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/21c62986a54bf9a651bd3da1be6ea2e9/</guid>
      <description> HTTP是超文本传输协议，通俗的讲就是网络连接传输文本信息的协议，既然我们要上网就必须遵循HTTP协议，这就是为什么我们每次打开网页在网址前端都会出现HTTP字样。就如同你是“天地会”成员，你和其他天地会成员接头时，首先要说“地震高岗，一派山西千古秀！”和“门朝大海，三合河水万年流”这样的接头暗号，说出来才能和会友进行沟通。所以每次网页出现“HTTP：//”就如何上面所讲的接头暗号，当暗号正确才能获得相关的信息。&#xA;HTTPS是安全超文本传输协议，是在HTTP协议基础上增加了使用SSL加密传送信息的协议，我们还是用天地会接头的例子来讲，大家可能觉得每次天地会都使用“地震高岗，一派西山千古秀”这类妇孺皆知的暗号，这样的组织没什么安全性可言，但是事实并不是如此，他们之间除了使用接头暗号外，可能还使用了“黑话”， 就是只有天地会成员才能听懂的黑话，这样即使交谈信息被泄露了，没有相关的解密的东西，大家谁也不知道这些黑话是什么？同意HTTPS就如同上面天地会的信息交谈一样，它也将自己需要传输的超文本协议通过SSL加密，让明文变成了“黑话”。&#xA;HTTPS和HTTP的区别&#xA;HTTPS协议需要到CA申请证书，一般免费的证书很少，需要交费 HTTP是超文本传输协议，信息时明文传输，HTTPS则是具有安全性的SSL加密传输协议 HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样。HTTP使用的是80端口，HTTPS使用的是443端口 HTTP的连接很简单，是无状态的 HTTPS协议是由SSL+HTTP协议构建的，可进行加密传输、身份认证的网络协议，要比HTTP协议安全 HTTP是属于应用层，HTTPS加密是在传输层 </description>
    </item>
    <item>
      <title>[2018]Anaconda安装及配置</title>
      <link>https://anwangtanmi.github.io/posts/9c2aafc1ef90a9fb05f2e8f81443d262/</link>
      <pubDate>Tue, 18 Sep 2018 17:01:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9c2aafc1ef90a9fb05f2e8f81443d262/</guid>
      <description>学习Python第一件是就是安装程序,对于初学者,有些坑总是要过的,我把自己学习过程中遇到过的问题&#xA;整理,供大家参考.&#xA;1.选择安装什么Python软件及版本&#xA;安装最新版Anaconda,里面有最新版的Python程序.好几个培训机构都使用的Anaconda.使用最受欢迎的软件准没错 最新版的Anaconda中Python应该是3.0以上的. Python3.0版向2.0版不兼容真的很好,只有这样才能最为一款最简洁的软件 2.路径配置&#xA;安装过程中有一步是否将Anaconda添加的环境变量中,建议勾选.如果不勾选,安装后需要自己添加环境变量,这又是一个坑&#xA;3.手动将Anaconda添加到环境变量&#xA;找到 控制面板→系统和安全→系统→高级系统设置→高级→环境变量→系统变量→Path 双击&#xA;将以下三个添加到Path中(win8系统)&#xA;D:\Anaconda\D:\Anaconda&#xA;D:\Anaconda\Scripts&#xA;D:\Anaconda\Library\bin&#xA;4.修改打开jupyter notebook的浏览器&#xA;默认是IE浏览器,一般很难用,而且如果IE浏览器过低,jupyter notebook如法打开&#xA;找到jupyter_notebook_config.py文件. 一般在 c:/users/username/.jupyter目录下,用记事本打开&#xA;找到# c.NotebookApp.browser = ‘’’’ 代码 修改成如下格式:&#xA;chrome浏览器&#xA;import webbrowser&#xA;webbrowser.register(“chrome”,None,webbrowser.GenericBrowser(u”C:\ProgramFiles(x86)\Google\Chrome\Application\chrome.exe”))&#xA;c.NotebookApp.browser = ‘chrome’&#xA;火狐浏览器&#xA;c.NotebookApp.browser = ‘Firefox’&#xA;import webbrowser&#xA;webbrowser.register(“Firefox”, None, webbrowser.GenericBrowser(u”C:\Program Files\Mozilla Firefox\firefox.exe”))&#xA;注意:删除前面#号,代码顶格&#xA;5.修改jupyter notebook中文件路径&#xA;原因:好找文件,文件显示简洁&#xA;同上,找到jupyter_notebook_config.py文件,用记事本打开&#xA;找到 #c.NotebookApp.notebook_dir = ‘’ 语句,删除#号&#xA;修改成 c.NotebookApp.notebook_dir =‘E:/python’ 表示文件在E盘的python文件夹下面</description>
    </item>
    <item>
      <title>爬虫系列（四）–全站爬取</title>
      <link>https://anwangtanmi.github.io/posts/151fedb0562137bae1ce00364bd4e1b7/</link>
      <pubDate>Mon, 17 Sep 2018 14:49:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/151fedb0562137bae1ce00364bd4e1b7/</guid>
      <description>爬虫系列（四）–全站爬取 全站爬取需要的数据基于一个这样的假设：某网站的页面上存在该网站其他页面的连接，通过这些连接跳转的新的页面进行数据的爬取。在开始这个之前，要先明白栈和队列。本篇中介绍的是单线程的实现方式，大规模的爬取需要多线程，分布式爬取。&#xA;1.实现步骤 （1）准备几个起始链接加入待队列Q中，例如Q=[“http://www.xxx.com/aaa/”,”http://www.xxx.com/bbb/”,”http://www.xxx.com/ccc/”]&#xA;（2）并将这几个链接加入一个入队集合S中,S={“http://www.xxx.com/aaa/”,”http://www.xxx.com/bbb/”,”http://www.xxx.com/ccc/”}这个集合作用是保证一个网页只爬取一次。&#xA;（3）从Q中取出一个链接url（出队，取出队首元素，并从队列中删除该元素），如果Q中没有链接，结束爬取。如果有链接url=”http://www.xxx.com/aaa/”，进行（4）步骤。&#xA;（4）对url爬取，保存需要的数据（写到文件中，建议使用json格式保存，一行一个页面），找出该页面上的所有链接urls&#xA;（5）把符合我们要求的连接（例如以http://www.xxx.com 开头的链接）找出来，判断每一个连接urli是否在S中。如果不在S中，把urli加入S，urli入队&#xA;（6）继续执行（3）步骤&#xA;注意1：这个是广度优先爬取，如果把队换成栈，会变成深度优先爬取。如果没有特殊的需求，一般都是使用广度优先爬取。&#xA;注意2：对于一个小网站来说，这样操作没有什么问题，但是有些网站页面很多，Q和S中存储的连接太多直接撑爆内存，这时可以实现一个硬盘队列（栈）和硬盘集合，本系列文章不实现这些功能。&#xA;注意3：有些网站的连接到站内的url形如”/aaa?a=1&amp;amp;b=2″,需要改写成”http://域名/aaa?a=1&amp;amp;b=2″&#xA;注意4：有些网站会根据短时间内一个ip访问大量页面制定反爬虫策略，可以爬取一个页面后，休眠一段时间接着爬取&#xA;2.代码实现(代码仅对于代码中要爬取的网站有效，其他网站需要重新配置规则) import time import os import json from urllib import request from lxml import etree header_dict = { &#34;Accept&#34;:&#34;application/json, text/javascript, */*; q=0.01&#34;, &#34;Accept-Language&#34;:&#34;zh-CN,zh;q=0.9&#34;, &#34;User-Agent&#34;:&#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36&#34;, } def get_http(load_url,header=None): res=&#34;&#34; try: req = request.Request(url=load_url,headers=header)#创建请求对象 coonect = request.urlopen(req)#打开该请求 byte_res = coonect.read()#读取所有数据，很暴力 try: res=byte_res.decode(encoding=&#39;utf-8&#39;) except: try: res=byte_res.decode(encoding=&#39;gbk&#39;) except: res=&#34;</description>
    </item>
    <item>
      <title>python爬虫常用库和安装 — windows7环境</title>
      <link>https://anwangtanmi.github.io/posts/cb07ed284e485b9256719322beb8cd3b/</link>
      <pubDate>Tue, 07 Aug 2018 21:46:03 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/cb07ed284e485b9256719322beb8cd3b/</guid>
      <description>1：urllib python自带&#xA;2：re python自带&#xA;3：requests pip install requests&#xA;4：selenium 需要依赖chromedriver&#xA;(selenium目前有版本限制：https://blog.csdn.net/qq_36625806/article/details/81463872)&#xA;下载地址：https://download.csdn.net/my&#xA;安装方式：解压后 python setup.py install –&amp;gt;&amp;gt;添加到环境变量path中&#xA;5：chromedriver 下载地址：https://download.csdn.net/download/qq_36625806/10589319&#xA;放到python安装目录中即可。&#xA;6：phantomjs 无界面浏览器，相当于一个网页控制台&#xA;下载地址：https://download.csdn.net/download/qq_36625806/10589328&#xA;将bin目录配置到环境变量的path中&#xA;7：xlml pip install lxml&#xA;或访问：https://pypi.python.org/pypi/lxml 下载&#xA;8：beautifulsoup pip install beautifulsoup4 (网页解析库，依赖xlml)&#xA;9：pyquery pip install pyquery (网页解析库，语法跟jquery完全一致)&#xA;官方api：https://pythonhosted.org/pyquery/api.html&#xA;10：pymysql pip install pymysql (操作mysql的库)&#xA;11：pymongo pip install pymongo (操作Mongodb数据库)&#xA;12：redis pip install redis (操作redis)&#xA;13：flask pip install flask (代理)&#xA;14：django pip install django (python前端框架)&#xA;官网：https://www.djangoproject.com&#xA;15：jupyter pip install jupyter (记事本，可以在线运行代码)</description>
    </item>
    <item>
      <title>seaborn中有关机器学习的一些知识点笔记6</title>
      <link>https://anwangtanmi.github.io/posts/d6463418ab8b65c9f0b53ce42e94dbae/</link>
      <pubDate>Sun, 05 Aug 2018 16:14:13 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d6463418ab8b65c9f0b53ce42e94dbae/</guid>
      <description>seaborn是python中一个可视化的图像处理库，是在matplotlib的基础上进行封装，提供丰富的模板，通过调用模板并传入参数实现画图。 1.整体布局风格：set_style：设置图表的样式 1.1.5种主题风格：暗网格(darkgrid)，白网格(whitegrid)，全黑(dark)，全白(white)，全刻度(ticks) 1.2.风格细节的设置：despine（）：可删除图表两边的边框 set_context（）：设置图表的线宽大小，里面的参数可进行细节的设置&#xA;import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns def simplot(flip=1): x=np.linspace(0,14,100) for i in range(1,7): plt.plot(x,np.sin(x+i*.5)*(7-i)*flip) sns.set()#设置参数simplot() plt.show() #1.整体布局风格：set_style：设置图表的样式 #1.1.5种主题风格：暗网格(darkgrid)，白网格(whitegrid)，全黑(dark)，全白(white)，全刻度(ticks) sns.set_style(&#34;whitegrid&#34;)#设置主题样式 data=np.random.normal(size=(20,6))+np.arange(6)/2 sns.boxplot(data=data) #盒图 plt.show() #主题为全黑色 sns.set_style(&#34;dark&#34;)#将“dark”进行改变就能获得其他主题色 simplot() plt.show() #1.2风格细节的设置：用despine（）：可删除图表两边的边框 sns.violinplot(data)#图的类型，sns格式就是调用模板然后传入数据 sns.despine(offset=100,left=True )#**图的下端离X轴的距离,左边的轴消失，即删除边框** plt.show() #设置图表的线宽大小：set_context（） sns.set_context(&#34;paper&#34;,re={&#34;lines.linewidth&#34;:1.5})#“talk”，“poster”，“notebook”有这四种形式的表现方式,还可通过linewidth设置线宽 simplot() plt.show() 2.接下来是关于调色板方面的一些知识：其中有一点去掉图片的边框这在matplotlib中是无法实现的，但可以通过seaborn实现，sns.despine(left=True,right=True,top=True,bottom=True)#去掉图片的边框&#xA;import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns #2.调色板（color_palette()能传入任何matplotlib所支持的颜色 #color_palette()不写参数默认颜色 #set_palette（）设置所有图的颜色 #2.</description>
    </item>
    <item>
      <title>我在部署ArcGIS API for Python时踩到的坑</title>
      <link>https://anwangtanmi.github.io/posts/7de596d7e029822d3b2ffdbee6000fcf/</link>
      <pubDate>Mon, 30 Jul 2018 10:17:31 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/7de596d7e029822d3b2ffdbee6000fcf/</guid>
      <description>ArcGIS API for Python相比于其他ESRI产品，还是很年轻。我在部署时踩到了坑，网上也找不到解决方法，很是煞风景，也很打击学习的积极性。&#xA;今天回顾一下，做个总结吧。一方面自己备忘，另一方面希望能帮到同行的小伙伴。&#xA;看ESRI的官方文档，ArcGIS API for Python部署很简单呀：&#xA;1 安装Anaconda&#xA;2 python环境中安装Arcgis包&#xA;关于版本要求，ESRI的官方文档是这样写的：&#xA;我装了Anaconda3-4.4.0（内置Python3.6），然后在默认Python环境下“conda install -c esri arcgis”安装ArcGIS包。一切顺利。&#xA;然后用jupyter notebook&#xA;出来的结果是MapView(basemaps=[‘dark-gray’, ‘dark-gray-vector’, ‘gray’, ‘gray-vector’ ……没有出来地图！&#xA;打开浏览器的开发者模式，看抓包信息里面，没有去调瓦片。&#xA;问题原因：Python3.6环境不支持ArcGIS API for Python 1.4.2&#xA;解决办法：新建Python3.5环境（conda create -n agsenv python=3.5），在此环境下安装ArcGIS API for Python（conda install -c esri arcgis）&#xA;分析：可能是arcgis1.4.2出文档时python最新版本还是3.5的吧。ESRI以为以后的Python版本也不会有问题。而我也天真地相信了。&#xA;又过了几天，我在一台新电脑上部署环境。这次我特意下载了Anaconda3-4.2.0-Windows-x86_64（内置Python3.5）。装好了Anaconda，在默认base环境下安装Arcgis包。&#xA;装的过程中，会出现 ImportError: cannot import name ‘ensure_dir_exists‘ 这样的错误。&#xA;但是import arcgis还是正常的&#xA;所以没有管它。但是运行jupyter notebook报错：&#xA;网上找到解决问题的方法：&#xA;conda update jupyter_core jupyter_client&#xA;jupyter升级到了最新版本，jupyter notebook正常了。&#xA;但是arcgis好像还是不好使。&#xA;仅仅升级jupyter是不够的，（我理解是因为jupyter版本低的原因，arcgis向jupyter中添加mapview失败）。因此需要卸载掉Arcgis包，再重新安装。当然如果是先升级jupyter，再安装arcgis包，就不会有这个问题了。&#xA;conda uninstall esri arcgis</description>
    </item>
    <item>
      <title>Python爬虫：网络信息爬取与处理知识梳理</title>
      <link>https://anwangtanmi.github.io/posts/70b3bda1564cee79fb1a62b67d0630c3/</link>
      <pubDate>Wed, 25 Jul 2018 22:45:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/70b3bda1564cee79fb1a62b67d0630c3/</guid>
      <description>HTTP协议 应用层协议 无状态：每次连接，传输都是独立的 无连接：每次连接只处理一个请求&#xA;HTTP请求 GET：没有request body POST: 有request body&#xA;HTTP状态码 2xx：成功 3xx：跳转 4xx: 客户端错误 403 Forbidden 没有登录，或ip被封 5xx：服务端错误&#xA;python urllib2会自动302跳转&#xA;爬取策略 种子站点 深度优先 广度优先&#xA;去重策略 1、数据库unique字段存访问过的url 效率太低 2、hashset存访问过的url O(1) 消耗内存 3、md5计算访问过的url后保存到hashset或数据库 碰撞概率较小 4、bit-map，访问过的url的md5值再经过哈希函数映射到bitset某一位 碰撞概率较大 5、bloom filter 使用多个哈希函数，创建一个m位的bitset,先初始化所有位为0，然后选择k个不同的哈希函数，第i个哈希函数对字符串str哈希的结果记为h(i, str),且h(i, str)的范围是0 – m-1&#xA;评估网页数量 百度：site:www.mafengwo.cn google：site:www.mafengwo.cn/travel-scenic-spot&#xA;pip install murmurhash3 bitarray pybloomfilter&#xA;安装以下两个组件之后依然安装失败 visualcppbuildtools_full.exe vc_redist.x64.exe&#xA;http://www.mafengwo.cn/robots.txt&#xA;Sitemap top-down&#xA;pip install lxml&#xA;爬取工具 多线程：线程来回切换造成额外开销 多进程多ip，可以提高效率 数据库具有读写保护&#xA;分布式数据库 mongodb， redis， hbase 分布式爬虫 分布式系统 master – slave 主从模式</description>
    </item>
    <item>
      <title>180709 利用Python与OpenCV裁剪图像做数据增强</title>
      <link>https://anwangtanmi.github.io/posts/ca34ccff47f3f513fa5eda42b6feee1b/</link>
      <pubDate>Mon, 09 Jul 2018 14:33:47 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ca34ccff47f3f513fa5eda42b6feee1b/</guid>
      <description>原始图像 裁剪图像 # -*- coding: utf-8 -*- &#34;&#34;&#34; Created on Mon Jul 9 11:38:19 2018 @author: guokai_liu &#34;&#34;&#34; import numpy as np import matplotlib.pyplot as plt import cv2 filename = &#39;Yuna2.jpg&#39; def crop_figure(fn,kw=100,kh=100,sx=50,sy=50): # assgn saving name sn = fn.split(&#39;.&#39;)[0] # read image img = cv2.imread(fn) # set parameters f_h, f_w,f_c = img.shape k_w = kw k_h = kh s_x = sx s_y = sy # get output numbers of rows and columns n_y = (f_h-k_w)//s_y n_x = (f_w-k_h)//s_x # begin points for rows and columns c_x = [i+s_x*i for i in range(n_x)] c_y = [i+s_y*i for i in range(n_y)] # crop images for idx_y, y in enumerate(c_y): for idx_x,x in enumerate(c_x): crop_img = img[y:y+k_h,x:x+k_w] cv2.</description>
    </item>
    <item>
      <title>深度学习从看懂到看开(一)——————-Tensorflow的安装</title>
      <link>https://anwangtanmi.github.io/posts/24d49563e363f86e08143c75da13da09/</link>
      <pubDate>Tue, 19 Jun 2018 21:29:20 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/24d49563e363f86e08143c75da13da09/</guid>
      <description>Tensortflow 简介： TensorFlow是&#xA;谷歌基于DistBelief进行研发的第二代&#xA;人工智能&#xA;学习系统，其命名来源于本身的运行原理。Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统。 TensorFlow可被用于&#xA;语音识别或&#xA;图像识别等多项机器学习和深度学习领域，对2011年开发的深度学习基础架构DistBelief进行了各方面的改进，它可在小到一部智能手机、大到数千台数据中心服务器的各种设备上运行。TensorFlow将完全开源，任何人都可以用。 一：安装python3 本文的python版本都是python3的，python官方已经宣布在2020年全面停止对Python2的维护更新，所以我们要趁早熟悉并使用python3.&#xA;废话不多说，现在我们开始来在ubuntu下安装python3.&#xA;系统默认安装Python2&#xA;安装Python3的命令&#xA;sudo apt-get install python3.6 安装成功后在终端输入 python 如果出现如图所示即安装成功&#xA;二：安装anaconda anaconda指的是一个开源的&#xA;Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。&#xA;[1]&#xA;因为包含了大量的科学包，Anaconda 的下载文件比较大（约 515 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用&#xA;Miniconda这个较小的发行版（仅包含conda和 Python）。 安装了anaconda后可以不用再去网上找各种各样的科学技术包，直接调用anaconda的编译环境就好，方便以后我们使用tensorflow. 下载 官方下载地址：&#xA;https://www.continuum.io/downloads&#xA;所有安装包地址：&#xA;https://repo.continuum.io/archive/&#xA;安装 安装较为简单，这里参考官方文档：https://docs.continuum.io/anaconda/install/linux.html 在终端中切换到你下载的anaconda文件目录下执行：&#xA;bash +xxxxxxx(你所下载的anaconda版本).sh&#xA;根据提示输入回车&#xA;一直输入yes 然后一直回车，它会自动帮你加入到环境变量中和创建文件夹&#xA;当出现如下画面即安装完成：&#xA;验证annacoda是否安装完成&#xA;在终端中输入python出现如下即验证成功&#xA;三：安装tensorflow 直接在终端输入命令：pip3 install tensorflow （python3.x的版本，支持CPU） 等待安装jies 验证tensorflow是否安装成功 进入python命令下，测试tensorflow：&#xA;import tensorflow as tf&#xA;sess = tf.Session()&#xA;hello=tf.constant(‘Hello,Tensorflow!’)&#xA;print(sess.run(hello))&#xA;出现：&#xA;即安装成功&#xA;四：Pycharm整合tensorflow环境 打开pycharm中的setting</description>
    </item>
    <item>
      <title>Win10安装Python-3.6.5-amd64&#43;PyQt5&#43;Pycharm教程</title>
      <link>https://anwangtanmi.github.io/posts/9a0af5e52c366a7626148b964b680489/</link>
      <pubDate>Sat, 16 Jun 2018 17:49:07 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9a0af5e52c366a7626148b964b680489/</guid>
      <description>下载Python-3.6.5-amd64 点击下载1Python-3.6.5-amd64版本，如需要Python3.6.5版本点击下载2，选择迅雷下载可以加速。&#xA;安装 Python-3.6.5-amd64 双击执行下载完成的exe文件，进入安装界面：勾选添加用户变量再选择自定义安装。&#xA;全部默认选择即可，继续下一步。&#xA;选择所有用户，自定义安装路径，点击Install安装。&#xA;Python-3.6.5-amd64开始安装…&#xA;下图提示安装成功！！点击Close关闭提示。&#xA;测试运行 win+r 调用运行窗口，并输入cmd&#xA;查看Python版本号&#xA;输出 hello world ！！&#xA;使用 pip 或 pip3 python3 已经默认安装 pip 错误使用 pip 方式&#xA;正确使用 pip 方式&#xA;安装PyQt5 python安装PyQt5对于版本要求比较高，然通过 pip 方式安装方便很多。&#xA;打开 cmd &amp;gt;python -m pip install Pillow &amp;gt;python -m pip install --upgrade pip &amp;gt;pip3 install sip &amp;gt;pip install --index-url https://pypi.douban.com/simple PyQt5 &amp;gt;pip install --index-url https://pypi.douban.com/simple PyQt5-tools 添加 PyQt5 环境变量 添加用户变量&#xA;变量名：QT_QPA_PLATFORM_PLUGIN_PATH&#xA;变量值：D:\Program Files\python3\Lib\site-packages\PyQt5\Qt\plugins&#xA;添加系统变量&#xA;测试 PyQt5，如下，证明成功</description>
    </item>
    <item>
      <title>python习题及答案</title>
      <link>https://anwangtanmi.github.io/posts/ffe66a0409dfd99e6a004554511ab597/</link>
      <pubDate>Sun, 10 Jun 2018 15:50:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ffe66a0409dfd99e6a004554511ab597/</guid>
      <description>作业四&#xA;按要求编写程序（任选三题）&#xA;1、 编写一个从 1 加到 end 的当型循环。变量 end 的值由键盘输入。假如输入 end&#xA;的值为 6，则代码输出的结果应该是 21，也就是 1+2+3+4+5+6 的结果（不要用&#xA;sum 作为变量，因为它是内置函数）。&#xA;a = input() b=int(a) total = 0 for i in range(b+1): total = i+total print(total) 2、从键盘输入一个整数，判断该数字能否被 2 和 3 同时整除，能否被 2 整除，能否被 3 整除，不能被 2 和 3 整除。输出相应信息。&#xA;a = input() b=int(a) if b%2==0 and b%3==0: print(&#39;该数字能被 2 和 3 同时整除&#39;) elif b%2==0: print(&#39;该数字能被 2 整除&#39;) elif b%3==0: print(&#39;该数字能被 3 整除&#39;) else : print(&#39;该数字不能被 2 和 3 整除&#39;) 3、 一个数如果恰好等于它的因子之和，这个数就称为“完数”，例如， 6 的因子</description>
    </item>
    <item>
      <title>Ubuntu16.04&#43;cuda8.0&#43;cudnn5.1&#43;anaconda&#43;tensorflow0.12.1暗影精灵三GTX1080ti</title>
      <link>https://anwangtanmi.github.io/posts/471dee9e5028129552513c002855b571/</link>
      <pubDate>Tue, 08 May 2018 20:47:12 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/471dee9e5028129552513c002855b571/</guid>
      <description>电脑配置：i7-8700K 16G 256GSSD＋2T GTX1080Ti 由于要用tf==0.12.1，所以配置的深度学习环境并不是最新的。想安装最新的ＴＦ可以参考此教程&#xA;1、win10+Ubuntu16.04双系统，安装教程、补充教程。&#xA;2、cuda8.0+cudnn5.1+anaconda 安装教程，建议从作者提供的百度云链接下载，英伟达官网链接有点慢。&#xA;注：安装英伟达的驱动，桌面&amp;gt;&amp;gt;系统设置&amp;gt;&amp;gt;软件和更新&amp;gt;&amp;gt;附加驱动&amp;gt;&amp;gt;选择英伟达的驱动。&#xA;注：ｃｕｄａ的例子没有下载，所以教程中没有ｍａｋｅ例子。&#xA;3、用anaconda安装TF，参照tensorflow官网&#xA;conda create -n tensorflow ##后面加 pip python=2.7 (or 3.X) #Activate the conda environment by issuing the following command: source activate tensorflow (tensorflow)$ # Your prompt should change (tensorflow)$ pip install --ignore-installed --upgrade \ tfBinaryURL ##轮子地址 也可以直接 pip install tensorflow-gpu==0.12.1&#xA;激活conda环境后，进入python&#xA;检验TF和cuda是否安装成功&#xA;# Python import tensorflow as tf hello = tf.constant(&#39;Hello, TensorFlow!&#39;) sess = tf.Session() print(sess.run(hello)) 我的一开始报错，&#xA;I tensorflow/stream_executor/dso_loader.cc:119]Could’t open CUDA library libcudnn.</description>
    </item>
    <item>
      <title>python初学爬虫，使用urllib.request模块，爬取众筹网相关内容</title>
      <link>https://anwangtanmi.github.io/posts/c1750387ee3a1ea20b6a9e7f4c1158c5/</link>
      <pubDate>Wed, 25 Apr 2018 17:43:18 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c1750387ee3a1ea20b6a9e7f4c1158c5/</guid>
      <description>#python 3.6 import mysql.connector import re import urllib conn = mysql.connector.connect(user=&#39;root&#39;, password=&#39;root&#39;,host = &#39;localhost&#39;,database = &#39;test&#39;) cursor = conn.cursor() def getHtml(url): req = urllib.request.urlopen(url).read() html = req.decode(&#39;utf-8&#39;) return html def getData(html,leibie): reg = re.compile(&#39;(.*?)筹款进度&#xA;&#39;,re.S) xiangmu = re.findall(reg,html) # print(xiangmu) shengfen = [&#39;河北&#39;,&#39;山西&#39;,&#39;辽宁&#39;,&#39;吉林&#39;,&#39;黑龙江&#39;,&#39;江苏&#39;,&#39;浙江&#39;,&#39;安徽&#39;,&#39;福建&#39;, &#39;江西&#39;,&#39;山东&#39;,&#39;河南&#39;,&#39;湖北&#39;,&#39;湖南&#39;,&#39;广东&#39;,&#39;海南&#39;,&#39;四川&#39;,&#39;贵州&#39;,&#39;云南&#39;, &#39;陕西&#39;,&#39;甘肃&#39;,&#39;青海&#39;,&#39;台湾&#39;,&#39;内蒙古&#39;,&#39;广西&#39;,&#39;西藏&#39;,&#39;宁夏&#39;,&#39;新疆&#39;,&#39;香港&#39;,&#39;澳门&#39;] zhixiashi = [&#39;北京&#39;,&#39;天津&#39;,&#39;上海&#39;,&#39;重庆&#39;] for x in range(len(xiangmu)): name = re.findall(&#39;class=&#34;siteCardICH3&#34; title=&#34;(.*.)&#34; target=&#34;_blank&#34;&#39;,xiangmu[x]) # print(name) yichouzhichijindu = re.findall(&#39;(.*.)&#xA;&#39;,xiangmu[x]) label = re.findall(&#39;site_ALink siteIlB_item&#34; target=&#34;_blank&#34;&amp;gt;(.*)&#39;,xiangmu[x]) index = 0 while 1: if label[index] in shengfen: province = label[index] city = label[index+1] index += 1 break elif label[index] in zhixiashi: province = label[index] city = &#39;&#39; index += 1 break else: index += 1 name = name[0].</description>
    </item>
    <item>
      <title>Pycharm在线/手动离线安装第三方库-以scapy为例（本地离线添加已经安装的第三方库通过添加Path实现）</title>
      <link>https://anwangtanmi.github.io/posts/cf3b39f860facbeea34a1b1301bfa66e/</link>
      <pubDate>Thu, 19 Apr 2018 16:59:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/cf3b39f860facbeea34a1b1301bfa66e/</guid>
      <description> 在线安装 运行Pycharm,打开需要添加scapy文件的项目，以TestScapy为例 点击工具栏的File选项，选中Settings,单击打开 选中Settings的Project-&amp;gt;Project Interpreter选项 点击右边选项栏上的‘+’按钮，在 弹出的Available Packages的搜索框中搜索想要添加所需要的第三方库的名称，例如scapy,选中想要的搜索结果，点击Install Package,然后等待安装成功即可。 安装成功的标志： 本地安装-通过添加第三方库的Path实现 前两部与在线安装操作相同，然后在打开的Settings窗口选择 设置-&amp;gt;Show all 在弹出的Progect Interpreters窗口中选择第5个按钮，点击弹出的Interpreter Patha 窗口中选择‘+’按钮，在弹出的Select Path窗口，选择添加本地第三方库文件（例如Scapy）所在位置的路径，点击‘OK’按钮，完成配置。 添加成功后Inerpreter Paths窗口多出一行，点击ok确认，退出重新运行程序即可（注：需要等待pycharm更新配置完成，才能运行成功，pycharm最下方有显示更新进度的进度条） </description>
    </item>
    <item>
      <title>python学习笔记（三）- numpy基础：array及matrix详解</title>
      <link>https://anwangtanmi.github.io/posts/ce57efaf3df4117cd35e8ebe1e44b166/</link>
      <pubDate>Fri, 19 Jan 2018 11:31:24 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ce57efaf3df4117cd35e8ebe1e44b166/</guid>
      <description>Numpy中的矩阵和数组 numpy包含两种基本的数据类型：数组（array）和矩阵（matrix）。无论是数组，还是矩阵，都由同种元素组成。&#xA;下面是测试程序：&#xA;# coding:utf-8 import numpy as np # print(dir(np)) M = 3 #---------------------------Matrix--------------------------- A = np.matrix(np.random.rand(M,M)) # 随机数矩阵 print(&#39;原矩阵：&#39;,A) # A矩阵 print(&#39;A矩阵维数：&#39;,A.shape) # 获取矩阵大小 print(&#39;A的转置:&#39;,A.T) # A的转置 print(&#39;sum=&#39;,np.sum(A,axis=1)) # 横着加 print(&#39;sorted=&#39;,np.sort(A,axis=1)) # 竖着排 print(&#39;sin(A[0])=&#39;,np.sin(A[0])) # 第一行元素取余弦值 print(&#39;A*A.T=&#39;,A*A.T) # A*A.T print(&#39;A.*A=&#39;,np.multiply(A,A)) # 点乘 print(&#39;mean(A)=&#39;,np.mean(A)) # 平均值,mean(A,axis=1)亦可 print(&#39;Rank(A)=&#39;,np.linalg.matrix_rank(A)) # 矩阵的秩 #--------------------------Array-----------------------------# B = np.array(np.random.randn(2,M,M)) # 可以是二维的 print(&#39;B =&#39;,B) # 原矩阵 print(&#39;Size(B)= [&#39;,B.shape[0],B.shape[1],B.shape[2],&#39;]; ndim(B)=&#39;,B.ndim) print(&#39;B[0]=&#39;,B[0]) # 第一维 Position = np.where(B[0]&amp;lt;0) #numpy.</description>
    </item>
    <item>
      <title>Windows10（64bit,显卡GTX1050Ti）环境下的python3.5.2&#43;tensorflow（gpu）&#43;opencv安装配置</title>
      <link>https://anwangtanmi.github.io/posts/844afdd9327d131a34551d467d574638/</link>
      <pubDate>Sat, 04 Nov 2017 17:34:35 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/844afdd9327d131a34551d467d574638/</guid>
      <description>Windows10（64bit,显卡GTX1050Ti）环境下的python3.5.2+tensorflow（gpu）+opencv安装配置 笔记本环境: windows10(64位)，显卡GTX050Ti 安装前的注意事项: 1.TensorFlow目前在windows下只支持64-bit Python 3.5 2.tensorflow1.3当前只支持CUDA8.0&#xA;1.安装python 3.5 这里，笔者使用的是python 3.5.2，具体安装如下： (1).Python3.5.2安装教程_百度经验http://jingyan.baidu.com/article/a17d5285ed78e88098c8f222.html 备注：想要下载安装anaconda 3.5的朋友可以参考下面这篇博文： http://blog.csdn.net/sb19931201/article/details/53648615&#xA;2.配置tensorflow(gpu)+opencv+其他 (2).pip升级（python3.5.2使用） 最新版本指令：打开CMD，输入 python -m pip install –upgrade pip&#xA;(3).安装tensorflow 命令:pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow/tensorflow-gpu 或pip install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow（CPU版）&#xA;(4).安装opencv 命令:pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python&#xA;(5).安装matplotlib 指令：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib&#xA;(6).安装scipy（找到安装包路径，在安装） 由于scipy依赖于有mkl的numpy库，而从pip安装的numpy的库不带mkl，所以需要从上面的网站下载。&#xA;这里，笔者分别下载了scipy-1.0.0-cp35-cp35m-win_amd64.whl和numpy-1.13.3+mkl-cp35-cp35m-win_amd64.whl放在python\Scripts文件夹下。 scipy下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy&#xA;numpy+mkl下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy&#xA;a.安装numpy+mkl，pip install &amp;lt;安装包的路径&amp;gt;numpy-1.13.3+mkl-cp35-cp35m-win_amd64.whl&#xA;b.安装scipy：pip install &amp;lt;安装包的路径&amp;gt;scipy-1.0.0-cp35-cp35m-win_amd64.whl (7).安装sklearn pip install -i https://pypi.tuna.tsinghua.edu.cn/simple sklearn&#xA;（8）.安装pyinstaller pip install -i https://pypi.</description>
    </item>
    <item>
      <title>python爬虫代理IP池(proxy pool)</title>
      <link>https://anwangtanmi.github.io/posts/c400ee992726fc5efc4e9cfef47ef0e8/</link>
      <pubDate>Thu, 14 Sep 2017 10:51:18 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c400ee992726fc5efc4e9cfef47ef0e8/</guid>
      <description>1.今天我们来讲下一个非常有用的东西，代理ip池，结果就是一个任务每隔一定时间去到 目标ip代理提供网站（www.bugng.com）去爬取可用数据存到mysql数据库，并且检测数据库已有数据是否可用，不可用就删除。&#xA;2. 编写 提取代理ip到数据库 的爬虫&#xA;2.1准备mysql表&#xA;CREATE TABLE `t_ips` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT &#39;主键&#39;, `ip` varchar(15) COLLATE utf8_unicode_ci DEFAULT NULL COMMENT &#39;ip&#39;, `port` int(10) NOT NULL COMMENT &#39;port&#39;, `type` int(10) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;0:http 1:https&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=421 DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci COMMENT=&#39;ip表&#39;; 2.2创建爬虫工程，编写items.py(对应数据库的字段)&#xA;import scrapy class IpsItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() ip = scrapy.</description>
    </item>
    <item>
      <title>使用pillow实现tensorflow中的一些图像增强函数(crop,contrast,flip,per_image_standardization)</title>
      <link>https://anwangtanmi.github.io/posts/fab4b1ba6c2baa055a262724503a12a1/</link>
      <pubDate>Mon, 14 Aug 2017 13:12:58 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/fab4b1ba6c2baa055a262724503a12a1/</guid>
      <description>使用tensorflow自带的tf.random_crop()、tf.image.random_flip_left_right()、以及random_contrast()等函数处理图像数据时，不仅需要使用session，而且处理速度非常慢。使用Pillow库完成这些函数接口，在实际数据处理时就非常快速和方便。&#xA;#!/usr/bin/env python #-*- coding:utf-8 -*- ############################ #File Name: pic_process_PIL.py #Author: Wang #Mail: @hotmail.com #Created Time:2017-08-14 10:28:14 ############################ from PIL import Image import ImageEnhance import numpy as np from random import randint import random img = Image.open(&#39;1.jpg&#39;) #print img.format, img.size, img.mode #img.resize((1080,768)) #img.crop((14,14,79,79)).show() #print img.getpixel((1920,1080)) def random_crop(img, width, height): width1 = randint(0, img.size[0] - width ) height1 = randint(0, img.size[1] - height) width2 = width1 + width height2 = height1 + height img = img.</description>
    </item>
    <item>
      <title>opencv实现视频实时去雾算法</title>
      <link>https://anwangtanmi.github.io/posts/41c7baadb49f115be7b37dfa199d0e0a/</link>
      <pubDate>Mon, 07 Aug 2017 15:48:38 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/41c7baadb49f115be7b37dfa199d0e0a/</guid>
      <description>现在一系列的文章，有不少算法都于去雾有关，比如限制对比度自适应直方图均衡化算法原理、实现及效果、局部自适应自动色阶/对比度算法在图像增强上的应用这两个增强算法都有一定的去雾能力，而最直接的就是《Single Image Haze Removal Using Dark Channel Prior》一文中图像去雾算法的原理、实现、效果及其他 一文，描述了暗通道去雾这一state-of-the-art algorithms的过程和实现，虽几经优化，对于常用的视频1024*768大小的图片，算法处理部分还是需要70MS的时间（I7 笔记本CPU），因此，这一算法用于实时要求时还有一定的难度，并且优化后的算法基本无法并行，而可并行的算法重复计算大，由于不熟悉GPU方面的理念，不晓得使用不优化的算法靠GPU是否能有多大速度的提升。 为此，我一直在找寻相关的论文，这种找寻的踪迹一般就是看到一篇好论文–》看其参考文献–》再看参考文献的参考文献，这样循环下去。 然后有某种机会或巧合，又看到一篇好论文，重复前面的过程，你就会发现很多交集，慢慢的就会有一些好运向你招手。 话说我原本只看英文的文献，所以一直忽略了国内的文章，前几日，一个QQ朋友推荐了一篇清华大学的论文，下载后稍微看了下，觉得其描述的结果还是比较吸引人的，于是就实现了下，实时的效果应该说很不错，这里就简单的介绍并推荐给大家。 算法原理没有什么复杂的地方，其实说原理，还不如说经验或实验，因为论文中可以用理论来推导的公式确实不多。不过这也没关系，有用的东西就应该拿来用&#xA;# -*- coding: utf-8 -*- from package_2 import autolevel import cv2 #cap = cv2.VideoCapture(&#39;LP_20170318104820.avi&#39;) cap = cv2.VideoCapture(&#39;test.avi&#39;) original_name = &#39;original&#39; dehaze_name = &#39;dehaze&#39; cv2.namedWindow(original_name,cv2.WINDOW_NORMAL) cv2.namedWindow(dehaze_name,cv2.WINDOW_NORMAL) def nop(): pass #def handle(): cv2.createTrackbar(&#39;lowcut&#39;,dehaze_name,0,100,nop) cv2.createTrackbar(&#39;highcut&#39;,dehaze_name,0,250,nop) success,frame = cap.read() iframe = frame while not success: success,frame = cap.read() iframe = frame while success: cv2.imshow(original_name,frame) lowcut = cv2.getTrackbarPos(&#39;lowcut&#39;,dehaze_name)/1000.0 highcut = cv2.</description>
    </item>
    <item>
      <title>安装pycharm for mac</title>
      <link>https://anwangtanmi.github.io/posts/9f7f90951133858c3a4b0223116b2da3/</link>
      <pubDate>Sat, 29 Jul 2017 07:15:49 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9f7f90951133858c3a4b0223116b2da3/</guid>
      <description>从官网下载，安装完之后设置一下主题的颜色，设置为暗黑色的用起来比较舒服。但是按照网上其他人的教程设置时，都说在File-&amp;gt;setting里面有选项，但是我在File里怎么也找不到Setting这个选项卡，最后在Pycharm-&amp;gt;Preferences中找到了。 发现很多的软件，在Preferences里面都有类似Setting的选项，以后也记得去这里设置吧。&#xA;将Apperance-&amp;gt;Theme改为Darcula就行了。 改好后的效果如下图。 同时，下图也是设置Python解释器的操作。我的系统中装了好几个python，在这里需要指定一下用哪个。除了版本的区别外，还有就是每个版本对应的库。因为我还装了Anaconda，想用它的解释器，所以就选最后的那个了，在解释器的下面会列出来当前这个解释器所安装的各种库，这些库其实都是Anaconda自己安装的，省了很多的操作，很方便。 下图是我选择了另外的解释器，可以看到，它下面的列表中几乎没有什么库，这可怎么用，到时候用啥没啥。所以还是选上面的Anaconda吧。 解释器的事情说完了，这面说说调试运行的操作。 具体来说就是在运行写好的代码之前，需要给它设置一下运行环境。具体如下图： 选择右上角的Edit Configuration那个按钮，进入了下图的界面： 然后点击左上角的那个“+”，添加一个配置。 我在这里使用的是Python这个选项，网上很多教程都说用Compound，但是我选择完了出现的界面又和他们的教程不一样，就没再按照他们的教程做，反正选择Python后运行也是好好的，结果也符合期望，就先用它吧，以后需要时再去使用compound。选择Python后又会出现对话框： 我起了跟源文件一样的名字。 然后接下来的这个界面比较重要。 上图中Script那一行可以不用填，最后把这个对话框关闭之后系统会自动填好，如下图。 下面的Script parameters这个比较有用，它是用来启动脚本时给脚本传递参数的，比如当前的这个脚本写好后文件名为mytest.py，它可以接受用户指定的参数，然后在命令行运行的时候需要这样写：&#xA;python mytest.py -a 1 -b 3 script parameters 后面的-a 1 -b 3就是传递给它的参数，要不然你在IDE里点击运行按钮之后，怎么把那一坨参数传给他？ 下图是在上图关闭后重新打开配置文件后的情况。可见，script一栏系统自动填好了，内容就是当前调试的py文件的路径。下面的参数我删除了，因为当前的测试程序没有解析参数的功能，楞添加参数后运行会出错误的。 最后上一张最终的运行图吧。 收工。</description>
    </item>
    <item>
      <title>关于python安装的几个版本的区别</title>
      <link>https://anwangtanmi.github.io/posts/0dc45ff6057cf34f8899de603aba9b94/</link>
      <pubDate>Thu, 22 Jun 2017 10:34:23 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0dc45ff6057cf34f8899de603aba9b94/</guid>
      <description>windows系统下： x86适合32位操作系统。 x86-64适合64位操作系统。 1、 web-based installer 是需要通过联网完成安装的 2、 executable installer 是可执行文件(*.exe)方式安装 3、embeddable zip file 嵌入式版本，可以集成到其它应用中。</description>
    </item>
    <item>
      <title>pycharm的字体修改与风格</title>
      <link>https://anwangtanmi.github.io/posts/b111c8154fe6a968c956f333a2e41faf/</link>
      <pubDate>Tue, 09 May 2017 23:20:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b111c8154fe6a968c956f333a2e41faf/</guid>
      <description>今天用cmd运行python一直出错，然后就想找一个python编译器试一下，于是就找到了pycharm（话说这货怎么和写java的idea那么像，官网都是那么像，而且都是东欧那群人搞出来的），后来发现编译器默认的风格好不舒服，然后就想修改一下，折腾了半天，如下&#xA;1，改成暗色，比较不伤眼&#xA;file -&amp;gt; settings -&amp;gt;appearance-&amp;gt;theme-&amp;gt;darcula&#xA;在appearance 的选项中，有一个override default fonds by …巴拉巴拉一堆，你修改一下size就是修改资源管理那一部分的字体大小，如图&#xA;2修改编辑的代码的字体的大小&#xA;file -&amp;gt; settings -&amp;gt;editor -&amp;gt;color&amp;amp;fonds-&amp;gt;fond-&amp;gt;size&#xA;我喜欢大一点的字体，看着舒服，所以修改到了17，一般15就差不多了</description>
    </item>
    <item>
      <title>【NSA黑客工具包】Windows 0day验证实验</title>
      <link>https://anwangtanmi.github.io/posts/8730e4e5a49bf9991e78b5c3dac7c2a6/</link>
      <pubDate>Thu, 04 May 2017 15:41:26 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/8730e4e5a49bf9991e78b5c3dac7c2a6/</guid>
      <description> 黑客组织发布出一份震惊世界的机密文档，其中包含了多个Windows 远程漏洞利用工具，此工具集覆盖大量的Windows服务 器，可以被任何人进行下载利用，攻击者可以利用工具对Windows目标主机进行溢出提权，危害极大。&#xA;目前已知受影响的 Windows 版本包括但不限于：Windows NT，Windows 2000（没错，古董也支持）、Windows XP、Windows 2003、Windows Vista、Windows 7、Windows 8， Windows 2008、Windows 2008 R2、Windows Server 2012 SP0，基本全球70%的Windows服务器可能都暴露在危险之中，这次事件影响力堪称网络大地震！&#xA;面对这种指哪儿打哪儿的神级漏洞，锦行科技CTO.Jannock、长亭科技.Monster、腾讯玄武实验室.TK、腾讯湛泸实验室.yuange等国内相关安全专家也给出了一些想法和解决方案。微软MSRC也发布了一则风险评估公告，报告中指出微软MSRC分析了由Shadow Brokers公开提供 的大量漏洞，大多数漏洞都已经被修补。剩余的漏洞，也被验证在Windows7、Windows近期版本、Exchange2010以及Exchange较新版本中没有得到复现，不过微软MSRC强烈建议仍在使用这些产品先前版本的用户升级到更新版本。&#xA;想要了解相关防御方案，来i春秋学院吧！i春秋全网首发，为您复现真实漏洞场景，将被暴 露出的工具包搭建到虚拟环境中，用于大家学习，了解修补方案，达到安全防护的目的。&#xA;课程目标 本次实验将复现真实漏洞场景，让同学们分别从&#xA;漏洞产生原理、&#xA;漏洞攻击手法以及&#xA;如何修复漏洞这三个方面进行学习。 </description>
    </item>
    <item>
      <title>网络爬虫二三事儿</title>
      <link>https://anwangtanmi.github.io/posts/cd9632ad89cd458430dbc346ebc92b3c/</link>
      <pubDate>Wed, 12 Apr 2017 10:33:27 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/cd9632ad89cd458430dbc346ebc92b3c/</guid>
      <description>目录(?)[+]&#xA;一、网络爬虫简介 网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。&#xA;二、网络爬虫分类 网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：&#xA;深层网络爬虫（Deep Web Crawler） 聚焦网络爬虫（Focused Web Crawler） 增量式网络爬虫（Incremental Web Crawler） 通用网络爬虫（General Purpose Web Crawler） 在实际的网络爬虫系统中，通常是几种爬虫技术相结合实现的。&#xA;三、聚焦网络爬虫 网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页URL，并重复上述过程，直到达到系统的某一条件时停止。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索；对于聚焦爬虫来说，这一过程所得到的分析结果还可能对以后的抓取过程给出反馈和指导。相对于通用网络爬虫，聚焦爬虫还需要解决三个主要问题： （1）对抓取目标的描述或定义； （2）对网页或数据的分析与过滤； （3）对URL的搜索策略。&#xA;四、网络爬虫示例 在本部分中，演示简单的网络爬虫过程，分别爬取本地和网页中“邮箱地址”信息，其中通过getMails()爬取本地邮箱地址，通过getMailsByWeb()爬取网页邮箱地址。&#xA;import java.io.BufferedReader; import java.io.FileReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.URL; import java.util.ArrayList; import java.util.Iterator; import java.util.List; import java.util.regex.Matcher; import java.util.regex.Pattern; public class InfoSpider { public static void main(String[] args) throws IOException { List list = getMailsByWeb(); // 创建迭代器对象 Iterator it = list.iterator(); while (it.</description>
    </item>
    <item>
      <title>【图像处理】Tensorflow:简易超分辨重建与坑</title>
      <link>https://anwangtanmi.github.io/posts/1c1e092adad079baccb5abc04ba452ce/</link>
      <pubDate>Fri, 31 Mar 2017 10:32:55 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1c1e092adad079baccb5abc04ba452ce/</guid>
      <description>超分辨重建是图像复原领域的一大热点，能在硬件有限的情况下最大还原原始场景的信号，在天文探索、显微成像等领域有重要作用。成像设备对物体成像时，由于距离较远，成像会模糊，可以类比多尺度高斯滤波；受限于成像机能，成像像素达不到最理想条件，可类比为对原始像进行一个下采样。超分辨重建就是要在这种条件下复原原始图像。 假设上帝有最好的成像设备，成像为X；我们成像设备成像为B，高斯滤波模板设为G；为了防止问题病态，加入lasso正则。那么有：&#xA;argmin [subsampling(conv(X,G))−B]2+λX&#xA;现在的问题是，Tensorflow如何表示subsampling并进行优化？&#xA;Tensorflow支持以下几种图像缩放/采样:&#xA;tf.image.resize_images，支持最近邻、双线性、双三次等缩放方法 tf.nn.max_pool 最大值下采样 tf.nn.avg_pool 均值下采样&#xA;现在我们逐个测试一下。图像经过三倍下采样： 1、tf.image.resize_images，双线性采样，振铃不严重，条纹很多： 2、tf.nn.max_pool，没有条纹、振铃，但是有一堆噪声，参数调了几次都没有什么更好的效果： 3、tf.nn.avg_pool，无条纹、噪声，有振铃，与原图相比颜色变暗，对比度下降: 4、来与原图做个对比 可以看出，效果最好的就是avg_pool了，在只有高斯模板参数，完全没有其他先验信息的情况，一秒钟内得到这个结果，已经让人非常惊讶了。猜测image-resize和max_pool其实在上下采样中都丢失相当多的信号，而avg_pool则保留了最多的信号，因此重建效果较好。 fast-neural-style文章提到过用感知特征来对图像进行超分辨重建，可以重建同样风格的细节，这个需要用生成网络对大量的图像进行训练，或者直接上vgg慢慢地计算感知特征来仿制风格细节。</description>
    </item>
    <item>
      <title>Python 面试题 https://github.com/taizilongxu/interview_python</title>
      <link>https://anwangtanmi.github.io/posts/1cc0f922f8e25918b0beed752c679a23/</link>
      <pubDate>Tue, 07 Feb 2017 16:48:19 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1cc0f922f8e25918b0beed752c679a23/</guid>
      <description> ﻿﻿ 转载&#xA;https://github.com/taizilongxu/interview_python&#xA;﻿﻿ </description>
    </item>
    <item>
      <title>Python网络编程：E-mail服务(八) 实现抄送和密送功能</title>
      <link>https://anwangtanmi.github.io/posts/aad8e8678f518bf4f48f616d2913b5a6/</link>
      <pubDate>Tue, 24 Jan 2017 17:30:04 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/aad8e8678f518bf4f48f616d2913b5a6/</guid>
      <description>简介 本文介绍如何通过smtp模块实现邮件的抄送和密送功能。 抄送功能实现 在发送邮件时，除了发送给相关的责任人，有时还需要知会某些人。这时就需要在邮件里指定抄送人员列表。相关实现如下： import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText import os FROMADDR = &#34;[email protected]&#34; PASSWORD = &#39;foo&#39; TOADDR = [&#39;[email protected]&#39;, &#39;[email protected]&#39;] CCADDR = [&#39;[email protected]&#39;, &#39;[email protected]&#39;] # Create message container - the correct MIME type is multipart/alternative. msg = MIMEMultipart(&#39;alternative&#39;) msg[&#39;Subject&#39;] = &#39;Test&#39; msg[&#39;From&#39;] = FROMADDR msg[&#39;To&#39;] = &#39;, &#39;.join(TOADDR) msg[&#39;Cc&#39;] = &#39;, &#39;.join(CCADDR) # Create the body of the message (an HTML version).</description>
    </item>
    <item>
      <title>python爬虫（requests）库安装</title>
      <link>https://anwangtanmi.github.io/posts/c4cfbc52cbee3217bd51b47b551c749d/</link>
      <pubDate>Fri, 16 Dec 2016 09:01:16 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c4cfbc52cbee3217bd51b47b551c749d/</guid>
      <description>requests是一个比较好的网络请求处理库&#xA;官网&#xA;http://www.python-requests.org/&#xA;安装&#xA;http://www.python-requests.org/en/master/user/install/#install&#xA;下载地址&#xA;https://github.com/kennethreitz/requests&#xA;1使用pip安装&#xA;(如果使用pip首先要安装pip https://pypi.python.org/pypi/pip 下载包后解压。执行python setup.py install)&#xA;然后在&#xA;pip install requests&#xA;2下载文件包。用命令行安装。下载&#xA;python setup.py install 尝试一下是否安装成功了&#xA;在python 中执行 import requests&#xA;没有报错。安装OK了</description>
    </item>
    <item>
      <title>八个最佳Python IDE</title>
      <link>https://anwangtanmi.github.io/posts/3757871ffe4667bc1fc58cd95a4c2c5d/</link>
      <pubDate>Sun, 02 Oct 2016 07:34:34 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3757871ffe4667bc1fc58cd95a4c2c5d/</guid>
      <description>转载至 http://blog.csdn.net/chszs Python是一种功能强大、语言简洁的编程语言。本文向大家推荐8个适合Python开发的IDE。&#xA;1. Eclipse with PyDev http://pydev.org/&#xA;Eclipse+PyDev插件，很适合开发Python Web应用，其特征包括自动代码完成、语法高亮、代码分析、调试器、以及内置的交互浏览器。&#xA;2. Komodo Edit http://komodoide.com/komodo-edit/&#xA;Komodo Edit是一个免费的、开源的、专业的Python IDE，其特征是非菜单的操作方式，开发高效。&#xA;3. Vim http://www.vim.org/download.php&#xA;Vim是一个简洁、高效的工具，也适合做Python开发。&#xA;4. Sublime Text http://www.sublimetext.com/&#xA;SublimeText也是适合Python开发的IDE工具，SublimeText虽然仅仅是一个编辑器，但是它有丰富的插件，使得对Python开发的支持非常到位。&#xA;5. Pycharm http://www.jetbrains.com/pycharm/&#xA;Pycharm是一个跨平台的Python开发工具，是JetBrains公司的产品。其特征包括：自动代码完成、集成的Python调试器、括号自动匹配、代码折叠。Pycharm支持Windows、MacOS以及Linux等系统，而且可以远程开发、调试、运行程序。&#xA;6. Emacs http://www.gnu.org/software/emacs/&#xA;Emacs是一个可扩展的文本编辑器，同样支持Python开发。Emacs本身以Lisp解释器作为其核心，而且包含了大量的扩展。&#xA;7. Wing https://wingware.com/&#xA;Wing是一个Python语言的超强IDE，适合做交互式的Python开发。Wing IDE同样支持自动代码完成、代码错误检查、开发技巧提示等，而且Wing IDE也支持多种操作系统，包括Windows、Linux和Mac OS X。&#xA;8. Pyscripter https://code.google.com/p/pyscripter/&#xA;Pyscriptor是一个开源的Python集成开发环境，很富有竞争力，同样有诸如代码自动完成、语法检查、视图分割文件编辑等功能。</description>
    </item>
  </channel>
</rss>
