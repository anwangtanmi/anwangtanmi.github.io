<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>图像处理 on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</link>
    <description>Recent content in 图像处理 on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 29 Sep 2019 10:38:06 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>穿墙透视算法|MIT华人Team通过墙壁和遮挡物的超强动作检测模型</title>
      <link>https://anwangtanmi.github.io/posts/478b8f6947bee6a2e5e26626af848f7c/</link>
      <pubDate>Sun, 29 Sep 2019 10:38:06 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/478b8f6947bee6a2e5e26626af848f7c/</guid>
      <description>介绍&#xA;了解人们的行为和互动通常取决于看到他们。从视觉数据中自动进行动作识别的过程已成为计算机视觉界众多研究的主题。但是如果太暗，或者人被遮挡或在墙壁后面怎么办？在本文中，我们介绍了一个神经网络该模型可以在光线不足的情况下通过墙壁和遮挡物检测人类的行为。我们的模型将射频（RF）信号作为输入，生成3D人体骨骼作为中间表示，并随着时间的推移识别多个人的动作和互动。通过将输入转换为基于中间骨架的表示形式，我们的模型可以从基于视觉的数据集和基于RF的数据集中学习，并允许这两个任务互相帮助。我们表明，在可见场景中，我们的模型可以达到与基于视觉的动作识别系统相当的准确性，但是在看不见人的情况下，该模型仍可以继续正常工作，因此可以解决超出当今基于视觉的动作识别的局限性的场景。&#xA;先来看一组动图&#xA;墙后动作可识别&#xA;黑暗环境可识别&#xA;他们的模型将射频（RF）信号作为输入，生成3D人体骨架作为中间表示，并随着时间的推移识别多个人的动作和互动。&#xA;多人模式可识别&#xA;在本文中，我们旨在弥合两个世界。我们引入了RF-Action，这是一个端到端的深度神经网络，可以识别无线信号中的人类动作。它的性能可与基于视觉的系统相媲美，但可以穿过墙壁和遮挡物，并且对光照条件不敏感。图1显示了RF-Action在两种情况下的性能。在左侧，两个人握手，但其中一个被遮挡。基于视觉的系统将无法识别动作，而RF-Action轻松将其归类为握手。在右侧，一个人正在打电话，而另一个人将要向他扔东西。由于光线不佳，基于视觉的系统几乎看不到后者。相反，RF-Action可以正确识别两个动作。&#xA;RF-Action基于多模式设计，可与无线信号和基于视觉的数据集一起使用。我们利用最近的工作显示即推断人类骨骼（即，姿势）从无线信号的可行性 ，并采用骨架作为适合于RF和基于视觉的系统的中间表示。使用骨架作为中间表示是有利的，&#xA;因为：&#xA;（1）它使模型与RF和视觉数据来训练，并利用现有的基于视觉的三维骨骼数据集如PKU-MMD和NTU-RGB + d [ 26，31 ]; （2）它允许对中间骨骼进行额外的监督，从而有助于指导学习过程，而不仅仅是过去基于RF的动作识别系统中使用的单纯动作标签；&#xA;（3）由于骨架表示受环境或主体身份的影响最小，因此提高了模型推广到新环境和新人的能力。&#xA;我们通过两项改进其性能的创新进一步扩展了我们的模型：首先，骨骼，尤其是从RF信号生成的骨骼，可能会出现错误和错误预测。为了解决这个问题，我们的中间表示除了骨架之外还包括每个关节上随时间变化的置信度得分。我们使用自我关注来允许模型随着时间的推移以不同的方式关注不同的关节，具体取决于它们的置信度得分。&#xA;其次，过去的动作识别模型可以随时生成单个动作。但是，场景中的不同人可能会采取不同的动作，如图1右图所示，一个人在电话上交谈，而另一个人在扔物体。我们的模型可以使用专门设计用于解决此问题的多提案模块来解决此类情况。&#xA;为了评估RF-Action，我们使用无线设备和多摄像头系统收集了来自不同环境的动作检测数据集。该数据集跨越25个小时，包含30个执行各种单人和多人动作的个人。我们的实验表明，RF-Action在可见场景中的性能可与基于视觉的系统相媲美，并且在存在完全遮挡的情况下仍能继续保持良好的性能。具体来说，RF-Action在无遮挡的情况下可达到87.8的平均平均精度（mAP），在穿墙场景中的mAP为83.0。我们的结果还表明，多模式训练可以改善视觉和无线模式的动作检测。使用RF数据集和PKU-MMD数据集训练模型，我们观察到测试集的mAP性能从83.3提高到87。&#xA;贡献：本文有以下贡献：&#xA;它提出了第一个使用无线电信号进行基于骨骼的动作识别的模型；它进一步证明了这种模型可以仅使用RF信号（如图1所示）就可以准确识别穿过墙壁的动作和相互作用，并且在极端恶劣的照明条件下。&#xA;本文提出了“骨架”作为跨各种形式来传递与动作识别相关的知识的中间表示，并通过经验证明这种知识传递可以提高绩效。&#xA;本文介绍了一个新的时空注意模块，该模块改进了基于骨骼的动作识别，而不管骨骼是从RF还是基于视觉的数据生成的。&#xA;它还提出了一种新颖的多提案模块，该模块扩展了基于骨骼的动作识别以检测多个人同时进行的动作和互动。&#xA;该图显示了我们系统的两个测试用例。在左侧，两个人握手，而其中一个在墙后。在右边，一个人躲在黑暗中，向另一个正在打电话的人扔东西。底行显示了由我们的模型生成的骨骼表示和动作预测。&#xA;相关知识&#xA;（a）基于视频的动作识别： 在过去的几年中，从视频中识别动作一直是一个热门话题。早期方法使用手工制作的功能。为实例，像HOG和SIFT图像描述符已经被扩展到3D 来提取视频时间线索。此外，诸如改进的密集轨迹（iDT）之类的描述符是专门设计用来跟踪视频中的运动信息的。&#xA;最新的解决方案基于深度学习，分为两大类。&#xA;第一类通过利用三维卷积网络提取运动和外观特征共同。&#xA;第二类分别通过使用两个流神经网络考虑空间特征和时间特征。&#xA;（二）基于骨架行为识别：基于骷髅动作识别最近获得广泛关注。这种方法具有多个优点。&#xA;首先，骨骼为人类动态提供了一种强大的表现力来抵抗背景噪声[ 23 ]。&#xA;其次，与RGB视频相比，骨骼更为简洁，这减少了计算开销，并允许使用更小的模型来适合移动平台。&#xA;基于骨骼的动作识别的先前工作可以分为三类。早期工作使用递归神经网络（RNN）对骨架数据中的时间依赖性进行建模。&#xA;然而，最近，文献转向了卷积神经网络（CNN），以学习时空特征并取得了令人印象深刻的性能。&#xA;此外，某些文件表示的骨架作为动作识别图形和利用图形神经网络（GNN）。&#xA;在我们的工作中，我们采用基于CNN的方法，并通过引入时空注意模块来处理从无线信号生成的骨骼，并在多提案中扩展了分层共现网络（HCN）模型。&#xA;模块以同时启用多个动作预测。&#xA;（C）无线电的基于动作的识别： 研究在无线系统中已经使用无线电信号探索动作识别，特别是用于家庭应用，其中隐私问题可以排除使用摄像机 。&#xA;这些作品可以分为两类：&#xA;第一类类似于RF-Action，因为它可以分析从人体反弹的无线电信号。他们用行动标签监督，简单分类。&#xA;他们只能识别简单的动作（例如步行，坐着和跑步），最多只能识别10个不同的动作。&#xA;而且，它们仅处理单人场景。&#xA;第二类依赖于传感器网络。&#xA;他们或者部署不同传感器，用于不同的动作，（例如，在冰箱门上的传感器可检测的饮食），或贴在每个主体部的可穿戴式传感器和基于其上的身体部位移动识别被摄体的动作。&#xA;这样的系统需要对环境或人的大量检测，这限制了它们的实用性和鲁棒性。&#xA;射频信号入门&#xA;同时记录RF热图和RGB图像。&#xA;我们使用在过去的工作常用于基于RF的动作识别一种类型的无线电的。&#xA;无线电会产生一个称为FMCW的波形，并在5.4至7.2 GHz之间工作。&#xA;该设备具有垂直和水平排列的两个天线阵列。&#xA;因此，我们的输入数据采用二维热图的形式，一个来自水平阵列，一个来自垂直阵列。如图2所示，水平热图是无线电信号在平行于地面的平面上的投影，而垂直热图是信号在垂直于地面的平面上的投影（红色表示大值，蓝色表示小值）。直观地，较高的值对应于来自某个位置的信号反射的强度更高。&#xA;无线电以30 FPS的帧速率工作，即每秒产生30对热图。&#xA;如图所示，RF信号与视觉数据具有不同的属性，这使基于RF的动作识别成为一个难题。尤其是：&#xA;穿过壁的频率中的RF信号的空间分辨率低于视觉数据。在我们的系统中，深度分辨率为10 cm，角度分辨率为10度。如此低的分辨率使得难以区分诸如挥手和梳头等活动。&#xA;人体在穿过墙壁的频率范围内镜面反射。RF镜面反射是当波长大于表面粗糙度时发生的物理现象。&#xA;在这种情况下，与散射体相反，物体的作用就像反射镜（即镜子）。我们收音机的波长约为5厘米，因此人类可以充当反射器。</description>
    </item>
    <item>
      <title>学习笔记之——水下图像增强/复原</title>
      <link>https://anwangtanmi.github.io/posts/a085b0f8c0625fcdd555d746fcbeaa70/</link>
      <pubDate>Mon, 29 Jul 2019 15:34:41 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a085b0f8c0625fcdd555d746fcbeaa70/</guid>
      <description>本博文为本人调研水下图像增强时做的一些笔记。可能整理得不是很清晰，仅仅供个人学习记录用~欢迎各位交流~&#xA;目录&#xA;背景&#xA;研究现状（常用的方法）&#xA;非物理模型的图像增强方法&#xA;白平衡方法&#xA;直方图均衡化方法&#xA;基于Retinex的方法&#xA;基于暗通道先验的方法（去雾的方法）&#xA;基于卷积神经网络的方法&#xA;基于物理模型的图像复原方法&#xA;水下图像成像理论&#xA;水下成像系统&#xA;水下散射模型&#xA;水下退化图像模糊类型&#xA;高斯模糊&#xA;运动模糊&#xA;散焦模糊&#xA;图像质量评价标准&#xA;参考资料：&#xA;背景 对于水下作业，特别是水下机器人作业等场景，水下图像增强具有广阔得应用前景（水下机器人主要是靠它的视觉系统来判断周围的环境信息，水下机器人的视觉系统就好比人体的眼睛，它可以为机器人提供了水下目标位置信息，根据目标的位置信息，研究人员可以对目标进行监测与追踪，它还可以将获得的环境实时状态一起抽象为供机器人管理的环境模型。）。基于声视觉和基于光视觉的水下目标检测识别技术是目前主流的水下目标检测识别技术。基于声视觉的水下目标检测识别技术是通过声呐实现的，但其图像信息采集能力弱，生成图像清晰度低。基于光视觉的水下目标检测识别技术则是通过光视觉传感器实现的，成像分辨率高的光视觉传感器更适用于短距离的目标识别和精确定位。二者相比，基于光视觉的水下目标检测识别技术在水下捕捞机器人应用中优势更为明显&#xA;基于光学的水下小目标检测识别是水下捕捞机器人智能化作业的关键。然而，基于光视觉的水下目标检测识别技术同样面临着巨大的挑战，其主要原因是海洋复杂成像环境导致光视觉系统获取到的水下图像严重退化（水下图像的衰退主要包括：光线吸收导致的颜色偏差，光线前向散射导致的细节模糊和光线后向散射造成的低对比度），出现颜色衰退、对比度低以及细节模糊等现象。首先，由于水对光的吸收作用，光线在传输过程中就会发生能量衰减，在一般情况下，红光在水中衰减最快，衰减最慢的是蓝绿色光线；另外，由于光在水中的散射作用也会造成水下图像成像效果不好。散射效应又分为前向散射和后向散射，前向散射的意思是水中物体反射的光向摄像机传输的过程中发生的小角度偏离原来的传输方向的散射现象；后向散射的意思是光线在照射到水中物体时遇到水中的杂质就会发生散射直接被摄像机接收的散射现象，导致图像对比度低。&#xA;严重退化的水下图像由于缺少用于目标识别的有效信息，导致水下目标检测识别难度提升。随着高科技水下成像设备的发展，获取的水下图像的质量也得到了一定程度的提升，但仍然存在颜色衰退、对比度低以及细节模糊等现象，此外实际应用成本也是需要考虑的问题，因此对水下图像进行增强仍然有其必要性。&#xA;水下图像处理技术的算法可以根据是否基于水下成像模型分为图像增强算法和图像复原算法两种。图像增强方法是对得到的水下图像的像素点进行研究增强，算法过程中不需要考虑图像的形成过程和降质过程；图像复原方法是要根据水下图像的成像过程来获得出真实的情况。图像复原方法需要水体的光学参数、摄像机参数和摄像机与目标物体的距离等信息，这些信息都要通过人工测量或其他方法估测出这些值。&#xA;水下视觉探测技术的重点和难点有两个方面，一是水下光传感设备参数的标定，另一个是水下模糊图像的复原。由于水质的复杂性，导致光线在水下的传播有折射、散射等现象，同时可能存在的浑浊水质也是导致水下图像退化的主要原因，水下图像就因为这种降质因素产生了严重的模糊。目前科学技术处理这种模糊还存在一定的难度。由于无法获得原始的水下清晰图像，以及无法精确测量到导致水下图像发生退化的模糊函数，水下图像复原技术的应用一直受到限制，也间接限制了水下探测等技术的发展。因此如何根据水下图像的形成机制和水下图像发生退化的原因，建立一个合理的水下图像退化的物理模型，同时使得该物理模型能够实际应用，能够通过软硬件手段估计得到模糊函数来复原水下图像是目前的研究重点，具备实际的研究价值和意义。(由于水下环境的复杂，且难以获得ground truth，故此若采用DL来做，其中一个难点就是如何获取成pair的训练数据，除非采用无监督学习)&#xA;研究现状（常用的方法） 受水下光传播过程中的衰减和散射的影响，在纯水区域中，水下能见度一般为20m，在浑浊海水中的能见度一般只有 5m。&#xA;McGlamery在 1979 年搭建了经典的计算机水下成像系统模型。他发现，摄像机水下成像系统获得到的光能量可以分成三个部分：直接传输的光能量、前向散射的光能量和后向散射的光能量。&#xA;在图像处理中，图像复原又叫做图像恢复，该项技术有着广泛的应用领域。同时图像复原技术又与其他图像技术之间有着一定的联系，比如：与图像增强相比，两种技术在一定程度上都是针对图像进行改善，从而提高图像质量。但是这两项技术有着不一样的图像质量评价指标和不同的设计算法。相对而言，图像增强一般是增强视觉感受，偏向于人的主观判断，丢失的细节信息不会得到修复。而图像复原计算则是根据图像退化的模型，进行图像建模，设计一定的代价函数来优化逆问题，从而估计出原始图像和模糊函数。经典的图像复原方法一般都是基于先验知识设计的，已知的先验知识越多，复原出来的效果就越好。但是，在实际应用中，先验知识往往都是未知的。因此经典的方法限制了实际的应用。&#xA;已知图像在采集、传输、储存和处理过程中，会出现畸变、模糊、失真和附加噪声的影响，造成图像发生降质，这种现象一般称为图像退化。图像复原的关键在于建立图像退化模型，此退化模型应该能反映图像退化的原因。造成图像退化的原因很多，典型原因表现为：成像系统的像差、畸变、带宽有限造成图像失真；太阳辐射、大气湍流、云层遮挡等造成的遥感图像失真；由于成像器件拍摄姿态和扫描非线性引起的图像几何失真；成像传感器与被拍摄景物之间的相对运动，引起所成图像的运动模糊；光学系统或成像传感器本身特性不均匀，造成同样亮度景物成像灰度不同；由于场景能量传输通道中的介质特性如大气湍流效应、大气成分变化引起图像失真；图像在成像、数字化、采集和处理过程中引入的噪声等。&#xA;（tips：傅里叶变换是一种频域变换，是将图像的空间域信息转换到频率域中，傅里叶变换的应用一般是将在空域中不明显的信息或隐藏的信息转换到频域中，使其在频域中很明显地显现出来。）&#xA;传统的图像增强方法有很多种，主要分为两大部分：空域图像处理和频域图像处理。空域图像处理方法直接针对图像的像素点，以灰度映射为基础来改善灰度层级，例如直方图均衡化、限制对比度直方图均衡化、灰度世界假设等，此外，还可以通过滤波的方式对图像去除噪声达到图像增强的目的，例如中值滤波、均值滤波等；频域图像增强方法则通过各种频域变换，如傅里叶变换、小波变换等，可以间接地增强图像。国内外很多学者采用空域和频域方法对水下图像进行了增强，传统的图像增强算法在一定程度上可以消除图像模糊、增强边缘等，但仍存在噪声大、清晰度较低和颜色失真等问题，因此还需要进一步加强和完善。&#xA;目前，用于增强或复原水下图像的处理方法大致可分为非物理模型的图像增强方法和基于物理模型的图像复原方法。&#xA;非物理模型的图像增强方法 图像增强技术不必过多考虑图像成像的过程和模型，可以叫非物理模型方法，这种方法力图通过单纯的图像处理手段提高水下图像质量，通过调整图像的像素值来改善视觉质量，不过实现过程往往较为复杂。&#xA;该类方法采用直接调整图像像素值的方式改善图像质量，并不考虑水下图像退化的物理过程（就是没有考虑退化模型、水下得信道），属于图像增强范畴。&#xA;在水下图像增强技术研究早期，对水下图像的处理经常直接应用一些空气中传统的的图像增强算法，传统的图像增强算法可以分为空间域法与频域法。空间域方法是对图像中的像素点直接处理，采用灰度映射的方法，比如选取合适的映射变换来增加图像的对比度，改善图像灰度级等。频域法是一种间接的图像处理方法，运用变换技术把图像映射到某种变换域内，然后利用变换域中特有性质进行某种滤波处理，再反变换到空间域内，即得到增强后的图像。常被应用到水下图像传统空间域增强算法有直方图均衡化、限制对比度直方图均衡化、灰度世界假设和白平衡算法等，频域增强算法有傅里叶变换、小波变换和滤波技术，主要包括低通滤波，高通滤波和同态滤波等。目前图像增强算法往往存在可能只是对某一类的图像增强效果好，而其他类型的效果不好的特点，而且由于水下环境的特殊性，仅仅通过研究把传统的图像增强算法应用在水下图像上无法彻底解决水下图像退化问题。&#xA;水下图像増强算法研究早期，传统的图像处理方法，例如白平衡，灰度世界假设和灰度边缘假设等颜色修正算法，直方图均衡化和限制对比度直方图均衡化等对比度增强算法，多被用来增强水下图像。相比于处理普通图像获取的较好的结果，这些方法在处理水下图像获取的结果并不理想，其主要原因是海洋环境复杂，多重不利因素如水介质对光线的散射、吸收作用以及水下悬浮粒子等对其产生严重干扰。考虑到水下图像存在的颜色衰退、低对比度以及模糊等特点，研究人员往往从图像颜色、对比度、细节等方面入手对水下图像进行增强。&#xA;白平衡方法 在不同光源照射下，观察同一个物体会发现其呈现的颜色是不同的，原因是不同光源具有的不同色温造成目标物体的反射光线光谱偏离其“真实”颜色。当同一白色目标物体在被高色温光源照射（如阴天时）将会表现为蓝色，被低色温光源照射（如床头灯等）将会表现为姜黄色。白平衡方法能够根据图像所呈现的色温纠正图像的色彩偏差，其具体实现过程建立在朗伯特反射模型上。&#xA;朗伯特反射模型表示反射图像主要与三项因素有关：光源光谱分布、物体表面的反射率以及成像设备感光函数。针对彩色图像，即包含ＲＧＢ三个颜色通道的图像，图像呈现的场景中某物体表面上的空间坐标点x的颜色可用朗伯特反射模型表示为：&#xA;白平衡方法以上述朗伯特反射模型为基础，考虑到传统的以标准白色参照物进行颜色校准的白平衡方法在实际应用中具有很大的局限性，研宄人员提出了多种白平衡算法纠正图像的色彩偏差。白平衡方法应用场景一般为普通偏色情况，对于并不严重的偏色可以起到较好的恢复作用，但是水下偏色情况一般较为严重，因此需要继续研究可以对水下颜色衰退起到良好恢复作用的白平衡算法。&#xA;直方图均衡化方法 将图像的灰度直方图从较为集中的某灰度区间均匀拉伸至全部灰度范围内，用以扩大图像灰度值的分布范围，提升图像对比度并突出部分细节效果。&#xA;基于Retinex的方法 Retinex 理论是 20 世纪 70 年代最早由 Land提出的一种基于颜色恒常性的理论，该理论基于三个假设：（1）真实世界是无颜色的，人类所见到的颜色是光与物体相互作用的结果；（2）每一种颜色由红、绿、蓝三原色组成的；（3）三原色决定了每个单位区域的颜色。Retinex 算法可以对不同类型的图像进行自适应增强，比传统的单一的图像增强算法具有更好的自适应性，因为传统的增强算法只能增强图像的某一类特征，而Retinex 增强算法则可以在动态范围内压缩、细节增强和颜色校正等方面达到较好的平衡效果，因此，Retinex 理论得到了广泛的发展和应用。&#xA;基于暗通道先验的方法（去雾的方法） DCP是一种去雾的图像增强方法（恺明神提出的方法）。接下来会写一篇博客专门对这种方法进行分析。&#xA;对于水下图像增强而言，跟去雾有很多相似的特性。故此在去雾中的一些方法也适用于水下图像增强。&#xA;基于卷积神经网络的方法 基于卷积网络的图像去雾、去模糊、图像盲复原等任务跟水下图像增强有异曲同工之妙。但是关键点在于训练集的获取以及卷积模型的泛化能力上。&#xA;其实对于运算量和实时性而言。个人觉得卷积网络只要训练好了，后面的使用过程是非常方便的。但是对于水下如此复杂的退化环境，要训练出一种泛化能力足够强的网络才是难点所在。与此同时，需要有相应的数据集的定制，也是难点所在。&#xA;基于物理模型的图像复原方法 该类方法针对水下图像退化过程构建数学模型，通过该模型反演图像退化过程，获得理想状态下未经退化的图像，属于图像复原范畴。（两种方法感觉好像有点类似于图像复原中的成pair的训练和不成pair的训练）&#xA;水下图像复原技术是基于物理模型的方法，是指对水下图像退化过程搭建一个合理的数学模型估算出模型参数信息，了解整个图像的退化过程使水下图像恢复到退化前的状态。图像复原技术适用范围更广，但是往往需要场景先验信息或深度信息来实现图像复原。</description>
    </item>
    <item>
      <title>echart相关操作xAxis,yAxis,series,grid,(包括x轴样式，y轴样式，折现样式，网格样式，折现阴影，折线上方显示数据，x轴文字倾斜)</title>
      <link>https://anwangtanmi.github.io/posts/3f989873d9f3cf073957b58b7781fc61/</link>
      <pubDate>Thu, 16 May 2019 15:49:52 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/3f989873d9f3cf073957b58b7781fc61/</guid>
      <description>样式截图大概如下：&#xA;1. x，y轴相关操作：xAxis，yAxis&#xA;(1) x，y轴的颜色：&#xA;axisLine: { lineStyle: { color: &#39;#2898e5&#39;, }, }, (2) x，y轴文字颜色：&#xA;axisLabel: { show: true, textStyle: { color: &#39;#019bf8&#39; } } （3）x，y轴刻度颜色：&#xA;axisTick: { lineStyle: { color: &#39;#2898e5&#39; } } (4) x，y轴坐标文字太长显示不全：,倾斜rotate&#xA;axisLabel: { show: true, interval: 0, rotate: 20 }, （5）x ,y 轴网格线的颜色：&#xA;splitLine: { show: true, lineStyle: { color: [&#39;rgb(1,155,246,0.3)&#39;], //网格线 width: 1, } }, 2. 折现 的样式&#xA;（1） 折现的平滑度series：&#xA;symbol: &#39;circle&#39;, //实心点 symbolSize: 6, //实心点的大小 smooth: true, //折现平滑 （2）折现的颜色：</description>
    </item>
    <item>
      <title>LIME:低光照下图像增强c&#43;&#43;代码</title>
      <link>https://anwangtanmi.github.io/posts/e9efbf3987758131ae9bb6c5c7e5e9ed/</link>
      <pubDate>Fri, 19 Apr 2019 14:34:48 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/e9efbf3987758131ae9bb6c5c7e5e9ed/</guid>
      <description>非matlab验证代码，非加密p代码，这里提供C++版本，有助于项目开发，如vSLAM、车道线检测等 代码地址：https://github.com/zj611/LIME_Processing&#xA;对比效果:&#xA;声明：转载请引用本文链接</description>
    </item>
    <item>
      <title>为什么要将8位图像的0～255的灰度值归一化为0～1？</title>
      <link>https://anwangtanmi.github.io/posts/12e675e1b16efaedf1ed73c3ad4bf80e/</link>
      <pubDate>Sat, 16 Mar 2019 10:48:24 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/12e675e1b16efaedf1ed73c3ad4bf80e/</guid>
      <description>一、简介 图像归一化是计算机视觉、模式识别等领域广泛使用的一种技术。所谓图像归一化, 就是通过一系列变换, 将待处理的原始图像转换成相应的唯一标准形式(该标准形式图像对平移、旋转、缩放等仿射变换具有不变特性)。 近年来, 基于矩的图像归一化技术受到了人们的普遍关注, 其基本工作原理为: 首先利用图像中对仿射变换具有不变性的矩来确定变换函数的参数, 然后利用此参数确定的变换函数把原始图像变换为一个标准形式的图像(该图像与仿射变换无关)。一般说来, 基于矩的图像归一化过程包括 4 个步骤，即坐标中心化、x-shearing 归一化、缩放归一化和旋转归一化。&#xA;基本上归一化思想是：利用图像的不变矩寻找一组参数使其能够消除其他变换函数对图像变换的影响。也就是转换成唯一的标准形式以抵抗仿射变换。图像归一化使得图像可以抵抗几何变换的攻击，它能够找出图像中的那些不变量，从而得知这些图像原本就是一样的或者一个系列的。以下你要知道的：&#xA;1、归一化处理并没有改变图像的对比度&#xA;2、归一化处理很简单，假设原图像是8位灰度图像，那么读入的像素矩阵最大值为256，最小值为1，定义矩阵为I，J＝I／256，就是归一化的图像矩阵，就是说归一化之后所有的像素值都在［0，1］区间内。&#xA;二、什么是归一化 归一化就是通过一系列变换（即利用图像的不变矩寻找一组参数使其能够消除其他变换函数对图像变换的影响），将待处理的原始图像转换成相应的唯一标准形式(该标准形式图像对平移、旋转、缩放等仿射变换具有不变特性)。&#xA;基于矩的图像归一化技术基本工作原理为：首先利用图像中对仿射变换具有不变性的矩来确定变换函数的参数， 然后利用此参数确定的变换函数把原始图像变换为一个标准形式的图像(该图像与仿射变换无关)。 一般说来，基于矩的图像归一化过程包括4个步骤，即坐标中心化、x-shearing 归一化、缩放归一化和旋转归一化。&#xA;图像归一化使得图像可以抵抗几何变换的攻击，它能够找出图像中的那些不变量，从而得知这些图像原本就是一样的或者一个系列的。&#xA;三、为什么归一化 1、基本上归一化思想是利用图像的不变矩寻找一组参数使其能够消除其他变换函数对图像变换的影响。也就是转换成唯一的标准形式以抵抗仿射变换。图像归一化使得图像可以抵抗几何变换的攻击，它能够找出图像中的那些不变量，从而得知这些图像原本就是一样的或者一个系列的。&#xA;2、matlab里图像数据有时候必须是浮点型才能处理，而图像数据本身是0-255的UNIT型数据所以需要归一化，转换到0-1之间。&#xA;3、归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。目的是为了：&#xA;(1)、避免具有不同物理意义和量纲的输入变量不能平等使用&#xA;(2)、bp中常采用sigmoid函数作为转移函数，归一化能够防止净输入绝对值过大引起的神经元输出饱和现象&#xA;(3)、保证输出数据中数值小的不被吞食&#xA;3、神经网络中归一化的原因： 归一化是为了加快训练网络的收敛性，可以不进行归一化处理；&#xA;归一化的具体作用是归纳统一样本的统计分布性。归一化在0-1之间是统计的概率分布，归一化在-1–+1之间是统计的坐标分布。归一化有同一、统一和合一的意思。无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测的，归一化是同一在0-1之间的统计概率分布；当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。为了避免出现这种情况，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于0或与其均方差相比很小。&#xA;归一化是因为sigmoid函数的取值是0到1之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。所以这样做分类的问题时用[0.9 0.1 0.1]就要比用[1 0 0]要好。&#xA;但是归一化处理并不总是合适的，根据输出值的分布情况，标准化等其它统计变换方法有时可能更好。</description>
    </item>
    <item>
      <title>【整理】3dsMax烘焙纹理模糊</title>
      <link>https://anwangtanmi.github.io/posts/f8e23b8a8704c0c1475e368521e8445a/</link>
      <pubDate>Fri, 15 Mar 2019 11:47:05 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f8e23b8a8704c0c1475e368521e8445a/</guid>
      <description>【2013年4月19日】&#xA;【出现问题】&#xA;3dsMax烘焙贴图时，纹理模糊。&#xA;【问题原因】&#xA;UV分布不合理；渲染器设置问题。&#xA;【解决方案】&#xA;1、合理分面，合理展UV，烘焙UV通道中的面的大小比例应与实际一致。&#xA;2、Vary渲染器设置：&#xA;1）全局开关中，关闭默认灯光。&#xA;2）环境设置中，GI天光使用“天蓝色”。&#xA;3）间接照明中，打开折射、反射；一次反弹0.5，二次反弹0.3.&#xA;4）发光贴图，预设为高。&#xA;5）光缓存，细分为600.&#xA;6）系统，渲染区域细分，X为30。&#xA;【备注说明】&#xA;很久之前整理的了，有待实践考证。。。</description>
    </item>
    <item>
      <title>python实现图像模糊</title>
      <link>https://anwangtanmi.github.io/posts/b3d70cb0b1dee0569876696171408071/</link>
      <pubDate>Fri, 08 Mar 2019 20:35:59 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b3d70cb0b1dee0569876696171408071/</guid>
      <description>代码：高斯模糊&#xA;from PIL import Image, ImageFilter import numpy as np img = Image.open(&#34;./1.jpg&#34;).filter(ImageFilter.GaussianBlur) img.save(&#34;1_.jpg&#34;) 原图： 高斯模糊：</description>
    </item>
    <item>
      <title>【HISI系列】海思芯片驱动使用方法</title>
      <link>https://anwangtanmi.github.io/posts/e887b6e508f2e5d826e537251b6e5970/</link>
      <pubDate>Sat, 23 Feb 2019 13:56:56 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/e887b6e508f2e5d826e537251b6e5970/</guid>
      <description>DATE: 2019-2-22 前言 在调试不同海思芯片的编码器时，遇到了需要加载和卸载驱动的情况，并且调试过程中出现不同硬件层面和编码的错误，特将问题定位方法记录一下以供后续参考。&#xA;1、海思芯片驱动使用方法 设备SDK包中的ko文件夹中存放了海思硬件运行需要的不同模块驱动，设备正常运行需要加载相应的驱动程序才可以。&#xA;HISI芯片驱动加载和卸载方法：&#xA;以Hi3559AV100为例：&#xA;cd ko ./load3559av100 -a -sensor imx334 查看已经安装的驱动模块：&#xA;lsmod 注意事项：不同模块驱动之间存在依赖关系，卸载模块驱动时存在先后顺序。&#xA;模块KO之间的依赖关系：参考文档：《HiMPP 媒体处理软件 FAQ.pdf》&#xA;每个加载上去的KO模块，有显示依赖关系的，lsmod查看时，会有Used by的标识。存在这种关系的KO之间需要按照顺序加载和相反顺序卸载。 有些模块KO是隐形依赖的，比如公共基础KO模块mmz.ko、hi_media.ko等需要先加载，这些KO模块若中途单独卸载再加载可能引起一些异常。 2、调试和问题定位方法 在运行海思编码器demo时，编码报错一般是由于编码API使用不当造成的，比如参数超出合法范围，系统驱动没有加载等。&#xA;2.1、MPP API调试工具 问题定位方法：&#xA;1、首先根据错误码（如下图6.5所示）大致定位问题的原因和方向。&#xA;2、查看mpp log信息：&#xA;cat /dev/logmpp 注：mpp log信息中会具体指定错误类型以及错误的原因和位置。&#xA;调用HISI SDK API接口出现错误怎么办?&#xA;下面参考自：https://blog.csdn.net/listener51/article/details/87891633&#xA;【现象】&#xA;需要查看日志和调整 log 日志的等级。&#xA;【分析】&#xA;Log 日志记录 SDK 运行时错误的原因、大致位置以及一些系统运行状态等信息。因此可通过查看 log 日志，辅助错误定位。&#xA;目前日志分为 7 个等级，默认设置为等级 3 。等级设置的越高，表示记录到日志中的信息量就越多，当等级为 7 时，系统的整个运行状态实时的被记录到日志中，此时的信息量非常庞大，会大大降低系统的整体性能。因此，通常情况下，推荐设置为等级 3 ，因为此时只有发生错误的情况下，才会将信息记录到日志中，辅助定位绝大多数的错误。&#xA;【解决】&#xA;获取日志记录或修改日志等级时用到的命令如下：&#xA;查看各模块的日志等级，可以使用命令 cat /proc/umap/logmpp ，此命令会列出所有模块日志等级。 修改某个模块的日志等级，可使用命令 echo “venc=4” &amp;gt; /proc/umap/logmpp ，其中 venc 是模块名，与 cat 命令列出的模块名一致即可。</description>
    </item>
    <item>
      <title>图像去雾算法</title>
      <link>https://anwangtanmi.github.io/posts/4ceb83fe138f4266afdc2f3a85ea22cf/</link>
      <pubDate>Wed, 13 Feb 2019 21:49:20 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4ceb83fe138f4266afdc2f3a85ea22cf/</guid>
      <description>何凯明经典图像去雾算法：https://www.cnblogs.com/molakejin/p/5708883.html&#xA;暗通道&#xA;卷积神经网络网络去雾DehazeNet：https://blog.csdn.net/Julialove102123/article/details/80199276&#xA;大气散射模型&#xA;去雾总结：https://blog.csdn.net/u012556077/article/details/53364438&#xA;参考文献：&#xA;上：https://blog.csdn.net/baimafujinji/article/details/27206237&#xA;下：https://blog.csdn.net/baimafujinji/article/details/30060161&#xA;内容丰富：http://www.cnblogs.com/Imageshop/p/3281703.html&#xA;课件：http://www.cnblogs.com/changkaizhao/p/3266798.html&#xA;matlab代码：&#xA;https://blog.csdn.net/chongshangyunxiao321/article/details/51076800&#xA;https://blog.csdn.net/shenziheng1/article/details/56951002&#xA;https://www.cnblogs.com/pursuit1996/p/4912202.html</description>
    </item>
    <item>
      <title>去雾算法AODNet pytorch</title>
      <link>https://anwangtanmi.github.io/posts/510759135abfb4409951b192ca21c564/</link>
      <pubDate>Mon, 07 Jan 2019 16:28:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/510759135abfb4409951b192ca21c564/</guid>
      <description>网络结构 K(x)随I（x）变化，算法为了学习到自适应的K(x)&#xA;所以算法核心就是K（x）&#xA;五层网络极为简单：&#xA;code import torch import torch.nn as nn import math class dehaze_net(nn.Module): def __init__(self): super(dehaze_net, self).__init__() self.relu = nn.ReLU(inplace=True) ############ 每个卷积层只用三个核 ############## self.e_conv1 = nn.Conv2d(3,3,1,1,0,bias=True) self.e_conv2 = nn.Conv2d(3,3,3,1,1,bias=True) self.e_conv3 = nn.Conv2d(6,3,5,1,2,bias=True) ## 连接1、2层3+3=6，输出3 self.e_conv4 = nn.Conv2d(6,3,7,1,3,bias=True) ##连接2，3层3+3=6，输出3 self.e_conv5 = nn.Conv2d(12,3,3,1,1,bias=True) #连接1，2，3，4层3+3+3+3=12，输出3 def forward(self, x): source = [] source.append(x) ######### K-estimation ########### x1 = self.relu(self.e_conv1(x)) x2 = self.relu(self.e_conv2(x1)) concat1 = torch.cat((x1,x2), 1) x3 = self.relu(self.e_conv3(concat1)) concat2 = torch.</description>
    </item>
    <item>
      <title>图计算论文笔记–Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
      <link>https://anwangtanmi.github.io/posts/49bc9a8d61ec3c0ff4c561e768a3d3cf/</link>
      <pubDate>Thu, 06 Dec 2018 21:21:08 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/49bc9a8d61ec3c0ff4c561e768a3d3cf/</guid>
      <description>Graph Convolutional Neural Networks for Web-Scale Recommender Systems abstract introduction related work method problem setup model architecture Forward propagation algorithm Importance-based neighborhoods Stacking convolutions model training Multi-GPU training with large minibatches Producer-consumer minibatch construction. Sampling negative items Node Embeddings via MapReduce 总结 abstract 在大规模数据上使用GCN做数据挖掘&#xA;introduction a random-walk-based GCN–PinSage 处理 3 billion nodes and 18 billion edges的图 为了在大规模图上运行，使用了On-the-fly convolutions；Producer-consumer minibatch construction； Efficient MapReduce inference 同时，使用的减少网络的复杂度的方法：Constructing convolutions via random walks；Importance pooling 使用Curriculumtraining来学习 related work method GCN的方法：一个节点可以形成一个local network，对很多个节点的local network进行GCN，这样，GCN网络的权重被每个network共享。</description>
    </item>
    <item>
      <title>【图像处理】一种低光照图像的亮度提升方法（Adaptive Local Tone Mapping Based on Retinex for High Dynamic Range Images）</title>
      <link>https://anwangtanmi.github.io/posts/381d993c888c1ce5ce1dfd9b9f1c7536/</link>
      <pubDate>Thu, 15 Nov 2018 21:25:11 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/381d993c888c1ce5ce1dfd9b9f1c7536/</guid>
      <description>【fishing-pan：https://blog.csdn.net/u013921430 转载请注明出处】 前言 在实际的拍照过程中，常常会遇到，光线不足的情况。这时候单反用户一般会调大感光度，调大光圈，以让照片整体更清晰，更亮。那么如果照片已经被拍的很暗了，怎么办呢？这时候我们可以利用算法来提升图像整体的光照情况，让图像更清晰。&#xA;2013年这篇《Adaptive Local Tone Mapping Based on Retinex for High Dynamic Range Images》发表在了IEEE上，如题目所说，文章提到将高动态图像在低动态范围显示设备上进行显式时，会面临信息丢失的问题。因此结合传统的CENTER/SURROUND RETINEX 技术提出了全局自适应和局部自适应的HDR实现过程，对HDR image 进行色调映射。而文中的全局自适应方法对于低照度图像具有很好的照度提升效果。作者将他的Matlab脚本上传到了Github，有兴趣的可以点击这里去查看。&#xA;全局自适应原理 全局自适应方法的原理很简单，就是两个公式；&#xA;L&#xA;g&#xA;(&#xA;x&#xA;,&#xA;y&#xA;)&#xA;=&#xA;l&#xA;o&#xA;g&#xA;(&#xA;L&#xA;w&#xA;(&#xA;x&#xA;,&#xA;y&#xA;)&#xA;/&#xA;L&#xA;w&#xA;ˉ&#xA;+&#xA;1&#xA;)&#xA;l&#xA;o&#xA;g&#xA;(&#xA;L&#xA;w&#xA;m&#xA;a&#xA;x&#xA;/&#xA;L&#xA;w&#xA;ˉ&#xA;+&#xA;1&#xA;)&#xA;L_{g}(x,y)=\frac{log(L_{w}(x,y)/\bar{L_{w}}+1)}{log(L_{wmax}/\bar{L_{w}}+1)}&#xA;Lg​(x,y)=log(Lwmax​/Lw​ˉ​+1)log(Lw​(x,y)/Lw​ˉ​+1)​&#xA;上述式子中，&#xA;L&#xA;g</description>
    </item>
    <item>
      <title>计算机视觉 常用算法总结</title>
      <link>https://anwangtanmi.github.io/posts/48ca9f37bc783da7b627d53e1d325964/</link>
      <pubDate>Mon, 22 Oct 2018 18:19:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/48ca9f37bc783da7b627d53e1d325964/</guid>
      <description>计算机视觉算法在图像识别方面的一些难点：&#xA;1）视角变化：同一物体，摄像头可以从多个角度来展现；&#xA;2）大小变化：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是由变化的）；&#xA;3）形变：很多东西的形状并非一成不变，会有很大变化；&#xA;4）遮挡：目标物体可能被遮挡。有时候只有物体的一部分（可以小到几个像素）是可见的； 5）光照条件：在像素层面上，光照的影响非常大；&#xA;6）背景干扰：物体可能混入背景之中，使之难以被辨认；&#xA;7）类内差异：一类物体的个体之间的外形差异很大，如椅子。这一类物体有许多不同的对象，每个都有自己的外形&#xA;人脸识别算法主要包含三个模块：&#xA;人脸检测（Face Detection）：确定人脸在图像中的大小和位置，也就是在图像中预测anchor；&#xA;人脸对齐（Face Alignment）：它的原理是找到人脸的若干个关键点（基准点，如眼角，鼻尖，嘴角等），然后利用这些对应的关键点通过相似变换（Similarity Transform，旋转、缩放和平移）将人脸尽可能变换到标准人脸；&#xA;人脸特征表征（Feature Representation）：它接受的输入是标准化的人脸图像，通过特征建模得到向量化的人脸特征，最后通过分类器判别得到识别的结果。关键点是怎样得到不同人脸的有区分度的特征，比如：鼻子、嘴巴、眼睛等。&#xA;早期算法：&#xA;子空间(线性降维)&#xA;PCA(主成成分分析) ：尽量多地保留原始数据的保留主要信息，降低冗余信息；&#xA;LDA(线性判别分析)：增大类间差距，减小类内差距。&#xA;非线性降维： 流形学习、加入核函数。&#xA;ICA(独立成分分析)：比PCA效果好，比较依赖于训练测试场景，且对光照、人脸的表情、姿态敏感，泛化能力不足。&#xA;HMM(隐马尔科夫) : 和前面这些算法相比，它对光照变化、表情和姿态的变化更鲁棒。&#xA;早期：数据和模型结构；&#xA;后期：loss，从而得到不同人脸的有区分度的特征。&#xA;常用算法总结 计算机视觉中的相关算法的源代码&#xA;计算机视觉常用算法博客&#xA;特征提取（找到若干个关键点）&#xA;(1) SIFT （尺度不变特征变换） 具有尺度不变性，可在图像中检测出关键点。&#xA;(2) SURF（加速稳健特征，SIFT加速版）&#xA;核心：构建Hessian矩阵，判别当前点是否为比邻域更亮或更暗的点，由此来确定关键点的位置。&#xA;优：特征稳定；&#xA;缺：对于边缘光滑的目标提取能力较弱。&#xA;(3) ORB 结合Fast与Brief算法，并给Fast特征点增加了方向性，使得特征点具有旋转不变性，并提出了构造金字塔方法，解决尺度不变性. ORB算法的速度是sift的100倍，是surf的10倍。 经显示观察到，ORB算法在特征点标记时数量较少，如图:&#xA;SIFT、SURF、ORB实现&#xA;(4) FAST角点检测&#xA;FAST的方法主要是考虑像素点附近的圆形窗口上的16个像素&#xA;如果要提高检测速度的话，只需要检测四个点就可以了，首先比较第1和第9个像素，如果两个点像素强度都在中心像素强度t变化范围内（及都同中心点相似），则说明这不是角点，如果接下来检测第5和13点时，发现上述四点中至少有三个点同中心点不相似，则可以说明这是个角点。&#xA;非极大值抑制：如果存在多个关键点，则删除角响应度较小的特征点。&#xA;(5) HOG (方向梯度直方图)&#xA;(6) LBP(局部二值特征）论述了高维特征和验证性能存在着正相关的关系，即人脸维度越高，验证的准确度就越高。&#xA;(7)Haar&#xA;人脸识别相关论文阅读 ICCV2017开放论文&#xA;一文带你了解人脸识别算法演化史！</description>
    </item>
    <item>
      <title>深度学习AI美颜系列—人像审美</title>
      <link>https://anwangtanmi.github.io/posts/d884c7265b302e5cb66b9593cdbe4529/</link>
      <pubDate>Thu, 18 Oct 2018 14:18:08 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d884c7265b302e5cb66b9593cdbe4529/</guid>
      <description>美颜是个常见的话题，暗藏了一个基础性的问题，什么是美，也就是人像之审美。&#xA;中国人审美的标准：&#xA;1，面部轮廓——“三庭五眼”；&#xA;2，人脸正中垂直轴上——“四高三低”；&#xA;3，人脸正中水平轴上——“丰”；&#xA;4，五官精致&#xA;符合上述三个条件，即中国人眼中的美貌了。&#xA;下面我们来具体分析：&#xA;①三庭五眼&#xA;世界各国普遍认为“瓜子脸、鹅蛋脸”是最美的脸形，从标准脸形的美学标准来看，面部长度与宽度的比例为1.618∶1，也就是黄金分割比例。&#xA;我们中国人所谓的三庭五眼，是人的脸长与脸宽的一般标准比例，从额头顶端发际线到眉毛、从眉毛到鼻子、从鼻子到下巴，各占1/3，这就是“三庭”；脸的宽度以眼睛的宽度为测量标准，分成5个等份，这就是“五眼”，具体如下图所示：&#xA;②四高三低&#xA;所谓“四高”是指：&#xA;第一高点，额部；&#xA;第二个高点，鼻尖；&#xA;第三高点，唇珠；&#xA;第四高点，下巴尖；&#xA;所谓“三低”是指：&#xA;第一低，两个眼睛之间，鼻额交界处必须是凹陷的；&#xA;第二低，在唇珠的上方，人中沟是凹陷的，美女的人中沟都很深，人中脊明显；&#xA;第三低，在下嘴唇唇的下方，有一个小小的凹陷；&#xA;“四高三低”在头侧面相上最明确，如下图所示：&#xA;③“丰”字审美&#xA;“丰”是指人脸正中横轴上符合“丰”字审美准则。在人的面部上画上一个“丰”字，来判断美丑。先作面部的中轴线，再通过太阳穴(颞部)作一条水平线，通过两侧颧骨最高点作一条平行线，再通过口角到下颌角作一条平行线。形成一个“丰”字。在“丰”字的三横上面，颞部不能太凹陷，也不能太突起；如下图所示：&#xA;④五官精致&#xA;五官精致包含一下几个方面：&#xA;1、眼睛美：&#xA;双眼对称，眼窝深浅适中&#xA;2、鼻子美：&#xA;鼻根与双眼皮位置等高&#xA;3、耳廓美：&#xA;双耳对称，大小及形态相同&#xA;4、口唇美：&#xA;上唇下1/3部微向前翘&#xA;5、牙齿美：&#xA;静止状态时上前牙覆盖下前牙形1/3；正中上前牙与面形相同，牙齿整齐，洁白；微笑时露出双侧尖牙；&#xA;上面这些就是我们东方人所认为的貌美审美观，也是本人根据网络资源所做的一些总结。当然这只是一种大众认可，而并非所有人。对于真正的美女，不单单是这些内容，还要有形体与身材的审美等。&#xA;本文这里给大家普及的内容，主要是作为美颜算法研究的知识储备，所谓知己知彼，只有我们更了解什么是美，那么，我们的美颜算法才能更上一层楼！</description>
    </item>
    <item>
      <title>数字图像的噪声分类</title>
      <link>https://anwangtanmi.github.io/posts/c4ffc6a24cacbc0b0fb2812aab0e9716/</link>
      <pubDate>Wed, 19 Sep 2018 21:02:54 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c4ffc6a24cacbc0b0fb2812aab0e9716/</guid>
      <description> 对噪声数字图像处理之前，必须首先了解数字图像中噪声的来源，产生机理及噪声的数学模型。系统的分析了CCD相机成像过程中的噪声组成，指出数字图像中主要的噪声种类包括模式噪声、暗电流噪声、光子噪声、读出噪声、热噪声、以及量化噪声，以下对各个噪声做具体说明。 （1）模式噪声在数字图像成像过程中形成，相机传感器通过感知光子的强度和数量，转换为一定对应强度关系的电信号，在此过程中，受现代工艺水平的限制，还无法做到所有感光元件性能绝对一致统一，形成了图像获取过程中的噪声。&#xA;（2）暗电流噪声指在没有入射光照条件下，对MCP两端施加电压信号，通道中输出的反向电流，可看作背景白噪声「sod，暗电流噪声对工作温度和制造工艺敏感，在低温条件下可忽略不计。&#xA;（3）光子噪声指光子的离散性或粒子性所引起的噪声，即便光照功率恒定，每一时刻到达传感器的光子数量也是随机的，这种数量的变动造成了光子噪声。&#xA;（4）读出噪声包括读出电路中各种电子元气件所具有的固有噪声和电路设计中引入的噪声。&#xA;（5）热噪声存在于所有电子元器件及传输介质中，任何的放大电路都存在热噪声，减少热噪声最好的方法就是将电路至于极低的温度环境下，这在现实应用中是不可能实现的。&#xA;（6）量化噪声是由于数字图像是经过模数电路量化转换的，在采样过程中存在信息的损失和近似误差。&#xA;图像噪声降低了图像的视觉效果，影响和限制了后续其它图像处理算法的妙果，图像去噪己经成为图像处理领域中必不可少的一环。 </description>
    </item>
    <item>
      <title>LOj10131 暗的连锁</title>
      <link>https://anwangtanmi.github.io/posts/53962bd7c5dd3969d8b51fd0ad5ed08d/</link>
      <pubDate>Thu, 06 Sep 2018 20:38:43 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/53962bd7c5dd3969d8b51fd0ad5ed08d/</guid>
      <description>题目描述&#xA;原题来自：POJ 3417&#xA;Dark 是一张无向图，图中有 N 个节点和两类边，一类边被称为主要边，而另一类被称为附加边。Dark 有 N–1 条主要边，并且 Dark 的任意两个节点之间都存在一条只由主要边构成的路径。另外，Dark 还有 M条附加边。&#xA;你的任务是把 Dark 斩为不连通的两部分。一开始 Dark 的附加边都处于无敌状态，你只能选择一条主要边切断。一旦你切断了一条主要边，Dark 就会进入防御模式，主要边会变为无敌的而附加边可以被切断。但是你的能力只能再切断 Dark 的一条附加边。&#xA;现在你想要知道，一共有多少种方案可以击败 Dark。注意，就算你第一步切断主要边之后就已经把 Dark 斩为两截，你也需要切断一条附加边才算击败了 Dark。&#xA;输入格式&#xA;第一行包含两个整数 N 和 M；&#xA;之后 N–1 行，每行包括两个整数 A 和 B表示 A 和 B 之间有一条主要边；&#xA;之后 M 行以同样的格式给出附加边。&#xA;输出格式&#xA;输出一个整数表示答案。&#xA;主要边是原图的一棵生成树，附加边是图中的非树边。&#xA;对于每条非树边（x,y），它会和树上x到y的路径构成一个环，当第一步切断x到y路径中的一条边时，第二步就需要切断非树边（x,y），才能保证原图不联通。&#xA;但我们第二步只能切断一条边啊，所以如果第一步的路径对应的要切断两条或以上的非树边，就没有办法了。&#xA;所以我们枚举每条非树边（x,y），把x到y路径上所有边的边权+1，对于每条树边，如果边权为0，则切断它之后原图已经不联通，第二条边随便切一条，共m种方案。如果边权为1，则第二步必须切断对应的那条边，方案数1，如果边权大于2则没有方案。&#xA;这个边权我们可以利用树上差分来维护，对于非树边（x,y），令结点x和y的点权加1，lca（x,y）的点权减2，这样每个点子树的点权和就是该点到它的父亲的边的边权。证明就不详细讲了。&#xA;代码：&#xA;#include #include #include #include #include #define maxn 100010 #define maxm 500010 using namespace std; int head[maxn],f[maxn][30],dep[maxn],cnt,num[maxn],fr[maxm],t[maxm],ff[maxn]; struct edge { int next; int to; }e[maxm]; void insert(int u,int v) { e[++cnt].</description>
    </item>
    <item>
      <title>HDR技术介绍</title>
      <link>https://anwangtanmi.github.io/posts/d883c98ff8e1c2a5508f1742df8c2310/</link>
      <pubDate>Thu, 16 Aug 2018 22:34:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d883c98ff8e1c2a5508f1742df8c2310/</guid>
      <description>我们常说的HDR（High dynamic range）指的是HDRI（HDR image），如下图所示，是前处理的HDR技术。由于常用的显示器（手机、平板、电视）亮度不够而无法显示高动态范围信号，ISP合成的HDR数据需进行tone mapping后才能在8bit或10bit显示设备上显示。 因此从显示的角度来讲，由于亮处不够亮（1000nits以上），暗处不够暗（0.01nits以下），已经不属于HDR了。 真正意义上是HDR，HDR system指能将相机捕捉的动态范围从源端传到终端显示，并且没有动态范围衰减包括sensor输出，前处理，编码传输，解码显示，如下图所示，关键技术是transfer functions(OETF/EOTF)的设计和优化。 高动态、宽色域、高分辨率、高帧率、高位宽电视能呈现更真实的画面，使得高亮处更亮，暗处更暗，亮暗极限对比度更高，同时，能提供丰富的亮暗区域细节。 人眼可以感知很宽的动态范围，而一般的sensor很难做到，自然场景的的亮度范围如下图所示，典型范围在0-10000nits，人眼可以毫不费劲的同时观察亮暗场景，而sensor在一次曝光中很难做到，当然，现在也有些相机可以做到，比如SONY F65，单次曝光，输出16bit线性RAW数据。 对于一般的Camera，可通过调整曝光参数，光圈大小，来捕捉不同亮度范围的图像，如下图所示。 更高的亮度，可以呈现更多的色彩，因此对于高动态数据，需要使用更宽的色域（颜色体积），比如2020色域，如下图所示，目前支持HDR功能的电视，大多使用2020色域，许多影院的设备能支持DCI P3色域，也有些手机、笔记本可支持到DCI P3，比如Galaxy S8，iMac pro。 BT.709 35.9% of CIE 1931 可显示CIE 1931色度图中的35.9% DCI P3 53.6% BT.2020 75.8% 支持HDR功能的显示设备，往往能够显示高位宽、广色域的数据，目前主要有两种HDR标准：HDR10、HLG，由于tone mapping技术的多样性，诞生了多种HDR标准，常见的显示HDR技术： 1. HDR10 2015年8月开始正式采用，该标准使用2020色域、10bit位宽、ST2084(PQ曲线)作为传递函数。 2. HLG Hybrid Log-Gamma标准由英国BBC和日本NHK共同提出，该标准采用HLG曲线作为传递函数，曲线低端与Gamma曲线相同，因此，在显示要求不是很高的场合，可以兼容现有的SDR电视，这也是该标准提出来的初衷。 3. Dolby vision 该标准由Dolby实验室提供，使用ST2084曲线作为传递函数，10bit/12bit位宽，2020色域，相比于HDR10的static metadata，Dovi使用的是Dynamic metadata，会根据场景信息对显示效果进行优化，现已被众多电视和机顶盒产品商用，需要说明的是，使用Dovi技术的产品需要Dolby授权，每台电视收取不超过3$的授权费。 4. HDR10+ 该标准由SAMSUNG和AMAZON共同提取，使用PQ曲线编码，HDR10+完全兼容HDR10，只不过使用Dynamic metadata，tone mapping曲线为Bezier曲线。 该标准是一个免版税的开放标准。 5. SL-HDR1 由STM、Philips和Technicolor共同提出，使用Dynamic metadata，兼容SDR电视&#xA;参考资料： [1] https://en.wikipedia.org/wiki/High-dynamic-range_video [2] https://en.wikipedia.org/wiki/Gamut [3] Recommendation ITUt R BT.2100t 0 (07/2016), “Image parameter values for high dynamic range television for use in production and international programme exchange” [4] https://en.</description>
    </item>
    <item>
      <title>图像处理经典文章合集</title>
      <link>https://anwangtanmi.github.io/posts/c70b5960b081501c04e522da1536c2b2/</link>
      <pubDate>Sun, 05 Aug 2018 14:09:57 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c70b5960b081501c04e522da1536c2b2/</guid>
      <description>Colorization and Color Transfer（图像上色和颜色迁移）&#xA;Semantic Colorization with Internet Images, Chia et al. SIGGRAPH ASIA 2011 Color Harmonization, Cohen-Or, Sorkine, Gal, Leyvand, and Xu. Web Page Computing the alpha-Channel with Probabilistic Segmentation for Image Colorization, Dalmau-Cedeno, Rivera, and Mayorga Bayesian Color Constancy Revisited, Gehler, Rother, Blake, Minka, and Sharp Color2Gray: Salience-Preserving Color Removal, Gooch, Olsen, Tumblin, and Gooch Color Conceptualization, Hou and Zhang Light Mixture Estimation for Spatially Varying White Balance, Hsu, Mertens, Paris, Avidan, and Durand.</description>
    </item>
    <item>
      <title>ArcGIS教程之DEM(高程)的应用（坡度坡向、提等高线）</title>
      <link>https://anwangtanmi.github.io/posts/6fd7fb1c321a43a3f4b114f6b212e3d2/</link>
      <pubDate>Fri, 03 Aug 2018 16:56:19 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/6fd7fb1c321a43a3f4b114f6b212e3d2/</guid>
      <description>ArcGIS教程之DEM(高程)的应用（坡度坡向、提等高线） 发布时间：2018-01-17 版权：&#xA;相关教程：&#xA;DEM水文分析（一）&#xA;Arcgis下DEM水文分析（二）&#xA;DEM的应用包括：坡度：Slope、坡向：Aspect、提取等高线、算地形表面的阴影图、可视性分析、地形剖面、水文分析等，其中涉及的知识点有：&#xA;a)掌握根据DEM 计算坡度、坡向的方法。&#xA;b)理解基于DEM数据进行水文分析的基本原理。&#xA;c)利用ArcGIS的提供的水文分析工具进行水文分析的基本方法和步骤。&#xA;第一步：需要的工具&#xA;1. BIGEMPA地图下载器（全能版已授权） 下载地址：http://download.bigemap.com/bmsetup.rar&#xA;2. Global Mapper 14. 下载地址：Global Mapper 14.1汉化版.rar&#xA;3. ARCGIS下载地址：http://www.bigemap.com/helps/doc2018011754.html&#xA;第二步骤：通过BIGEMAP下载高程数据&#xA;1. 启动BIGEMAP地图下载器软件，查看左上角是否显示【已授权：所有地图】，如果没有该显示，请联系我们的客服人员。如下图所示：&#xA;2. 选择左上角属性选项，选择【高程】，如下图：&#xA;3. 选在你要的区域，双击下载，如下图所示：&#xA;4. 选择下载的级别，建议尽量下载16级的，16级为最好级别。如果16级不能勾选，请选择下载小一点的范围，高程为矢量数据，超过20M大小，一般电脑都很难处理生成的等高线。下载之后的数据为tiff格式，实际为dem高程数据。&#xA;6. 启动安装好的Global Mapper软件，启动中文版在安装好的目录下有个chs或则chinese的启动图标，如下图所示：&#xA;7. 将下载好的高程数据（下载目录下的后缀为tiff格式）拖入到global mapper中，如下图所示：&#xA;修改下载的高程数据的投影为【UTM】&#xA;在Global mapper中选择：工具-&amp;gt;设置，弹出对话框，如下：&#xA;然后另存为【DEM】格式，如下图：&#xA;在弹出的对话框中选择【DEM】格式，如下图:&#xA;点击【确定】，保存。&#xA;DEM应用之坡度：Slope&#xA;打开ArcMap软件，选择添加按钮，将刚才保存的DEM文件打开，如下图：&#xA;在ArcMap中，需要打开【扩展模块】功能，如下图：&#xA;弹出的对话框中勾选所有选项，如下图：&#xA;点击【关闭】。&#xA;2) 在【ArcToolbox】中，执行命令[3D Analyst工具]——[栅格表面]——[坡度]， 如下图所示，指定各参数：&#xA;执行后，得到坡度栅格Slope_tingri1：坡度栅格中，栅格单元的值在[0 -82] 度间变化，如下图：&#xA;右键点击图层[Slope_TinGrid]，执行[属性命令]，设置图层[符号系统]，重新调整坡度分级。将类别调整为5，点[分类]按钮，用手动分级法，将中断值调整为：8，15，25，35，90。&#xA;确定后，如下：&#xA;【下面计算剖面曲率】&#xA;在【ArcToolbox】中，执行命令[3D Analyst工具]——[栅格表面]————[坡度]。按如下所示，指定各参数。得到剖面曲率栅格：[Slope _Slope]，如下图：&#xA;DEM应用之坡向：Aspect&#xA;打开【ArcToolbox】，执行命令[3D Analyst工具]——[栅格表面]——[坡向]，按下图所示，指定各参数：&#xA;执行结果为（得到坡向栅格：[Aspect_tgrid]）：&#xA;【以下计算平面曲率】：&#xA;在【ArcToolbox】中，执行命令[3D Analyst工具]——&amp;gt;[栅格表面]——&amp;gt;[坡度]，按下图所示，指定各参数，按下图所示指定各参数：</description>
    </item>
    <item>
      <title>ISP（图像信号处理）之——坏点校正</title>
      <link>https://anwangtanmi.github.io/posts/ef9974e7dbeb606c7a87c9f4b2584ed4/</link>
      <pubDate>Mon, 30 Jul 2018 17:40:11 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ef9974e7dbeb606c7a87c9f4b2584ed4/</guid>
      <description>什么是坏点 坏点的定义：在数码相机上，坏点指无法正确捕捉光线的感光组件单元，若感光元件出现坏点，会直接造成成像的暇疵。LCD的坏点包括亮点、暗点与色点三类，感光元件与LCD相似，也存在这三类坏点。&#xA;引起坏点的原因：&#xA;（1）工艺 ： （a）在sensor 的制作过程中有灰尘等引起。&#xA;（b）电子产品的寿命影响有限，导致会随着使用时间增加而引起坏点。&#xA;（2）noise ：（a）sensor gain 增大&#xA;（b）温度增高等&#xA;坏点分为静态坏点和动态坏点：&#xA;静态坏点： 不会随着时间、增益等改变，从sensor 制造时因为工艺等产生的坏点。&#xA;动态坏点： 因为增益、温度等引起的坏点，会随着时间变化而改变。&#xA;类型：hot pixel、dead pixel、weak pixel&#xA;hot pixel ： 比周围点亮很多的坏点 。&#xA;dead pixel： 比周围点暗很多的坏点。&#xA;weak pixel：没有提供一个正确的像素值，但是并没有比周围点特别亮或者特别暗的像素。&#xA;坏点和噪点的区别 图像的噪点是由扫描仪或数码相机的感光元件和图像处理电路产生图像时造成的亮度或颜色信息的随机变化。&#xA;以下是噪点与坏点的区别对比：&#xA;噪点　坏点&#xA;位置　随机　固定&#xA;颜色　随机　暗点和亮点固定，色点不固定&#xA;亮度　随机　暗点和亮点固定，色点不固定&#xA;受温度影响　是　否&#xA;受感光度影响 非常明显 亮点和色点受轻微影响&#xA;受曝光时间影响　明显　亮点和色点受轻微影响&#xA;坏点校正过程 识别坏点 ISP支持高中低端不同的sensor，不同级别的sensor特点不同。&#xA;高端sensor分辨率高，在制造出厂时，已经通过测试知道了坏点的位置。&#xA;低端sensor分辨率低，而且一般会有大量的坏点（defects），而且在出厂时，一般没有测试坏点位置，需要通过ISP来设别这些坏点。&#xA;另一个问题是，对于热噪点（hot pixel），持续时间长，密度甚至可达5%，对于高分辨率的sensor来说，坏点个数就非常多。而且，这种坏点是实时变化的。&#xA;识别方法：&#xA;在RGB Bayer域上做5×5的评估，取在评估窗内偏离度超过阈值的点为坏点。一个典型的简单方法：&#xA;avg=sum（*）&#xA;dif=（次max点 – 次min点）</description>
    </item>
    <item>
      <title>卷积网络中卷积和池化之后，图像的尺寸变化</title>
      <link>https://anwangtanmi.github.io/posts/25f66edca57c5f2726dfcb81497f2430/</link>
      <pubDate>Tue, 17 Jul 2018 11:07:38 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/25f66edca57c5f2726dfcb81497f2430/</guid>
      <description>在卷积和和池化的过程中存在着两种对图像的处理形式：&#xA;在这个例子中： 输入跨度为13 滤波器宽度为6 步幅为5&#xA;“VALID”：只会丢掉最右边的列（或最底部的行） “SAME”：尝试向左或右均匀填充，但如果添加的列数是奇数，它将向右添加偶数，向左侧添加奇数个列（向下添加偶数个列，向上添加奇数个列）&#xA;在 tensorflow 中，tf.nn.conv2d函数和tf.nn.max_pool函数，尺寸变化： 对于SAME填充，输出高度和宽度计算如下： out_height = ceil（float（in_height）/ float（strides [1]））&#xA;out_width = ceil（float（in_width）/ float（strides [2]））&#xA;和&#xA;对于VALID填充，输出高度和宽度计算如下： out_height = ceil( float(in_height – filter_height + 1）/ float(strides[1]))&#xA;out_width = ceil( float(in_width – filter_width + 1）/ float(strides [2])）</description>
    </item>
    <item>
      <title>基于快速去雾的图像亮度增强方法</title>
      <link>https://anwangtanmi.github.io/posts/43e486859a19f3b09441eda33da6c506/</link>
      <pubDate>Sat, 09 Jun 2018 23:08:30 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/43e486859a19f3b09441eda33da6c506/</guid>
      <description>起因 最近在做一个图像处理的算法，因为图像太暗，所以需要对图像的亮度进行增强(不考虑噪声的放大)。尝试了网上搜索到的各种方法后，发现它们存在两个问题：容易造成原本较亮的地方过曝，并且参数不好设置。尝试了一些暗光增强的paper的算法后，发现它们又太慢了。这时我想到曾经看过的一篇paper说过，有一种亮度增强的算法是基于去雾来做的，步骤很简单： 1. 将RGB图像取反(关于图像取反，请参考我的这篇博客OpenCV图像取反)； 2. 对取反后的图像进行去雾； 3. 将去雾后的图像取反。&#xA;其背后的理念是：暗光图像取反后，原本接近黑色的像素就会变成接近白色，整张图像就会类似于有雾的图像。于是对这样的图像进行去雾后，白色的像素就变暗了，再反色后，像素就变亮了！没毛病！&#xA;快速去雾算法 说起去雾，恐怕大多数人的第一反应就是鼎鼎大名的何恺明博士的暗通道先验去雾算法。我的第一反应也是这个。但是在了解之后，我发现这种方法速度太慢了，难以应用到我的算法中。于是我开始搜索快速的去雾算法，很快找到了这篇论文《基于单幅图像的快速去雾算法》刘倩, 陈茂银, 周东华(这篇文章中也提到了暗通道先验去雾算法的速度太慢)，速度很快，只有O(1)复杂度。&#xA;这篇论文我并没有仔细看，而是秉承“拿来主义”的精神，直接根据论文提供的算法流程实现了代码。原因是它的算法流程太简单了，在轻松地实现了代码之后，就没有再看的欲望了~这里贴一下它的算法流程，你们自己看： 源码解释 我把实现的代码放到了GitHub：IBEABFHR(原谅我这个取名废)，请点进去看效果图，我这里就不重复放了。我感觉效果还是很好的，暗处的亮度增强得很好，亮处虽然有过曝，但并不是很明显。而且控制亮度的参数很好调整，只要随便找一张图像调整好参数，就可以应用于所有图片了。&#xA;运行速度 代码是用OpenCV实现的，同时支持彩色图像和灰度图像。在我的电脑上(CPU: E3-1230 v3)测试，运行100次取平均值，速度如下：&#xA;分辨率 类型 时间 1024×768 灰度图像 8.77ms 1024×768 彩色图像 16.24ms 1920×1080 灰度图像 22.61ms 1920×1080 彩色图像 40.60ms 4160×2340 灰度图像 104.57ms 4160×2340 彩色图像 186.14ms 但是如果你只使用我的算法一次，可能速度要慢得多，原因是第一次取反操作因为未知原因耗费了额外的时间。关于这点，请参看我的这篇博客OpenCV图像取反。&#xA;参数调整 这个算法总共有两个可变参数。一个是在step 3中进行均值滤波时的所用的滤波半径radius，另一个是在step 5中用的ρ。&#xA;radius参数在某些图像上可以控制对比度，数值越大，对比度越强，但在某些图像上不起作用。这个参数取值不能太小，否则增强后的图像会出现光晕。一般不应小于50或者图像宽度和高度最大值的的1/20。&#xA;ρ控制图像增强的亮度，数值越大，增强后的图像越亮。一般的取值范围为[1.0, 2.0]。在我的实现中，我使用了一种简单选择的策略，请参看源代码，仅供参考。&#xA;关于参数的效果及设置，我参考了这篇博客一种可实时处理 O(1)复杂度图像去雾算法的实现，在此进行感谢。</description>
    </item>
    <item>
      <title>图像处理入门：基本概念</title>
      <link>https://anwangtanmi.github.io/posts/496f19816ba402ed89928350c64420dd/</link>
      <pubDate>Fri, 18 May 2018 15:25:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/496f19816ba402ed89928350c64420dd/</guid>
      <description>数字图像：每一个数字图像都是一个像素点矩阵，这个矩阵包含所有像素点的强度值&#xA;像素点：最小的图像单元，一张图像由好多的像素点组成。像素就是图像的尺寸&#xA;位图：也称点阵图，它是由许多点组成的，这些点称为像素。当许多不同颜色的点组合在一起后，便构成了一副完整的图像。 位图可以记录每一个点的数据信息，从而精确地制作色彩和色调变化丰富的图像。但是，由于位图图像与分辨率有关，它所包含的图像像素数目是一定的，若将图像放大到一定程度后，图像就会失真，边缘出现锯齿。&#xA;灰度：表示图像像素明暗程度的数值，也就是黑白图像中点的颜色深度，范围一般从0到255，白色为255 ，黑色为0。灰度值指的是单个像素点的亮度，灰度值越大表示越亮。灰度级表明图像中不同灰度的最大数量灰度级越大，图像的亮度范围越大。&#xA;通道：把图像分解成一个或多个颜色成分；①单通道：一个像素点只需一个数值表示，只能表示灰度，0为黑色； ②三通道：RGB模式，把图像分为红绿蓝三个通道，可以表示彩色，全0表示黑色；③四通道：在RGB基础上加上alpha通道，表示透明度，alpha=0表示全透明&#xA;深度：深度即位数（比特数）①位深：一个像素点所占的总位数，也叫像素深度、图像深度等，其中位深 = 通道数 × 每个通道所占位数 ②256色图：n位的像素点可以表示2^n种颜色，称2^n色图，n=8时为256色图 ③8位RGB与8位图：前者的位数指每个通道所占的位数，后者指整个像素点共占的位数，其中8位RGB是一个24位图，也称为真彩&#xA;对比度：指不同颜色之间的差别。对比度越大，不同颜色之间的反差越大，即所谓黑白分明，对比度过大，图像就会显得很刺眼。对比度越小，不同颜色之间的反差就越小。对比度=最大灰度值/最小灰度值&#xA;亮度：指照射在景物或图像上光线的明暗程度。图像亮度增加时，就会显得耀眼或刺眼，亮度越小时，图像就会显得灰暗。&#xA;色相：颜色，调整色相就是调整景物的颜色，例如，彩虹由红、橙、黄、绿、青、蓝、紫七色组成，那么它就有七种色相。顾名思义即各类色彩的相貌称谓，如大红、普蓝、柠檬黄等。色相是色彩的首要特征，是区别各种不同色彩的最准确的标准。事实上任何黑白灰以外的颜色都有色相的属性，而色相也就是由原色、间色和复色来构成的&#xA;色调：各种图像色彩模式下原色的明暗程度，级别范围从0到255，共256级色调。例如对灰度图像，当色调级别为255时，就是白色，当级别为0时，就是黑色，中间是各种程度不同的灰色。在RGB模式中，色调代表红、绿、蓝三种原色的明暗程度，对绿色就有淡绿、浅绿、深绿等不同的色调。&#xA;色调是指色彩外观的基本倾向。在明度、纯度、色相这三个要素中，某种因素起主导作有用，可以称之为某种色调&#xA;饱和度：指图像颜色的浓度。饱和度越高，颜色越饱满，即所谓的青翠欲滴的感觉。饱和度越低，颜色就会显得越陈旧、惨淡，饱和度为0时，图像就为灰度图像。可以通过调整电视机的饱和度来进一步理解饱和度的概念。&#xA;频率：灰度值变化剧烈程度的指标，是灰度在平面空间上的梯度。低频就是颜色缓慢地变化,也就是灰度缓慢地变化,就代表着那是连续渐变的一块区域,这部分就是低频。 高频就是频率变化快，即相邻区域之间灰度相差很大,这就是变化得快。图像中,一个影像与背景的边缘部位的频率高，即高频显示图像边缘。图像的细节处也是属于灰度值急剧变化的区域，正是因为灰度值的急剧变化，才会出现细节。另外噪声（即噪点）也是这样,在一个像素所在的位置,之所以是噪点,就是因为它与正常的点颜色不一样了，灰度有了快速地变化。固有“图像的低频是轮廓，高频是噪声和细节”。&#xA;空域：也叫空间域，即所说的像素域，在空域的处理就是在像素级的处理，如在像素级的图像叠加。通过傅立叶变换后，得到的是图像的频谱。表示图像的能量梯度。&#xA;频域： 也叫频率域，任何一个波形都可以分解成多个正弦波之和。每个正弦波都有自己的频率和振幅。所以任意一个波形信号有自己的频率和振幅的集合。频率域就是空间域经过傅立叶变换的信号&#xA;图像分辨率：每英寸图像内的像素点数。分辨率越高，像素的点密度越高，图像越逼真。&#xA;空间分辨率：图像中可辨别的最小细节的度量，如果一幅图像的尺寸为MxN，表明在成像时采集了MxN个样本，空间分辨率是MxN。&#xA;灰度分辨率：在灰度级中可分辨的最小变化，在数字图像处理教程中，灰度分辨率指的是色阶，色阶是表示图像亮度强弱的指数标准，也就是我们说的色彩指数。灰度分辨率指亮度，和颜色无关，但最亮的只有白色，最不亮的只有黑色。&#xA;颜色空间(颜色模型)：描述颜色的三维空间坐标系，一个颜色定义为颜色空间的一个点。&#xA;1.灰度模式：“灰度”模式可以表现出丰富的色调，但是也只能表现黑白图像。“灰度”模式图像中的像素是由8位的分辨率来记录的，能够表现出256种色调，从而使黑白图像表现的更完美。灰度模式的图像只有明暗值，没有色相和饱和度这两种颜色信息。其中，0%为黑色，100%为白色，K值是用来衡量黑色油墨用量的。使用黑白和灰度扫描仪产生的图像常以灰度模式显示。&#xA;2.位图模式：“位图”模式的图像又叫黑白图像，它用黑、白两种颜色值来表示图像中的像素。其中的每个像素都是用1 bit的位分辨率来记录色彩信息的，占用的存储空间较小，因此它要求的磁盘空间最少。位图模式只能制作出黑、白颜色对比强烈的图像。如果需要将一副彩色图像转换成黑白颜色的图像，必须先将其转换成“灰度”模式的图像，然后再转换成黑白模式的图像，即“位图”模式的图像。&#xA;在OpenCV中，彩色图像使用OpenCV加载时是BGR模式。但是在Matplotlib中是RGB模式，所以彩色图像如果已经被OpenCV读取，那他将不会被Matplotlib正确显示。&#xA;3.HSV颜色模型：也称六角锥体模型(Hexcone Model)。、这个模型中颜色的参数分别是：色调（H），饱和度（S），亮度（V）。在HSV颜色空间中要比在BGR空间中更容易表示一个特定颜色。&#xA;H参数表示色彩信息，即所处的光谱颜色的位置。该参数用一角度量来表示，红、绿、蓝分别纯度S为一比例值，范围从0到1，它表示成所选颜色的纯度和该颜色最大的纯度之间的比率。S=0时，只有灰度。相隔120度。互补色分别相差180度。&#xA;V表示色彩的明亮程度，范围从0到1。有一点要注意：它和光强度之间并没有直接的联系。（以下的HSI模型应该就是HSV模型）&#xA;图像描述：&#xA;距离度量：描述图像中像素之间的距离。&#xA;欧几里得距离：距离点(i,j)小于或者等于某一值是以点(i,j)为原点的圆&#xA;城市街区(小区)距离：距离点(i,j)小于或者等于某一值是以点(i,j)为中心的菱形&#xA;棋盘距离距离：;距离点(i,j)小于或者等于某一值是以点(i,j)为中心的正方形</description>
    </item>
    <item>
      <title>暗通道去雾法-对算法的理解Dark Channel Prior</title>
      <link>https://anwangtanmi.github.io/posts/2bcfabec08b36b225a4e3e3e69c7554b/</link>
      <pubDate>Wed, 18 Apr 2018 17:42:01 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2bcfabec08b36b225a4e3e3e69c7554b/</guid>
      <description>暗通道最早是由He提出的，并且也发展得很快，具体的在这里不说，这篇文章，只是谈一下暗通道去雾的实现过程&#xA;1.该方法最原始的模型是I=Jt+A(1-t)，这个模型在我的其他文章中有详细介绍，这里不详细讲解。&#xA;可以看出带有雾的图像的I是由没有散射的原图像J和大气光(air light)共同组成的。这里刚好看到t，1-t和为1，实则不然，只是凑巧这样，原作者有详细的推导，这个是推导而来并非为了凑和为1。&#xA;2.看了上面的模型进一步会想，要是我们把t变为1，不就I=J，完事了么……理论上是这样的，不过一般给你的图像都是拍好的，你要想t=1，那你得找没有雾的图，没有雾的图你去啥雾。在这里我们不是改变t的值，而是对t进行求解。&#xA;3.为了求t，He老大的暗通道出现了，他发现一个规律，在没有雾的图像中，图像的RGB通道，总会有一个通道的值偏低。该区域的最小值是一个很小的值。颜色鲜艳和阴影都会表现出暗通道极小的值。下图是选了三通道最小值后又做了个最小值滤波。该公式就可以表示暗通道的性质了。c为通道，Ω局部窗范围（最小滤波）。这里还有一点可以看出，天空那不是黑的。在He.2009年的论文中也说明了，在带有天空的图像中，暗通道方法并不是很适用，后期应该是他已经改进了，想要了解的可以去查一下。&#xA;4.J=0，你有没有看出点什么。我们未知参数就只有一个t了。&#xA;通过上面的公式我们的t就求出来了，t出来了我们真实的J就可以求解出来了。&#xA;5.这里说明几个问题&#xA;①求出的t咋用？这里说明一点，在图像中，不同点的t是不同的，因此每个区域都有一个t，把t的矩阵带入到最上面即可。&#xA;②大气光A怎么求？首先从暗通道图像中按照亮度的大小提取最亮的前0.1%像素。然后，在原始有雾图像II中寻找对应位置上的具有最高亮度的点的值，并以此作为AA的值。&#xA;③三个通道怎么处理呢？一个一个处理，t是一样的，只是不同的Ic和Ac……（c：R、G、B）&#xA;④网上提到了soft matting和导向图是用来干啥的？咱们用最小滤波出来的暗通道会是一块一块的，那么咱们的t估计也好不到哪去…………为了得到更好地t，我们就要优化我们的暗通道图。He最初用soft matting……网上的评论一致认为很慢，我没试，也就不说了。导向图是用来替代soft matting的，也是听他们说很快。&#xA;图片来自大神博客：http://www.cnblogs.com/Imageshop/p/3281703.html</description>
    </item>
    <item>
      <title>ISP算法：镜头阴影矫正</title>
      <link>https://anwangtanmi.github.io/posts/22adf50441f02010fb7315e0f0bd73ff/</link>
      <pubDate>Thu, 01 Feb 2018 13:49:20 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/22adf50441f02010fb7315e0f0bd73ff/</guid>
      <description>镜头阴影矫正（Lens Shading Correction）是为了解决由于lens的光学特性，由于镜头对于光学折射不均匀导致的镜头周围出现阴影的情况。&#xA;lens shading分为两种luma shading(亮度阴影)和color shading(色彩偏差)。&#xA;luma shading：由于Lens的光学特性，Sensor影像区的边缘区域接收的光强比中心小，所造成的中心和四角亮度不一致的现象。镜头本身就是一个凸透镜，由于凸透镜原理，中心的感光必然比周边多。如图所示：&#xA;color shading： 由于各种颜色的波长不同，经过了透镜的折射，折射的角度也不一样，因此会造成color shading的现象，这也是为什么太阳光经过三棱镜可以呈现彩虹的效果。如图所示： 矫正方法：&#xA;任何Shading都可以通过当前像素乘以增益gain的方式进行矫正。比较常见的两种思路：&#xA;&amp;lt;1&amp;gt; 半径矫正（radial shading correct）&#xA;&amp;lt;2&amp;gt; 网格矫正（mesh shading correct）&#xA;这两种矫正的gain都是基于中心像素灰度值和边缘像素灰度值以及矫正像素位置到中心像素位置的距离共同决定。&#xA;参考资料：&#xA;https://www.mm-sol.com/products/computational-camera/automatic-lens-shading-correction.html&#xA;http://blog.csdn.net/u013531497/article/details/39082415&#xA;https://wenku.baidu.com/view/2b20c21ffad6195f312ba6df.html&#xA;http://blog.csdn.net/xiaoyouck/article/details/77206505&#xA;http://blog.csdn.net/qq377801394/article/details/72472034</description>
    </item>
    <item>
      <title>欢you度yuan元旦赛（18.1.1）</title>
      <link>https://anwangtanmi.github.io/posts/a29191aecd74d40d0ca8cc766e1314e9/</link>
      <pubDate>Mon, 01 Jan 2018 12:49:35 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a29191aecd74d40d0ca8cc766e1314e9/</guid>
      <description>T1 最优图像 【题目描述】 小E在好友小W的家中发现一幅神奇的图画，对此颇有兴趣。它可以被看做一个包含N×M个像素的黑白图像，为了方便起见，我们用0表示白色像素，1表示黑色像素。小E认为这幅图画暗藏玄机，因此他记录下了这幅图像中每行、每列的黑色像素数量，以回去慢慢研究其中的奥妙。 有一天，小W不慎将图画打湿，原本的图像已经很难分辨。他十分着急，于是找来小E，希望共同还原这幅图画。根据打湿后的图画，他们无法确定真正的图像，然而可以推测出每个像素原本是黑色像素的概率Pij%。那么，一个完整的图像的出现概率就可以定义为 ，其中Sij表示在还原后的图像中，像素是白色(0)还是黑色(1)。换句话说，一个完整图像出现概率就等于其所有黑色像素的出现概率之积。显然，图像的黑色像素不能包含概率为0的像素。 然而，小E对此也无能为力。因此他们找到了会编程的小F，也就是你，请你根据以上信息，告诉他们最有可能是原始图像的答案是什么。&#xA;【输入文件】 输入文件image.in的第一行是两个正整数N和M，表示图像大小。 接下来N行每行包含M个整数，表示每个像素是黑色像素的概率为Pij%。0 ≤ Pij &amp;lt; 100。 接下来一行有N个非负整数，表示每一行中黑色像素的个数。 接下来一行有M个非负整数，表示每一列中黑色像素的个数。&#xA;【输出文件】 输出文件image.out包含一个N×M的01矩阵，表示你还原出的图像。输出不包含空格。图像每行、每列中1的个数必须与输入一致，且是所有可能的图像中出现概率最大的一个。输入数据保证至少存在一个可能的图像。如果有多种最优图像，任意输出一种即可。&#xA;【样例输入】 2 2 90 10 20 80 1 1 1 1&#xA;【样例输出】 10 01&#xA;【样例解释】 共有两种可能的图像： 01 10 和 10 01 前者的出现概率是0.1×0.2=0.02，后者的出现概率是0.9×0.8=0.72，故后者是最优图像。&#xA;【数据规模和约定】 对于20%的数据，N , M ≤ 5 对于100%的数据，N , M ≤ 100&#xA;分析： 首先看到这道题，感觉又像贪心，又像dp&#xA;我们就应该有一种直觉：网络流 建图方式显而易见 但是每一种图的贡献是一种连乘的形式 而网络流好像只能计算相乘的形式，怎么办呢&#xA;中午吃饭的时候，dp表示：可以取一个lg，这样就可以把乘变加了 觉得非常有道理&#xA;建图：用1…n表示行，n+1…n+m表示列，&#xA;若Pij&amp;gt;0，则连一条边(i,n+j)，费用为-lg(Pij)*100000，容量为1 然后从s向1…n连边，费用为0，容量为这一行的黑色像素数量 然后从n+1..n+m向t连边，费用为0，容量为这一列的黑色像素数量 求这个图的最小费用最大流&#xA;写完测了一下，只能过掉20%的数据 听说需要用ZKW网络流优化 （本文中不再冗述，重点在于解题思路）&#xA;tip 注意输出的时候没有空格（mdzz，太不认真了。。。）</description>
    </item>
    <item>
      <title>感知哈希算法——Python实现</title>
      <link>https://anwangtanmi.github.io/posts/0edd9ab4cff38adbf55b156c1f73e835/</link>
      <pubDate>Sun, 24 Dec 2017 20:54:30 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0edd9ab4cff38adbf55b156c1f73e835/</guid>
      <description>1. 前言 现在手中只有一张图像需要在一个集合中去找到与之最相近的那一张，这个过程实际是一个匹配的过程，特别是在多模态医学图像中解决这样的问题是比较迫切的，今年试验了一种广泛使用的算法——感知哈希算法！具体的实验结果将在下文中给出。&#xA;2. 算法原理 step1：缩小图片尺寸 将图片缩小到8×8的尺寸, 总共64个像素. 这一步的作用是去除各种图片尺寸和图片比例的差异, 只保留结构、明暗等基本信息。 step2：转为灰度图片 将缩小后的图片, 转为64级灰度图片。 step3：计算灰度平均值 计算图片中所有像素的灰度平均值 step4：比较像素的灰度 将每个像素的灰度与平均值进行比较, 如果大于或等于平均值记为1, 小于平均值记为0。 step5：计算哈希值 将上一步的比较结果, 组合在一起, 就构成了一个64位的二进制整数, 这就是这张图片的指纹。 step6：对比图片指纹 得到图片的指纹后, 就可以对比不同的图片的指纹, 计算出64位中有多少位是不一样的. 如果不相同的数据位数不超过5, 就说明两张图片很相似, 如果大于10, 说明它们是两张不同的图片。&#xA;3. Python实现 # -*- coding=utf-8 -*- import numpy as np from PIL import Image import matplotlib.pyplot as plt # extract feature # lines: src_img path def Extra_Featrue(lines, new_rows=64, new_cols=64): for name in lines: ori_img = Image.open(name.strip()) feature_img = ori_img.</description>
    </item>
    <item>
      <title>CNN结构：色彩特征提取-色彩属性HSV空间（色彩冷暖初始）</title>
      <link>https://anwangtanmi.github.io/posts/24aab89c8916bb82bd4aa4805d65f21c/</link>
      <pubDate>Thu, 23 Nov 2017 17:49:13 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/24aab89c8916bb82bd4aa4805d65f21c/</guid>
      <description>看完颜色的物理和数学描述基础，再来分析颜色的哲学基础，颜色的人文语义属性。颜色的基本三属性为色相、明度和纯度。&#xA;来自于百科：色彩是通过眼、脑和我们的生活经验所产生的一种对光的视觉效应。人对颜色的感觉不仅仅由光的物理性质所决定，比如人类对颜色的感觉往往受到周围颜色的影响。有时人们也将物质产生不同颜色的物理特性直接称为颜色。&#xA;人眼对色彩的感知一般来源于来自于光源的直射色和物体表面的反射色。&#xA;基础理论，查看百度经验：色彩基础知识 。下面文章摘抄了一小部分。&#xA;参考：如何通俗地解释色彩三要素：色相、明度、纯度？ HSV(Hue, Saturation, Value)是根据颜色的直观特性由A. R. Smith在1978年创建的一种颜色空间, 也称六角锥体模型(Hexcone Model)。色彩的基本属性离开RGB色彩空间，重新定义了色彩的HSV空间，依然表示 单个色彩的唯一性，但有可能仍然没有包含提夫尼蓝。参考：色彩的HSV表示模型，Hue, Saturation, Value。HSV空间和RGB空间是双射映射。&#xA;RGB为生成色，CMYK为反射色，RGB和CMY颜色模型都是面向硬件的，而HSV（Hue Saturation Value）颜色模型是面向用户的。&#xA;色彩基本属性 色相-hue&#xA;色相是指色彩的相貌,是色彩最显著的特征,是不同波长的色彩被感觉的结果。光谱上的红、橙、黄、绿、青、蓝、紫就是七种不同的基本色相。&#xA;色相变化&#xA;参考：色相环&#xA;明度-value&#xA;明度是指色彩的明暗、深浅程度的差别,它取决于反射光的强弱。它包括两个含义:一是指一种颜色本身的明与暗,二是指不同色相之间存在着明与暗的差别。&#xA;颜色有深浅、明暗的变化。比如，深黄、中黄、淡黄、柠檬黄等黄颜色在明度上就不一样，紫红、深红、玫瑰红、大红、朱红、桔红等红颜色在亮度上也不尽相同。这些颜色在明暗、深浅上的不同变化，也就是色彩的又一重要特征一一明度变化。&#xA;色彩的明度变化有许多种情况，一是不同&#xA;色相之间的明度变化。如：在未调配过得原色黄色明度最高、黄比橙亮、橙比红亮、红比紫亮、紫比黑亮；二是在某种颜色中，加白色明度就会逐渐提高，加黑色明度就会变暗，但同时它们的&#xA;纯度(颜色的&#xA;饱和度)就会降低，三是相同的颜色，因光线照射的强弱不同也会产生不同的明暗变化。 纯度-saturation&#xA;也称彩度、艳度、浓度、饱和度,是指色彩的纯净程度。&#xA;其他色彩名词&#xA;相关关系及影响&#xA;0.三要素环&#xA;1. 明度对比类型 为了细致的研究色彩明暗对比，可以把黑、灰、白划分为11个色阶。靠近白的3阶为高调色，靠近黑的三阶为低调色，中间三阶为中调色。&#xA;高调：具有柔软、轻快、纯洁、淡雅之感；&#xA;中调：具有柔和、含蓄、稳重、明确之感；&#xA;低调：具有朴素、浑厚、沉重、压抑之感。&#xA;2. 纯度对比类型.p76&#xA;低纯度基调1-3：给人以平淡、消极、无力、陈旧的感觉，同时也能给人自然、简朴、柔和、超俗、宁静的感受。&#xA;中纯度基调4-6：能够传达中庸、文雅、安详的感觉。&#xA;高纯度基调7-9：由高纯色组成的基调，由鲜艳、冲动、热烈、活波的视觉感受，给人以积极、强烈而冲动的感觉，如图3-37所示，如运用不当也会产生残暴、恐怖、低俗、刺激的感觉。&#xA;图片：高纯度的红色照片，联想一下贞子&#xA;降低纯度的方法：加白（产生色相偏差）、加黑（失去光亮感，感觉变得沉着冷静）、加灰（变得柔和柔软）。&#xA;3.空间基底映射 RGB空间和HSV空间是双射的，转换矩阵如下所示。&#xA;引用地址：http://wenku.baidu.com/view/3ccc1ec58bd63186bcebbc0c.html###&#xA;金属光泽&#xA;金属光泽是光泽强度的等级之一。一般指反射率R&amp;gt;0.25者，宛如金属抛光后所产生的光泽。同时金属光泽也是矿物光泽的一种。一些硫化物和氧化物矿物。如黄铁矿、方铅矿、镜铁矿等就具有金属光泽。金属光泽矿物均属不透明矿物，很少用作宝石。&#xA;[1]&#xA;金属光泽是指如同金属&#xA;抛光后的表面所反射的光泽，如同平滑的金属光洁表面所呈现的光泽，反光极强，同&#xA;非金属光泽、&#xA;半金属光泽并列。 特别之处： 金属光泽在RGB空间可以在三通道均匀分布，转化到HSV空间，会产生奇特的特征，表示为金属光泽物体只有V值，其H值和S值均为0. 其他属性&#xA;冷暖对比&#xA;由于色彩的冷暖差别而形成的色彩对比,称为冷暖对比。红、橙、黄使人感觉温暖;蓝、蓝绿、蓝紫使人感觉寒冷;绿与紫介于期间,另外,色彩的冷暖对比还受明度与纯度的影响,白光反射高而感觉冷,黑色吸收率高而感觉暖。&#xA;在色相环上，红橙黄为暖色，其中橙色称为“暖极”，绿青蓝为冷色，其中天蓝色被称为“冷极”。在色重上，暖色偏重，冷色偏轻。在湿度上，冷色湿润。纯度越高，冷暖感越强。纯度越低，冷暖感也随之降低。&#xA;补色对比&#xA;将红与绿、黄与紫、蓝与橙等具有补色关系的色彩彼此并置,使色彩感觉更为鲜明,纯度增加,称为补色对比。&#xA;色调 色调指得是一幅画中画面色彩的总体倾向,是大的色彩效果。在大自然中,我们经常见到这样一种现象:不同颜色的物体或被笼罩在一片金色的阳光之中,或被笼罩在一片轻纱薄雾似的、淡蓝色的月色之中;或被秋天迷人的黄金色所笼罩;或被统一在冬季银白色的世界之中。这种在不同颜色的物体上,笼罩着某一种色彩,使不同颜色的物体都带有同一色彩倾向,这样的色彩现象就是色调。色调是画面色彩构成的总体效果。</description>
    </item>
    <item>
      <title>echarts 设置背景颜色</title>
      <link>https://anwangtanmi.github.io/posts/aa9c298c17880b7a322265846c3d971a/</link>
      <pubDate>Wed, 08 Nov 2017 15:11:17 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/aa9c298c17880b7a322265846c3d971a/</guid>
      <description>问题：echarts 设置背景颜色描述:&#xA;@Kener-林峰 你好，想跟你请教个问题：请问怎么设置echarts整个图标的背景颜色？？我用了color属性，在官网实验的时候不显示。。。。&#xA;解决方案1:&#xA;/*#################开始 数据源处理器情况#################################*/&#xA;option= {&#xA;backgroundColor: ‘#1b1b1b’,//背景色&#xA;title : {&#xA;text: ‘动态数据’,&#xA;subtext: ‘纯属虚构’&#xA;},&#xA;tooltip : {&#xA;trigger: ‘axis’&#xA;解决方案2:&#xA;背景颜色就是dom自己的颜色，图表背景本身是透明的，可以给都没加css控制整体背景&#xA;以上介绍了“echarts 设置背景颜色”的问题解答，希望对有需要的网友有所帮助。本文网址链接：http://www.codes51.com/itwd/1523298.html</description>
    </item>
    <item>
      <title>MatLab2016b破解版安装教程</title>
      <link>https://anwangtanmi.github.io/posts/8e8a05c9c82f54f8152734fc6857d58c/</link>
      <pubDate>Sat, 21 Oct 2017 11:09:44 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/8e8a05c9c82f54f8152734fc6857d58c/</guid>
      <description>学习本专业时，出现了一个相对于物理来说的其他的不同学科，感觉还是可以学一下的，每次学不同的知识，下载软件是必不可少的，这里是从百度上下载的软件，仅供参考； 链接： http://pan.baidu.com/s/1jHVwa5S 密码：d1kn&#xA;1、 下载之后出现三个文件：&#xA;R2016b_win64_dvd1.iso（安装文件);&#xA;R2016b_win64_dvd2.iso（安装文件）； Matlab2016b Win64 Crack.rar（破解文件）。&#xA;2、 下面进行破解【其实只要利用好以上的文件就可以了】：&#xA;3、 解压安装文件，安装文件为iso格式，但是不能通过虚拟光驱安装，需要将iso文件用解压软件解压。注意，R2016b_win64_dvd1.iso和R2016b_win64_dvd2.iso两个文件均需要解压，且必须解压到同一个文件夹，如图所示：&#xA;4、 点击setup.exe，进行安装。如图所示&#xA;5、点击setup.exe，之后稍等一会儿，MathWorks安装程序启动，启动后选择“使用文件安装密钥 不需要Internet连接”，如图所示：&#xA;6、点击下一步，是否接受许可协议的条款哪儿选择“是”，如图所示&#xA;7、点击下一步，选择“我已有我的许可证的文件安装密钥”，在下面输入秘钥：“09806-07443-53955-64350-21751-41297”，如图所示&#xA;8、选择要安装的目录。&#xA;9、选择需要安装的产品，建议全选，也可根据自己需要选择要安装的产品，我是全选的，选择之后点击“下一步”。如图所示：&#xA;10、进入“确认”页面，点击“安装”。&#xA;11，进入安装界面，有进度条显示安装进度，安装过程需要较长时间。&#xA;12， 安装完成之后，进入“产品配置说明”界面，点击“下一步”继续。&#xA;13，之后进入“安装完毕”界面，点击“完成”按钮，完成安装。&#xA;14，将下载的“Matlab 2016a Win64Crack.rar”解压，解压后文件如图所示，我见文件解压在桌面上，文件夹中的“license_standalone.lic”就是许可文件，如图所示：&#xA;15，之后打开MATLAB的安装目录D:\Program Files (x86)\bin\win64点击“activate_matlab.exe”进行激活，选择“在不选择Internet情况下手动激活”，点击“下一步”继续。如图所示：&#xA;16，选择“输入许可证文件的完成路径（包括文件名）”，点击“浏览”，选择刚才解压到桌面的Matlab 2016a Win64 Crack文件夹中的许可文件，点击选择。&#xA;17，之后进入“离线激活”界面，现在的许可证文件的路径已经填写完整，点击“下一步”。&#xA;18，之后提示“激活完成”，如图所示。此时不要打开软件。继续看下一步。&#xA;19，打开解压的Matlab 2016a Win64Crack文件夹中的MATLAB Production Server\R2016a\bin\win64，复制其中的四个dll文件，如图所示：&#xA;20，之后打开D:\ProgramFiles\MATLAB\R2016b\bin\win64文件，将复制的dll文件粘贴在这个文件夹中，系统会提示“复制文件”选项，所有文件都选择“复制和替换”即可&#xA;21，之后打开D:\ProgramFiles\MATLAB\R2016b\bin文件夹下的MATLAB启动图标，点击右键发送桌面快捷方式，就可以正常使用了。&#xA;22，之后打开软件，就拥有一个完全免费的matlab2016a中文破解版本了</description>
    </item>
    <item>
      <title>unity 仿ue4湖水效果</title>
      <link>https://anwangtanmi.github.io/posts/96cd22d3061f64b327c98aadf3771613/</link>
      <pubDate>Sat, 23 Sep 2017 12:09:16 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/96cd22d3061f64b327c98aadf3771613/</guid>
      <description>这次是做个测试，看看材质通用性&#xA;为了方便使用的是untiy 5.6 + shaderforge 1.38&#xA;下面是最后的效果（当然比起ue4里面还添加了一些，用的线性空间和deferred模式）&#xA;下面是ue4原本的&#xA;因为主要是法线上的效果，unity在法线和光照效果上还是差挺多的&#xA;连节点中还发现深度信息和ue4略有不同这个要注意&#xA;而且在测试中还发现了shaderforge的bug&#xA;当然depth相关节点最终连接到normal节点的话会报下面的错&#xA;Shader error in ‘Shader Forge/test’: undeclared identifier ‘partZ’ at line 106 (on d3d11)&#xA;但是只需要改源码，把partZ声明放在使用之前就可以了&#xA;下面是第一个版本的shaderforge节点&#xA;基本和ue4那个很像，需要注意的是某些节点虽然和ue4很像，但是略有不同&#xA;预览是错误，因为需要改源码，但每次改过后，shaderforge打开又会开回来&#xA;这个bug其实挺多见的，节点连多了，各种bug就出现了&#xA;效果如下&#xA;但是这个缺少放入物体虚化的效果，于是又改了下&#xA;效果就是第一幅图的了，注意这个需要开启透明，使用的时候只能是不透明的物体放入水中，否则会有深度问题&#xA;节点如下&#xA;预览也是这个问题，shader文件里直接改&#xA;放上改好的shader下载地址（注意只有shader文件）&#xA;http://download.csdn.net/download/shenmifangke/9991699</description>
    </item>
    <item>
      <title>图像处理实例–图像去噪</title>
      <link>https://anwangtanmi.github.io/posts/95551a049563d236267bd8a60be3c416/</link>
      <pubDate>Thu, 24 Aug 2017 16:12:58 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/95551a049563d236267bd8a60be3c416/</guid>
      <description>常见的噪声种类 数字图像在获取、传输的过程中都可能会受到噪声的污染，常见的噪声主要有高斯噪声和椒盐噪声。其中，高斯噪声主要是由摄像机传感器元器件内部产生的，椒盐噪声主要是由图像切割所产生的黑白相间的亮暗点噪声，“椒”表示黑色噪声，“盐”表示白色噪声。&#xA;f=imread(&#39;3.jpg&#39;); g=imnoise(f,&#39;salt &amp;amp; pepper&#39;,0.06); h=imnoise(f,&#39;gaussian&#39;,0.05,0.05); subplot(1, 3, 1), imshow(f), title(&#39;原图&#39;); subplot(1, 3, 2), imshow(g), title(&#39;椒盐&#39;); subplot(1, 3, 3), imshow(h), title(&#39;高斯&#39;); 去噪方法 空域 空域图像去噪用的有均值滤波算法和中值滤波算法，主要是对图像像素做邻域的运算来达到去噪结果。&#xA;g1 = imfilter(g, fspecial(&#39;average&#39;)); g2 = medfilt2(g, [5 5]); subplot(2, 2, 3), imshow(g1), title(&#39;均值滤波&#39;); subplot(2, 2, 4), imshow(g2), title(&#39;中值滤波&#39;); 形态学去噪 数学形态学图像处理通过采用具有一定形态的结构元素去度量和提取图像中的对应形状，借助于集合理论来达到对图像进行分析和识别的目标，该算法具有以下特征。&#xA;1、图像信息的保持&#xA;在图像形态学处理中，可以通过已有目标的几何特征信息来选择基于形态学的形态滤波器，这样在进行处理时既可以有效地进行滤波，又可以保持图像中的原有信息。&#xA;2、图像边缘的提取&#xA;基于数学形态学的理论进行处理，可以在一定程度上避免噪声的干扰，相对于微分算子的技术而言具有较高的稳定性。形态学技术提取的边缘也比较光滑，更能体现细节信息。&#xA;3、图像骨架的提取&#xA;基于数学形态学进行骨架提取，可以充分利用集合运算的优点，避免出现大量的断点，骨架也较为连续。&#xA;4、图像处理的效率&#xA;基于数学形态学进行图像处理，可以方便地应用并行处理的技术来进行集合运算，具有效率高、易于硬件实现的特点。&#xA;权重自适应的多结构形态学去噪 在数学形态学图像去噪的过程中，通过适当地选取结构元素的形状和维数可以提高滤波去噪的效果。在多结构元素的级联过程中，需要考虑到结构元素的形状和维数。数字图像在进行数学形态滤波去噪时，根据噪声特点可以尝试采用维数由小到大的结构元素来进行处理，进而达到滤除不同噪声的目的。采用数学形态学的多结构元素，可以更多地保持数字图像的几何特征。因此，选择构建串联滤波器来进行图像滤波，就是将同一形状的结构元素按维数从小到大的顺序来对图像进行滤波。&#xA;&amp;gt;&amp;gt; i=imread(&#39;food.jpg&#39;); &amp;gt;&amp;gt; i=rgb2gray(i); &amp;gt;&amp;gt; %添加噪声 &amp;gt;&amp;gt; ig=imnoise(i,&#39;poisson&#39;); &amp;gt;&amp;gt; %获取算子 &amp;gt;&amp;gt; s=GetStrelList(); &amp;gt;&amp;gt; %串联去噪 &amp;gt;&amp;gt; edit erodeList &amp;gt;&amp;gt; e=erodeList(ig,s); &amp;gt;&amp;gt; edit getRateList &amp;gt;&amp;gt; %计算权重 &amp;gt;&amp;gt; f=getRateList(ig,e); &amp;gt;&amp;gt; edit getRemoveResult &amp;gt;&amp;gt; %并联 &amp;gt;&amp;gt; igo=getRemoveResult(f,e); &amp;gt;&amp;gt; %显示结果 &amp;gt;&amp;gt; subplot(1,2,1),imshow(f); &amp;gt;&amp;gt; subplot(1,2,1),imshow(i); &amp;gt;&amp;gt; title(&#39;原图像&#39;); &amp;gt;&amp;gt; subplot(1,2,2),imshow(ig),title(&#39;噪声图像&#39;); &amp;gt;&amp;gt; figure; &amp;gt;&amp;gt; subplot(2,2,1),imshow(e.</description>
    </item>
    <item>
      <title>【3dsmax】物体冻结与半透明显示</title>
      <link>https://anwangtanmi.github.io/posts/02526030b8d6481a786729ffc6355770/</link>
      <pubDate>Sun, 30 Jul 2017 20:36:11 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/02526030b8d6481a786729ffc6355770/</guid>
      <description>如果photoshop的图层一样，在平时的3dsmax设计的时候，会遭遇到物体繁多的情况，类似于photoshop需要隐藏图层一样，3dsmax也需要冻结物体，让我们的设计和编程更加高效。右键点击要隐藏的物体，然后选择Object Properties，然后点选Freeze，并且去掉勾选Show Frozen in Gray。冻结物体并且让其正常显示。虽然右键菜单直接就有Freeze Selection选项，但这样会使物体变灰。&#xA;同时要想到，如果发现物体无法选择和移动，考虑到物体是否被冻结。&#xA;如果不想冻结物体，还可以用Alt+X来变透明显示物体，从而更好地看出物体之间的交错关系。这样只是改变在编辑时候的显示效果，渲染的时候不受影响。</description>
    </item>
    <item>
      <title>图像处理基础</title>
      <link>https://anwangtanmi.github.io/posts/c996afbc9a065f4a93aa452442d7eb8a/</link>
      <pubDate>Wed, 26 Jul 2017 22:19:35 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c996afbc9a065f4a93aa452442d7eb8a/</guid>
      <description>第一章 数字图像基本知识 1、 彩色图像、灰度图像、二值图像和索引图像区别？ 答：（1）彩色图像，每个像素通常是由红（R）、绿（G）、蓝（B）三个分量来表示的，分量介于（0，255）。M、N分别表示图像的行列数，三个M x N的二维矩阵分别表示各个像素的R、G、B三个颜色分量。RGB图像的数据类型一般为8位无符号整形，通常用于表示和存放真彩色图像，当然也可以存放灰度图像。 （2）灰度图像（gray image）是每个像素只有一个采样颜色的图像，这类图像通常显示为从最暗黑色到最亮的白色的灰度。灰度图像与黑白图像不同，在计算机图像领域中黑白图像只有黑色与白色两种颜色；用于显示的灰度图像通常用每个采样像素8位的非线性尺度来保存，这样可以有256级灰度（如果用16位，则有65536级）。&#xA;（3）二值图像（binary image），即一幅二值图像的二维矩阵仅由0、1两个值构成，“0”代表黑色，“1”代白色。由于每一像素（矩阵中每一元素）取值仅有0、1两种可能，计算机存储的二值化图像用0和255来表示。二值图像通常用于文字、线条图的扫描识别（OCR）和掩膜图像的存储。&#xA;（4）索引图像是为了减少RGB真彩色存储容量而提出的，它的实际像素点和灰度图一样用二维数组存储，只不过灰度值的意义在于表示颜色表索引位置；而颜色表是指颜色索引矩阵MAP，MAP的大小由存放图像的矩阵元素值域决定，如矩阵元素值域为[0，255]，则MAP矩阵的大小为256×3，MAP中每一行的三个元素分别指定该行对应颜色的红、绿、蓝单色值。&#xA;如某一像素的灰度值为64，则该像素的颜色值就是MAP中的第64行的RGB组合。也就是说，图像在屏幕上显示时，每一像素的颜色由存放在矩阵中该像素的灰度值作为索引通过检索颜色索引矩阵MAP得到。索引图像一般用于存放色彩要求比较简单的图像，如Windows中色彩构成比较简单的壁纸多采用索引图像存放，如果图像的色彩比较复杂，就要用到RGB真彩色图像。&#xA;2、奈奎斯特采样定理 答：奈奎斯特采样定理解释了采样率和所测信号频率之间的关系，即采样率fs必须大于被测信号最高频率分量fmax的两倍，fmax频率通常被称为奈奎斯特频率，公式：&#xA;fs&amp;gt;2*fmax;&#xA;至于奈奎斯特采样定理成立的原因，可见下图：&#xA;第二章 图像增强 1、图像增强包括哪些？ 答：（1）图像增强主要分为空间域增强方法和频域增强方法。&#xA;空间域就是指图像本身，频域指图像经过傅里叶变换的信号；&#xA;（2）空间域图像增强操作很多：&#xA;灰度变换：如二值化、图像反转（255-grayValue）、对数变换（增大像素的灰度值，尤其对源图的暗像素效果明显，参见对数曲线）、反对数变换（减小像素灰度值，尤其对亮像素效果明显，参见反对数曲线）、幂次变换（又叫伽马校正，其可以增大或减少像素灰度值，具有变换程度与指数γ大小有关，参见幂函数曲线）、分段线性函数变换（包括对比拉伸、灰度切割、位图切割）；&#xA;直方图处理（直方图均衡化、直方图局部增强）；&#xA;算数逻辑操作增强（图像减法处理、图像平均处理）；&#xA;平滑滤波、线性滤波、统计排序滤波、均值滤波、中值滤波等滤波；&#xA;锐化处理（拉普拉斯算子锐化、梯度法锐化）；&#xA;等等，都是空间域图像增强方法。&#xA;2、灰度直方图 答：灰度直方图是横坐标为灰度级、纵坐标为像素个数的直方图，用于表示每个灰度范围内的像素个数；&#xA;归一化灰度直方图：将灰度直方图的纵坐标值除以像素总数，产生的新直方图即是。&#xA;3、直方图均衡化与应用 答：（1）直方图均衡化：就是通过变换函数，使得图像的灰度分布较为均匀，将灰度值集中的部分均匀分散到整个灰度范围，使得直方图的各个灰度级y轴较为平坦。从而实现图像增强，如较暗的图片变得较为明亮，过亮的图片变的正常，从而利于观察识别。&#xA;（2）均衡化的变换函数：就是一个映射函数，必须满足两个条件：1）一个单值单增函数；2）映射后灰度范围不变。实际中常用累积分布函数。累积分布函数如下定义：&#xA;①先求当前灰度级的累计概率（即当前灰度级以及小于当前灰度级的像素个数和在图像中的比例）：&#xA;Sk是当前像素值的累计概率，k是当前像素的像素值，n是图像中像素个数的总和，nj是当像素值等于j的像素个数，L是图像中可能的灰度级总数。&#xA;②求当前像素的映射像素值：&#xA;映射函数g=最大灰度值*Sk；（例如最大灰度值为255，则g=255*Sk）&#xA;（3）缺点： 1）变换后图像的灰度级减少，某些细节消失； 2）某些图像，如直方图有高峰，经处理后对比度不自然的过分增强。 （4）应用：改善光线对图像处理的影响。成像中由于光照过大或过小，会造成图像结果偏暗或偏亮或者光线不均匀，这样图像直方图就会表现：灰度的两个高峰分别向某一边滑动，同时灰度值都较为集中，不能真实反应目标图像的特征。所以使用直方图均衡化可以减少这种影响。&#xA;（5）直方图均衡化步骤：&#xA;①找到一个映射函数，定义为g = EQ (f)。f完整表示是f(x,y)，代表图像中某个位置的像素值；&#xA;②依次使用EQ将图像中每个位置的像素值映射为新的像素值。&#xA;4、直方图匹配（或直方图规定化） 答：直方图匹配是指将一个图像的直方图变换到指定的形状（直方图均衡化是变换到均匀分布），是一种图像增强技术；&#xA;5、直方图局部增强 答：前述直方图处理都是直接对整幅图像求直方图，然后针对直方图处理。而直方图局部增强则是对图像的每个指定大小区域分别求直方图，然后针对每个直方图进行均衡化或者规定化处理。&#xA;例如，用一个k*k的矩阵从图像左上角滑动，每滑动一次计算一次该矩阵范围内图像的直方图，进行相关直方图处理。直到整幅图都被滑过，就实现了局部增强操作。&#xA;作用：可以实现对图像细节的增强。&#xA;6、算术/逻辑操作增强 答：主要包括与、或、加、减法操作。&#xA;图像相减法：应用最为成功的是医学领域的掩模式X射线成像术，另外图像相减法在图像分割中也有应用；&#xA;7、空间滤波基础概念 答：（1）掩模：在滤波器中常提到的k*k的矩阵（一般k为奇数），用于依次滑过每一像素点并在每点进行滤波计算，矩阵中的数据成为掩模系数。&#xA;滤波器在每个像素点的滤波结果就是掩模与掩模覆盖下的图像进行计算得到的结果；如线性滤波器就是掩模系数与覆盖下的对应图像像素点进行乘法操作，最后求和得到的结果。&#xA;（2）邻域处理：上述线性滤波器就属于邻域处理滤波器，因为其将本像素点为中心的邻域像素都纳入计算中了；实际非线性滤波器也是邻域处理方式，如常见的非线性滤波器：中值滤波器。&#xA;（3）如何解决图像边缘掩模滤波问题：掩模移动都是以矩阵中心作为基准点的，那么对于图像边缘，掩模矩阵就会有一部分超出图像范围。一般解决方法有：&#xA;1）限制掩模移动范围，使得掩模始终在图像范围内。缺点是边缘部分像素得不到滤波处理；&#xA;2）使用灰度值0或者边缘灰度值扩充边缘，滤波后删除。缺点是影响靠近边缘像素的滤波结果；&#xA;3）掩模超出部分不参与滤波计算；&#xA;三种方式中1）是最佳选择。&#xA;8、平滑线性滤波器 答：常见的线性滤波器有：均值、加权均值滤波器等。</description>
    </item>
    <item>
      <title>lens shading correction</title>
      <link>https://anwangtanmi.github.io/posts/b3437a21372fefa2bf0b1f6e8fbe3d8c/</link>
      <pubDate>Wed, 31 May 2017 21:44:09 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/b3437a21372fefa2bf0b1f6e8fbe3d8c/</guid>
      <description>lens shading分为两种luma shading(亮度阴影)和color shading(色彩偏差)。 luma shading产生的原因是镜头的通光量从中心到边角依次较少，导致图像看起来，中间亮度正常，四周偏暗。 采用网格矫正的方法： 1、获取矫正系数 拍一张均匀亮度的图像，将图像分成n*m个格子，每个格子的四个点都有一个校正系数，将n*m个校正系数存入表中。 2、计算待矫正像素点所在网格四个顶点的校正系数 根据待校正像素的坐标，计算该点落在哪一个网格中，求得网格的编号，再求得该网格四个顶点的编号，通过查表，求得网格四个顶点的校正值。 3、计算待矫正像素的校正值 通过双线性插值，由网格的四个顶点计算出待矫正像素的校正值。 4、矫正 将当前像素值乘以校正系数。</description>
    </item>
    <item>
      <title>扁平化立体字教程</title>
      <link>https://anwangtanmi.github.io/posts/1e486c4122105aba11baead5d6e04a2d/</link>
      <pubDate>Sun, 28 May 2017 21:18:23 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1e486c4122105aba11baead5d6e04a2d/</guid>
      <description>首先看看最终效果图 有点类似于扁平化的风格，但立体的感觉却更加明显，虽然看起来不难做，但实际操作的时候却会发现不少的问题，这里不仅涉及到了构图，也涉及到了有关色彩灰度与色彩明暗的问题，而且网络上这类设计的教程并未出现多少，废话不多说，接下来就是主题部分 首先用最通俗最明了的东西来告诉你什么是色彩明度与色彩灰度 是不是很明了，如果再看不明白建议去看看ps的新手教程 1.创建画布, 2.设置参考线与网格，这里是为了设置世界坐标轴 3.制作世界坐标轴 使用钢笔工具连接对角的点 4.再将其拷贝一层后逆时针60度 5.将最初设置的网格去除，开始绘制字母 6.这里需要思考字母结构与立方体的结构,已经如何去搭配，主要的的思路是在一条直线的会在一个面内，使用不同灰度来制作内部的结构,亮度提升后制作顶部 7.然后同样的方法来制作剩下的字 完结撒花，dwt，当然就是我了，看完后相信大家对此类设计都有了一定的了解 后面会继续更新更多的教程，谢谢观赏~~</description>
    </item>
    <item>
      <title>洛谷OJ – P1347 排序（拓扑排序）</title>
      <link>https://anwangtanmi.github.io/posts/9f6879e63917fee697661148e804773b/</link>
      <pubDate>Wed, 26 Apr 2017 22:23:14 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9f6879e63917fee697661148e804773b/</guid>
      <description>题目描述：&#xA;一个不同的值的升序排序数列指的是一个从左到右元素依次增大的序列，例如，一个有序的数列A,B,C,D 表示A 输入输出格式 输入格式：&#xA;第一行有两个整数n,m，n表示需要排序的元素数量，2&amp;lt;=n&amp;lt;=26，第1到n个元素将用大写的A,B,C,D….表示。m表示将给出的形如A 接下来有m行，每行有3个字符，分别为一个大写字母，一个&amp;lt;符号，一个大写字母，表示两个元素之间的关系。&#xA;输出格式：&#xA;若根据前x个关系即可确定这n个元素的顺序yyy..y（如ABC），输出&#xA;Sorted sequence determined after xxx relations: yyy…y.&#xA;若根据前x个关系即发现存在矛盾（如A Inconsistency found after 2 relations.&#xA;若根据这m个关系无法确定这n个元素的顺序，输出&#xA;Sorted sequence cannot be determined.&#xA;（提示：确定n个元素的顺序后即可结束程序，可以不用考虑确定顺序之后出现矛盾的情况）&#xA;输入输出样例 输入样例#1：&#xA;1: 4 6 A 输出样例#1：&#xA;1: Sorted sequence determined after 4 relations: ABCD. 2: Inconsistency found after 2 relations. 3: Sorted sequence cannot be determined. 题目思路：&#xA;每输入一条边进行一定判断，首先判断是否存在环，然后判断是否所有的结点都已出现，如果结点都出现，并且无环，那么找到入度为0的结点开始深搜，如果能遍历输出拓扑排序后的结果。如果不能遍历完，说明所有的结点并没有全部连通。继续下一步。&#xA;题目代码：&#xA;#include #include #include #include #include #include #include #define LL long long using namespace std; vector G[30]; int out[30], in[30]; int n, m, t, tot = 0; int exist[30], topo[30]; int vis[30]; string s; // 判断是否存在环 bool dfs(int x){ vis[x] = -1; for(int i = 0; i &amp;lt; G[x].</description>
    </item>
    <item>
      <title>【图像处理】Tensorflow:简易超分辨重建与坑</title>
      <link>https://anwangtanmi.github.io/posts/1c1e092adad079baccb5abc04ba452ce/</link>
      <pubDate>Fri, 31 Mar 2017 10:32:55 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1c1e092adad079baccb5abc04ba452ce/</guid>
      <description>超分辨重建是图像复原领域的一大热点，能在硬件有限的情况下最大还原原始场景的信号，在天文探索、显微成像等领域有重要作用。成像设备对物体成像时，由于距离较远，成像会模糊，可以类比多尺度高斯滤波；受限于成像机能，成像像素达不到最理想条件，可类比为对原始像进行一个下采样。超分辨重建就是要在这种条件下复原原始图像。 假设上帝有最好的成像设备，成像为X；我们成像设备成像为B，高斯滤波模板设为G；为了防止问题病态，加入lasso正则。那么有：&#xA;argmin [subsampling(conv(X,G))−B]2+λX&#xA;现在的问题是，Tensorflow如何表示subsampling并进行优化？&#xA;Tensorflow支持以下几种图像缩放/采样:&#xA;tf.image.resize_images，支持最近邻、双线性、双三次等缩放方法 tf.nn.max_pool 最大值下采样 tf.nn.avg_pool 均值下采样&#xA;现在我们逐个测试一下。图像经过三倍下采样： 1、tf.image.resize_images，双线性采样，振铃不严重，条纹很多： 2、tf.nn.max_pool，没有条纹、振铃，但是有一堆噪声，参数调了几次都没有什么更好的效果： 3、tf.nn.avg_pool，无条纹、噪声，有振铃，与原图相比颜色变暗，对比度下降: 4、来与原图做个对比 可以看出，效果最好的就是avg_pool了，在只有高斯模板参数，完全没有其他先验信息的情况，一秒钟内得到这个结果，已经让人非常惊讶了。猜测image-resize和max_pool其实在上下采样中都丢失相当多的信号，而avg_pool则保留了最多的信号，因此重建效果较好。 fast-neural-style文章提到过用感知特征来对图像进行超分辨重建，可以重建同样风格的细节，这个需要用生成网络对大量的图像进行训练，或者直接上vgg慢慢地计算感知特征来仿制风格细节。</description>
    </item>
    <item>
      <title>Stanford CS231n Lecture 1 计算机视觉历史回顾与课程大纲</title>
      <link>https://anwangtanmi.github.io/posts/4d193aa53b982cbe262b524b8992432d/</link>
      <pubDate>Mon, 06 Mar 2017 15:50:29 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4d193aa53b982cbe262b524b8992432d/</guid>
      <description>本系列文章是斯坦福CS231n: Convolutional Neural Networks for Visual Recognition (winter 1516) 的听课笔记与课下补充资料。&#xA;计算机视觉历史回顾 这是一个视觉时代，目前（2016）有85%的网络信息是像素形式，它们难以被利用，李飞飞将它们称为“网络中的暗物质”。CS231n聚焦于神经网络这一模型和计算机视觉这一应用。&#xA;视觉大约在5.4亿年前出现，有人认为，视觉的出现导致并驱动了生物大爆发。&#xA;16世纪，达芬奇发明了照相暗盒，开始复制人们看到的信息。&#xA;1959年，Huber&amp;amp;Wiesel研究生物的大脑是如何处理视觉信息的。他们发现，大脑从简单的形状（例如，边缘）开始处理视觉信息。在视觉处理的第一步，基础视觉区的神经元按一列一列组织起来，每一列神经元只“喜欢”某一种特定的形状。&#xA;1963年，Larry Roberts认为是边缘决定了物体外形，他的博士论文Block World是现代CV先驱。&#xA;1966年，MIT的人工智能实验室成立，标志着CV的诞生。&#xA;20世纪70年代，David Marr提出了一个重要思想，“视觉是分层的”。这一思想被Deep Learning/CNN汲取。&#xA;20世纪90年代，进入彩色时代，问题转向“感知分组”（人看到世界时会在脑中自动分割出各物体，而不是觉得只是一堆像素），这一问题现在仍未彻底解决。&#xA;进入21世纪，问题焦点从3D建模变迁到识别问题。出现了第一个商用的CV算法（实时人脸检测），它的特征学习过程有很强的深度学习特质。此后又出现了SIFT, Deformable Part Model等。后来又出现了ImageNet和神经网络的复兴。&#xA;CS231n 课程大纲 CS321n聚焦于视觉识别中的一个重要问题，图像分类。视觉识别中有很多问题与图像分类相关，比如物体检测、图像说明。&#xA;CNN是物体识别的重要工具。2012年之前的ILSVRC冠军模型采用的是“特征+SVM”，没有端到端学习的风格特色。2012年，Hinton和他的学生引领了神经网络的复兴。到2015年的MSRA，采用的仍是CNN。&#xA;AlexNet的成功离不开大量数据和高性能GPU，AlexNet只在1998年LeNet上做了很少改动。&#xA;视觉识别远不止物体识别，还包括很多内容，比如对全场景中的物体都打上标签、深入理解一幅图像……</description>
    </item>
    <item>
      <title>OpenCV实践（3）- 改变图像的对比度和亮度</title>
      <link>https://anwangtanmi.github.io/posts/d1d9f95f269dfc53a827cb91bed5ef94/</link>
      <pubDate>Wed, 01 Feb 2017 17:45:50 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d1d9f95f269dfc53a827cb91bed5ef94/</guid>
      <description>1 目标 （1）访问像素值； （2）初始化矩阵为0； （3）学习saturate_cast做什么和它为什么有用？ （4）Get some cool info about pixel transformations 2 理论 可以参考[计算机视觉：算法和应用](http://szeliski.org/Book/)一文。 3 图像处理 （1）图像处理运算就是一个函数把输入的一个或多个图像，转换为输出图像的过程； （2）图像变换可以被看成： a) 像素点的转换； b) 邻域的操作（基于域的概念）； 4 像素变换 在这类转换中，每一个输出像素点仅仅依赖于相对应的输入像素点（潜在地附加一些收集的全面信息和参数）。 这类操作的例子有：亮度和对比度的调整，以及色彩校正和转换。&#xA;4.1 亮度和对比度调整 （1） 理论公式为 （2） 在这里， 和 被称为增益和偏差参数，有时候，也被认为分别控制对比度和亮度。 （3）你可以认为f(x)是输入图像的像素值，g(x)是输出图像的像素值。那么为了方便我们就可以把上面的公式写为： 在这里，i和j分别代表像素点在行和列的位置。&#xA;4.2 代码实现 #include #include #include using namespace cv; double alpha; /**&amp;lt; Simple contrast control */ int beta; /**&amp;lt; Simple brightness control */ int main( int argc, char** argv ) { /// Read image given by user Mat image = imread( argv[1] ); Mat new_image = Mat::zeros( image.</description>
    </item>
    <item>
      <title>图像对比度增强算法</title>
      <link>https://anwangtanmi.github.io/posts/cb79f6f17562c7263cce9005c5e4191e/</link>
      <pubDate>Tue, 17 Jan 2017 17:53:19 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/cb79f6f17562c7263cce9005c5e4191e/</guid>
      <description>一、全局对比度增强&#xA;1&#xA;1. 直方图均衡化 Histogram Equalization a. 基本思想 对于图像中的灰度点做映射，使得整体图像的灰度大致符合均匀分布。增强图像的对比度。 – 对于灰度值连续的情况，使用灰度的累积分布函数CDF做转换函数，可以使得输出图像的灰度符合均匀分布。 – 对于灰度值不连续的情况，存在舍入误差，得到的灰度分布大致符合均匀分布。 – 直观地理解，如果某一个灰度范围（如200-201）的像素点很少，那么它的概率密度值就会很小，所以CDF在200-201附近的增长变化就会很小；反之，如果某一个灰度范围（如100-101）的像素点很多，CDF在100-101附近的增长变化会很大。总体来看，以灰度为横轴，CDF为纵轴画曲线。这种向上凸的曲线，很像gamma变换: s=crγ&#xA;中&#xA;γ&amp;lt;1&#xA;的情形。将灰度集中的部分拉伸，而将灰度不集中的部分压缩，达到提高对比度的效果。 – 直方图均衡可以看做自适应的gamma变换或者分段变换。前者的优势在于，不需要指定任何参数，所有运算都是基于图像本身的。&#xA;b. 算法 根据图像灰度计算灰度概率密度函数&#xA;PDF&#xA;计算累积概率分布函数&#xA;CDF&#xA;将&#xA;CDF&#xA;归一化到原图灰度取值范围，如[0,255]。 之后&#xA;CDF&#xA;四舍五入取整，得到灰度转换函数&#xA;sk=T(rk)&#xA;将&#xA;CDF&#xA;作为转换函数，将灰度为&#xA;rk&#xA;的点转换为&#xA;sk&#xA;灰度 c. matlab实验 代码&#xA;2&#xA;：&#xA;%% 直方图均衡 clear all;clc;close all; ImgFile=&#39;E:\图像处理\冈萨雷斯图片库\DIP3E_Original_Images_CH03\Fig0310(b)(washed_out_pollen_image).tif&#39;; ImgIn=imread(ImgFile); ImgHistEq=histeq(ImgIn,256); figure;subplot(121);imshow(uint8(ImgIn));title(&#39;原图&#39;); subplot(122);imshow(ImgHistEq);title(&#39;全局灰度增强 - 直方图均衡&#39;); figure;subplot(121);imhist(ImgIn,256); axis([0 255 0 1e5]);title(&#39;原图的直方图&#39;); subplot(122);imhist(ImgHistEq,256);axis([0 255 0 1e5]);title(&#39;直方图均衡化后的直方图&#39;); % 自定义直方图均衡 [counts,x]=imhist(ImgIn,256); cum_counts=cumsum(counts); cum_counts=uint8(cum_counts/max(cum_counts)*255);% 转化函数 figure;plot(x,cum_counts);axis([0 255 0 255]); xlabel(&#39;原图灰度&#39;);ylabel(&#39;转换后灰度&#39;);title(&#39;原图CDF转化的灰度映射函数&#39;); ImgOut=nan(size(ImgIn)); for i=1:length(x) ImgOut(ImgIn==x(i))=cum_counts(i); end ImgOut=uint8(ImgOut); figure;imshow(uint8(ImgOut));title(&#39;自定义直方图均衡&#39;) figure;imhist(ImgOut,256);axis([0 255 0 1e5]);title(&#39;自定义直方图均衡的直方图&#39;) 输出： 图1.</description>
    </item>
    <item>
      <title>图像处理之灰度模糊图像与彩色清晰图像的变换</title>
      <link>https://anwangtanmi.github.io/posts/ffcae77037d225b828a7f7991ca37860/</link>
      <pubDate>Sat, 24 Dec 2016 17:18:25 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ffcae77037d225b828a7f7991ca37860/</guid>
      <description>针对模糊图像的处理，个人觉得主要分两条路，一种是自我激发型，另外一种属于外部学习型。接下来我们一起学习这两条路的具体方式。&#xA;第一种 自我激发型 基于图像处理的方法，如图像增强和图像复原，以及曾经很火的超分辨率算法。都是在不增加额外信息的前提下的实现方式。 １.　图像增强&#xA;图像增强是图像预处理中非常重要且常用的一种方法，图像增强不考虑图像质量下降的原因，只是选择地突出图像中感兴趣的特征，抑制其它不需要的特征，主要目的就是提高图像的视觉效果。先上一张示例图： 图像增强中常见的几种具体处理方法为：&#xA;直方图均衡&#xA;在图像处理中，图像直方图表示了图像中像素灰度值的分布情况。为使图像变得清晰，增大反差，凸显图像细节，通常希望图像灰度的分布从暗到亮大致均匀。直方图均衡就是把那些直方图分布不均匀的图像（如大部分像素灰度集中分布在某一段）经过一种函数变换，使之成一幅具有均匀灰度分布的新图像，其灰度直方图的动态范围扩大。用于直方均衡化的变换函数不是统一的，它是输入图像直方图的积分，即累积分布函数。&#xA;灰度变换&#xA;灰度变换可使图像动态范围增大，对比度得到扩展，使图像清晰、特征明显，是图像增强的重要手段之一。它主要利用图像的点运算来修正像素灰度，由输入像素点的灰度值确定相应输出像素点的灰度值，可以看作是“从像素到像素”的变换操作，不改变图像内的空间关系。像素灰度级的改变是根据输入图像f(x,y)灰度值和输出图像g(x,y)灰度值之间的转换函数g(x，y)=T[f(x，y)]进行的。 灰度变换包含的方法很多，如逆反处理、阈值变换、灰度拉伸、灰度切分、灰度级修正、动态范围调整等。&#xA;图像平滑&#xA;在空间域中进行平滑滤波技术主要用于消除图像中的噪声，主要有邻域平均法、中值滤波法等等。这种局部平均的方法在削弱噪声的同时，常常会带来图像细节信息的损失。 邻域平均，也称均值滤波，对于给定的图像f(x,y)中的每个像素点(x,y)，它所在邻域S中所有M个像素灰度值平均值为其滤波输出，即用一像素邻域内所有像素的灰度平均值来代替该像素原来的灰度。 中值滤波，对于给定像素点(x,y)所在领域S中的n个像素值数值{f1,f2,…,fn}，将它们按大小进行有序排列，位于中间位置的那个像素数值称为这n个数值的中值。某像素点中值滤波后的输出等于该像素点邻域中所有像素灰度的中值。中值滤波是一种非线性滤波，运算简单，实现方便，而且能较好的保护边界。&#xA;图像锐化&#xA;采集图像变得模糊的原因往往是图像受到了平均或者积分运算，因此，如果对其进行微分运算，就可以使边缘等细节信息变得清晰。这就是在空间域中的图像锐化处理，其的基本方法是对图像进行微分处理，并且将运算结果与原图像叠加。从频域中来看，锐化或微分运算意味着对高频分量的提升。常见的连续变量的微分运算有一阶的梯度运算、二阶的拉普拉斯算子运算，它们分别对应离散变量的一阶差分和二阶差分运算。&#xA;２.　图像复原&#xA;其目标是对退化（传播过程中的噪声啊，大气扰动啊好多原因）的图像进行处理，尽可能获得未退化的原始图像。如果把退化过程当一个黑匣子（系统H），图片经过这个系统变成了一个较烂的图。这类原因可能是光学系统的像差或离焦、摄像系统与被摄物之间的相对运动、电子或光学系统的噪声和介于摄像系统与被摄像物间的大气湍流等。图像复原常用二种方法。当不知道图像本身的性质时，可以建立退化源的数学模型，然后施行复原算法除去或减少退化源的影响。当有了关于图像本身的先验知识时，可以建立原始图像的模型，然后在观测到的退化图像中通过检测原始图像而复原图像。 ３.　图像超分辨率　一张图我们想脑补细节信息好难，但是相似的多幅图我们就能互相脑洞了。所以，我们可以通过一系列相似的低分辨图来共同脑补出一张高清晰图啊，有了这一张犯罪人的脸，我就可以画通缉令了啊。。。 超分辨率复原技术的目的就是要在提高图像质量的同时恢复成像系统截止频率之外的信息，重建高于系统分辨率的图像。继续说超分辨，它其实就是根据多幅低质量的图片间的关系以及一些先验知识来重构一个高分辨的图片。示例图如下： 第二种 外部学习型 外部学习型，就如同照葫芦画瓢一样的道理。其算法主要是深度学习中的卷积神经网络，我们在待处理信息量不可扩充的前提下（即模糊的图像本身就未包含场景中的细节信息），可以借助海量的同类数据或相似数据训练一个神经网络，然后让神经网络获得对图像内容进行理解、判断和预测的功能，这时候，再把待处理的模糊图像输入，神经网络就会自动为其添加细节，尽管这种添加仅仅是一种概率层面的预测，并非一定准确。&#xA;本文介绍一种在灰度图像复原成彩色RGB图像方面的代表性工作：《全局和局部图像的联合端到端学习图像自动着色并且同时进行分类》。利用神经网络给黑白图像上色，使其变为彩色图像。稍作解释，黑白图像，实际上只有一个通道的信息，即灰度信息。彩色图像，则为RGB图像（其他颜色空间不一一列举，仅以RGB为例讲解），有三个通道的信息。彩色图像转换为黑白图像极其简单，属于有损压缩数据；反之则很难，因为数据不会凭空增多。&#xA;搭建一个神经网络，给一张黑白图像，然后提供大量与其相同年代的彩色图像作为训练数据（色调比较接近），然后输入黑白图像，人工智能按照之前的训练结果为其上色，输出彩色图像，先来看一张效果图： 本文工作 • 用户无干预的灰度图像着色方法。 • 一个新颖的端到端网络，联合学习图像的全局和局部特征。 • 一种利用分类标签提高性能的学习方法。 • 基于利用全局特征的风格转换技术。 • 通过用户研究和许多不同的例子深入评估模型，包括百年的黑白照片。&#xA;着色框架 模型框架包括四个主要组件：低级特征提取网络，中级特征提取网络，全局特征提取网络和着色网络。 这些部件都以端对端的方式紧密耦合和训练。 模型的输出是图像的色度，其与亮度融合以形成输出图像。 与另外两个工作对比&#xA;• Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning Representations for Automatic Colorization. In ECCV 2016. •Richard Zhang, Phillip Isola, and Alexei A.</description>
    </item>
    <item>
      <title>图像检索系列一：Deep Learning of Binary Hash Codes for Fast Image Retrieval</title>
      <link>https://anwangtanmi.github.io/posts/d1b35c65fb111d5c9969c067b1d95ca1/</link>
      <pubDate>Wed, 23 Nov 2016 14:44:06 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d1b35c65fb111d5c9969c067b1d95ca1/</guid>
      <description>Deep Learning of Binary Hash Codes for Fast Image Retrieval 这篇文章发表在2015CVPR workshop&#xA;文章链接：http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W03/papers/Lin_Deep_Learning_of_2015_CVPR_paper.pdf&#xA;代码链接：https://github.com/kevinlin311tw/caffe-cvprw15&#xA;图一 算法框架流程&#xA;这篇文章的想法很巧妙，在一个深层CNN的最后一个全连接层（fc8）和倒数第二个全连接层（fc7）之间加了一层全连接隐层，就是图一中绿色的latent layer （H）。这样一来，既可以得到深层的CNN特征，文中主要用的是fc7的特征，还可以得到二分的哈希编码，即来自H。这个隐层H不仅是对fc7的一个特征概括，而且是一个连接CNN网络的中层特征与高层特征的桥梁。&#xA;1. Domain Adaption&#xA;为了让一个网络能够对某一类物体高鲁棒，即target domain adaption，用一类主题目标数据集来整定(fine-tune)整个网络。fc8的节点数由目标类别数决定，H的节点数在文中有两种尝试：48和128。这两个层在fine-tune时，是随机初始化的，其中H的初始化参考了LSH[1]的方法，即通过随机映射来构造哈希位。通过这样训练，得到的网络能够产生对特定物体的描述子以及对应的哈希编码。&#xA;2. Image Retrieval&#xA;主要提出了一种从粗糙到细致的检索方案（coarse-to-fine）。H层首先被二值化：&#xA;粗糙检索是用H层的二分哈希码，相似性用hamming距离衡量。待检索图像设为I，将I和所有的图像的对应H层编码进行比对后，选择出hamming距离小于一个阈值的m个构成一个池，其中包含了这m个比较相似的图像。&#xA;细致检索则用到的是fc7层的特征，相似性用欧氏距离衡量。距离越小，则越相似。从粗糙检索得到的m个图像池中选出最相似的前k个图像作为最后的检索结果。每两张图128维的H层哈希码距离计算速度是0.113ms，4096维的fc7层特征的距离计算需要109.767ms，因此可见二值化哈希码检索的速度优势。&#xA;3. 实验结果&#xA;作者在MINIST，CIFAR-10，YAHOO-1M三个数据集上做了实验，并且在分类和检索上都做了实验，结果都很不错，特别是在CIFAR-10上图像检索的精度有30%的提升。&#xA;（1）MINIST&#xA;左边第一列是待检索图像，右边是48和128位H层节点分别得到的结果。可以看到检索出的数字都是正确的，并且在这个数据集上48位的效果更好，128位的太高，容易引起过拟合。&#xA;（2）CIFAR-10&#xA;在这个数据集上128位的H层节点比48位的效果更好，比如128检索出更多的马头，而48位的更多的全身的马。&#xA;（3）YAHOO-1M&#xA;作者在这个数据集上比较了只用fc7,只用H和同时用两者（粗糙到细致）的结果，实验结果表明是两者都用的效果更好。&#xA;可以看到如果只用alexnet而不进行fine-tune的话，检索出的结果精度很低。&#xA;4. 总结&#xA;这个方法整篇文章看下来给人的感觉比较工程，全篇讲理论和方法的部分很少，几乎没有什么数学公式，但是效果好，这个最重要。想法很简单，但是很巧妙，值得学习。代码已经开源，准备尝试。&#xA;[1] Gionis A, Indyk P, Motwani R. Similarity search in high dimensions via hashing[C]//VLDB. 1999, 99(6): 518-529.</description>
    </item>
    <item>
      <title>低照度图像增强（附步骤及源码）</title>
      <link>https://anwangtanmi.github.io/posts/84ea27d92df899973e58fa700a470b2f/</link>
      <pubDate>Sat, 19 Nov 2016 19:40:38 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/84ea27d92df899973e58fa700a470b2f/</guid>
      <description>好久没写博客了，工作中不断学习新东西，每天都会积累一点点，有时很想将学到的东西整理好，用文字记录并分享出来，即可以加深印象，又可以帮助有需要的朋友。但是工作起来经常身不由己，一年来的996工作制，多多少少有些忙乱，写博客的事自然就一拖再拖。过去一年多时间做了很多图像算法工作，记不清读了多少paper，查阅了多少开源代码，后面会陆续整理成文字及代码片段，就像前面说的，即是对过去一段时间工作的一个记录，又可以帮助做相关工作的朋友。 本文提到的低照度图像增强算法，原理很简单，但是效果尚可。当时在查阅相关资料时，无意中看到一个使用Photoshop拉亮低照度图像的方法，按照教程介绍，自己使用Photoshop操作一遍，觉得效果还可以。对，做图像算法还是要掌握一点Photoshop用法的，因为有时候图像算法就是把Photoshop上面的处理图像步骤程序化一下，比如后面介绍的图像倒影算法。本文介绍的低照度图像增强算法基本可以分2步： 将绿色通道反色后作为系数值，分别与各个通道相乘，得到新图层； 将新图层与原图做一次滤色混合，f(a, b) = 1 – (1 – a)*(1 – b)； 如果觉得增强程度不够，可以多做几次图层滤色操作。下面是相关代码： #include #include using namespace cv; using namespace std; #define BLUE 0 #define GREEN&#x9;1 #define RED 2 #ifndef SCREEN_XY #define SCREEN_XY(x, y) (255 - ((255 - (x))*(255 - (y)) &amp;gt;&amp;gt; 8)) #endif int main() { char imgfile[256] = &#34;E:\\t11.jpg&#34;; Mat src = imread(imgfile, CV_LOAD_IMAGE_COLOR); imshow(&#34;src&#34;, src); int size = src.cols * src.rows; uchar r = 0, g = 0, b = 0, g_alpha = 0; uchar *pSrcData = src.</description>
    </item>
    <item>
      <title>灰度图像简单二值化方法</title>
      <link>https://anwangtanmi.github.io/posts/83b08b710952c8dec2c4613029975e3e/</link>
      <pubDate>Fri, 12 Aug 2016 18:37:35 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/83b08b710952c8dec2c4613029975e3e/</guid>
      <description> 方法原理：&#xA;1.算出图像的均值&#xA;2.判断每个像素值，大于均值的赋值为255，小于均值的赋值为0&#xA;示例代码如下：&#xA;void img_binary(unsigned char* img, int h, int w) { int i, j; int totalPixValue = 0; int mean = 0; for (i = 0; i &amp;lt; h; i++) { for (j = 0; j &amp;lt; w; j++) { totalPixValue += img[i*w + j]; } } mean = totalPixValue / (h*w); for (i = 0; i &amp;lt; h; i++) { for (j = 0; j &amp;lt; w; j++) { if (img[i*w + j] &amp;lt; mean) img[i*w + j] = 0; else img[i*w + j] = 255; } } } </description>
    </item>
    <item>
      <title>魔板</title>
      <link>https://anwangtanmi.github.io/posts/0aac16511d57804b93a98c1f6468da59/</link>
      <pubDate>Wed, 30 Mar 2016 15:03:49 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0aac16511d57804b93a98c1f6468da59/</guid>
      <description>问题描述 有这样一种魔板：它是一个长方形的面板，被划分成n行m列的n*m个方格。每个方格内有一个小灯泡，灯泡的状态有两种(亮或暗)。我们可以通过若干操作使魔板从一个状态改变为另一个状态。操作的方式有两种： (1)任选一行，改变该行中所有灯泡的状态，即亮的变暗、暗的变亮； (2)任选两列，交换其位置。 当然并不是任意的两种状态都可以通过若干操作来实现互相转化的。 你的任务就是根据给定两个魔板状态，判断两个状态能否互相转化。 【输入】 文件中包含多组数据。第一行一个整数k，表示有k组数据。 每组数据的第一行两个整数n和m。(0&amp;lt; n，m≤100) 以下的n行描述第一个魔板。每行有m个数字(0或1)，中间用空格分隔。若第x行的 第y个数字为0，则表示魔板的第x行y列的灯泡为“亮”；否则为“暗”。 然后的n行描述第二个魔板。数据格式同上。 任意两组数据间没有空行。 【输出】 共k行，依次描述每一组数据的结果。 若两个魔板可以相互转化，则输出YES，否则输出NO。(注意：请使用大写字母) 【样例】 panel.in 2 3 4 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 2 2 0 0 0 1 1 1 1 1 panel.out YES NO&#xA;题解 这个题水得= =&#xA;匹配嘛，对于第一列有m种匹配方法，处理出每种里需要倒置的行然后打标记，后面的列就找一个在这种倒置情况下能够匹配的列，然后DFS解。 绝大多数数据可以A，但是对于1 100这种情况的复杂度最坏好像是100!，所以对于n==1的情况，对那一行排序再搞一下就行了&#xA;代码 暴搜都好长啊= = #include #include #include #include #include #define rep(i,x) for(int i=1;i&amp;lt;=x;i++) using namespace std; const int maxn=100+10; int mx1[maxn][maxn],mx2[maxn][maxn]; #define MX1 mx1[j][cur] #define MX2 mx2[j][i] int n,m; int used[maxn],chan[maxn],OK; void dfs(int cur) { if(cur==m+1) {OK=1;return ;} if(cur==1) { rep(i,m) if(!</description>
    </item>
    <item>
      <title>【图形学】谈谈噪声</title>
      <link>https://anwangtanmi.github.io/posts/c11084a10fc8a635759685714fecda15/</link>
      <pubDate>Mon, 21 Dec 2015 17:36:14 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c11084a10fc8a635759685714fecda15/</guid>
      <description>写在前面 很早就想学习和整理下噪声，稍微接触过图形学的人大概都听到过噪声，然后就会发现有各种噪声，Perlin噪声，Worley噪声，分形（fractal）噪声等等。尤其是Perlin噪声，一搜资料发现大家说的各不相同，更加不明所以。我也总是困惑，后来发现还是要相信wiki和paper。&#xA;这篇文章在于总结上面这些常见的噪声（即图形学中常见的程序噪声），它们是什么，怎么算出来的，以及一些应用。文章里的所有代码可以在我的Shadertoy上找到：&#xA;2D版：&#xA;width=”500″ height=”320″ src=”https://www.shadertoy.com/embed/ldc3RB?gui=true&amp;amp;t=10&amp;amp;paused=true&amp;amp;muted=false” allowfullscreen=””&amp;gt;&#xA;3D版：&#xA;width=”500″ height=”320″ src=”https://www.shadertoy.com/embed/4sc3z2?gui=true&amp;amp;t=10&amp;amp;paused=false&amp;amp;muted=false” allowfullscreen=””&amp;gt;&#xA;什么是噪声 在图形学中，我们使用噪声就是为了把一些随机变量来引入到程序中。从程序角度来说，噪声很好理解，我们希望给定一个输入，程序可以给出一个输出：&#xA;value_type noise(value_type p) { ... } 它的输入和输出类型的维数可以是不同的组合，例如输入二维输出一维，输入二维输出二维等。我们今天就是想讨论一下上面函数中的实现部分是长什么样的。&#xA;为什么我们需要这么多噪声 我对噪声的学习还没有很深，在此只想谈一点自己的想法。噪声其实就是为了把一些随机变量引入到程序中。在我们写一些C++这样的程序时，也经常会使用random这样的函数。这些函数通常会产生一些伪随机数，但很多情况下也足够满足我们的需要。同样，在图形学中我们也经常会需要使用随机变量，例如火焰、地形、云朵的模拟等等。相信你肯定听过大名鼎鼎的Minecraft游戏，这个游戏里面的地形生成也大量使用了随机变量。那么我们直接使用random这种函数不就好了吗？为什么要引入这么多名字的噪声呢？&#xA;这种直接使用随机生成器生成的随机值固然有它的好处，但它的问题在于生成的随机值太“随机”了。在图形学中，我们可以认为这种噪声就是白噪声（White noise）。wiki上说白噪声是功率谱密度在整个频域内均匀分布的噪声，听不懂对不对？通俗来讲，之所以称它为“白”噪声，是因为它类似于光学中包括全部可见光频率在内的白光。我相信你肯定听过白噪声，小时候电视机收音机没信号时，发出的那个沙沙声就是一种声音上的白噪声。我们这里只需要把白噪声理解为最简单的随机值，例如二维的白噪声纹理可以是下面这个样子：&#xA;可以看出白噪声非常不自然，听起来很刺耳，看起来也不好看。不光你这么想，图形学领域的前辈们也早发现了。如果你观察现实生活中的自然噪声，它们不会长成上面这个样子。例如木头纹理、山脉起伏，它们的形状大多是趋于分形状（fractal）的，即包含了不同程度的细节。比如地形，它有起伏很大的山脉，也有起伏稍小的山丘，也有细节非常多的石子等，这些不同程度的细节共同组成了一个自然的地形表面。那么，我们如何用程序来生成类似这样的自然的随机数（可以想象对应了地形不同的高度）呢？学者们根据效率、用途、自然程度（即效果好坏）等方面的衡量，提出了许多希望用程序模拟自然噪声的方法。例如，Perlin噪声被大量用于云朵、火焰和地形等自然环境的模拟；Simplex噪声在其基础上进行了改进，提到了效率和效果；而Worley噪声被提出用于模拟一些多孔结构，例如纸张、木纹等。&#xA;因此，学习和理解这些噪声在图形学中是十分必要的，因为它们的应用实在是太广泛了！&#xA;噪声的分类 根据wiki，由程序产生噪声的方法大致可以分为两类：&#xA;类别 名称 基于晶格的方法（Lattice based） 又可细分为两种：&#xA;第一种是梯度噪声（Gradient noise），包括Perlin噪声， Simplex噪声，Wavelet噪声等；&#xA;第二种是Value噪声（Value noise）。 基于点的方法（Point based） Worley噪声 需要注意的是，一些文章经常会把Perlin噪声、Value噪声与分形噪声（Fractal noise）弄混，这实际在概念上是有些不一样的。分形噪声会把多个不同振幅、不同频率的octave相叠加，得到一个更加自然的噪声。而这些octave则对应了不同的来源，它可以是Gradient噪声（例如Perlin噪声）或Value噪声，也可以是一个简单的白噪声（White noise）。&#xA;一些非常出色的文章也错误把这种分形噪声声称为Perlin噪声，例如：&#xA;Hugo Elias的文章，这篇文章讲得挺有趣的，关于什么是octave、怎么混合它们都讲得很细致，也非常有名，但作者错误地把值噪声+分形噪声标识为Perlin噪声，他的文章链接也出现了wiki的值噪声（Value noise）的页面中。&#xA;Devmag的如何在你的游戏中使用Perlin噪声一文，同样非常有名，但同样错误地把白噪声+分形噪声认为是Perlin噪声。&#xA;如果读者常逛shadertoy的话，会发现很多shader使用了类似名为fbm的噪声函数。fbm实际就是分型布朗运动（Fractal Brownian Motion）的缩写，读者可以把它等同于我们上面所说的分形噪声（Fractal noise），我们以下均使用fbm来表示这种噪声的计算方法。如果要通俗地说fbm和之前提及的Perlin噪声、Simplex噪声、Value噪声、白噪声之间的联系，我们可以认为是很多个不同频率、不同振幅的基础噪声（指之前提到的Perlin噪声、Simplex噪声、Value噪声、白噪声等之一）之间相互叠加，最后形成了最终的分形噪声。这里的频率指的是计算噪声时的采样距离，例如对于基于晶格的噪声们，频率越高，单位面积（特指二维）内的晶格数目越多，看起来噪声纹理“越密集”；而振幅指的就是噪声的值域。下图显示了一些基础噪声和它们fbm后的效果：&#xA;说明：分割线左侧表示单层的基础噪声，右侧表示通过叠加不同频率噪声后的fbm效果。上面效果来源于shadertoy：Perlin噪声，Simplex噪声，Value噪声，Worley噪声。&#xA;由于Worley噪声的生成和其他噪声有明显不同，因此不是本文的重点。它主要用于产生孔状的噪声，有兴趣的读者可以参见偶像iq的文章：&#xA;http://www.iquilezles.org/www/articles/smoothvoronoi/smoothvoronoi.htm http://www.iquilezles.org/www/articles/voronoise/voronoise.htm Perlin噪声、Simplex噪声和Value噪声在性能上大致满足：Perlin噪声 &amp;gt; Value噪声 &amp;gt; Simplex噪声，Simplex噪声性能最好。Perlin噪声和Value噪声的复杂度是&#xA;O(2n)&#xA;，其中n是维数，但Perlin噪声比Value噪声需要进行更多的乘法（点乘）操作。而Simplex噪声的复杂度为&#xA;O(n2)&#xA;，在高纬度上优化明显。&#xA;下面的内容就是重点解释Perlin噪声、Perlin噪声和Simplex噪声这三种常见的噪声，最后再介绍fbm。 Perlin噪声 先介绍大名鼎鼎的Perlin噪声。很多人都知道，Perlin噪声的名字来源于它的创始人Ken Perlin。Ken Perlin早在1983年就提出了Perlin noise，当时他正在参与制作迪士尼的动画电影《电子世界争霸战》（英语：TRON），但是他不满足于当时计算机产生的那种非常不自然的纹理效果，因此提出了Perlin噪声。随后，他在1984年的SIGGRAPH Course上做了名为Advanced Image Synthesis1的课程演讲，并在SIGGRAPH 1985上发表了他的论文2。由于Perlin噪声的算法简单，被迅速应用到各种商业软件中。我们这位善良的Perlin先生却并没有对Perlin噪声算法申请专利（他说他的祖母曾叫他这么做过……），如果他这么做了那会是多大一笔费用啊！（不过在2001年的时候，旁人看不下去了，把三维以上的Simplex噪声的专利主动授予了Perlin。对，Simplex噪声也是人家提出的……）再后来Perlin继续研究程序纹理的生成，并和他的一名学生又在SIGGRAPH 1989上发表了一篇文章3，提出了超级纹理（hypertexture）。他们使用噪声+fbm+ray marching实现了各种有趣的效果。到1990年，已经有大量公司在他们的产品中使用了Perlin噪声。在1999年的GDCHardCore大会上，Ken Perlin做了名为Making Noise的演讲4，系统地介绍了Perlin噪声的发展、实现细节和应用。如果读者不想读论文的话，强烈建议你看一下Perlin演讲的PPT。</description>
    </item>
    <item>
      <title>高级纹理映射知识汇总</title>
      <link>https://anwangtanmi.github.io/posts/483e7aa093297dd933d422a06761410d/</link>
      <pubDate>Mon, 03 Aug 2015 08:56:49 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/483e7aa093297dd933d422a06761410d/</guid>
      <description>参考整理自： http://17de.com/library/d3d_6im/d3dim6_20.htm http://www.cppblog.com/lovedday/archive/2008/05/22/50782.html 多阶层纹理的基本概念 纹理拥有一个surface表面和格式大小等数据，后台缓存也有一个surface,是可以从后台缓存获取surface拷贝到texture的surface的但是有点影响效率。 这里多阶层纹理融合是指对单个物体多边形顶点像素上设置多个阶层纹理时候的融合操作，最终得到物体多边形顶点像素上的颜色。而&#xA;一般的融合指着色阶段的网格物体颜色和后台缓存的融合，该blend融合也可以设置融合操作和基于颜色还是alpha融合。 多通道纹理&#xA;(多次渲染, 每次设置一个纹理)已经不用了，现在DX6后都是用单通道多阶层Stage纹理（一次渲染可以将多个纹理融合到一个多边形顶点上），老的多通道纹理是可以转换到多阶层纹理的。 多阶段纹理设置的才启用，不设置的不启用，如果要禁用一个阶段纹理设置它的颜色操作为COLOROP_DISABLE即可。&#xA;Device-&amp;gt;SetTextureStageState(i, D3DTSS_COLOROP, D3DTOP_DISABLE); Device-&amp;gt;SetTextureStageState(i, D3DTSS_ALPHAOP, D3DTOP_DISABLE); 每个阶段纹理都有自己的对应纹理指针对象，和纹理采用的顶点坐标，还有纹理过滤的采样状态。 Device-&amp;gt;SetTexture(i, pTexObj); Device-&amp;gt;SetTextureStageState(i, D3DTSS_TEXCOORDINDEX, 0); Device-&amp;gt;SetSamplerState(&#xA;i, D3DSAMP_MAGFILTER, D3DTEXF_LINEAR); Device-&amp;gt;SetSamplerState(&#xA;i, D3DSAMP_MINFILTER, D3DTEXF_LINEAR); // MipMapTexture 多幅渐进纹理,在CreateTextureFromFile和CreateTextureFromFileEx时候就会创建，这里选择纹理采用线性插值 Device-&amp;gt;SetSamplerState(&#xA;i, D3DSAMP_MIPFILTER, D3DTEXF_LINEAR); 每个阶段纹理都有自己的两个参数纹理和操作，两个参数arg1和arg2, arg1是当前stage的纹理，arg2是第0层纹理或者是上一层融合的结果纹理，操作有两种分别是alpha操作和颜色操作，两个参数arg1和arg2 会根据操作进行设置值，操作就是指在两个颜色之间进行的从操作：color = arg1 opr arg2, opr是D3DTOP_ADD就是加，不会像物体和后台缓存融合一样先进行分量乘再相加。 pd3dDevice-&amp;gt;SetTextureStageState(i, D3DTSS_XXXARG1, D3DTA_XXX); pd3dDevice-&amp;gt;SetTextureStageState(i, D3DTSS_XXXARG2, D3DTA_XXX); pd3dDevice-&amp;gt;SetTextureStageState(i, D3DTSS_XXXOP, D3DTOP_XXX); 阶段状态前缀：D3DTSS_ 前缀是Texture stage state, 有D3DTSS_COLOROP和D3DTSS_ALPHAOP以及他们相同的操作值，有 D3DTSS_COLORARG1相同以及他们相同的操作值。&#xA;状态的值类型为：D3DTOP_ 前缀是Operation。 或者状态的值类型为：D3DTA_ 前缀是Texture Argument。 参数一般是两元组合，如果需要三元组合操作那么需要： D3DTSS_COLORARG0 = 26, /* D3DTA_* third arg for triadic ops */ D3DTSS_ALPHAARG0 = 27, /* D3DTA_* third arg for triadic ops */ 检测多阶层纹理融合阶层数： D3DCAPS9 pCaps; Device-&amp;gt;GetDeviceCaps(&amp;amp;pCaps); if(pCaps.</description>
    </item>
    <item>
      <title>获取网络图片的大小</title>
      <link>https://anwangtanmi.github.io/posts/9bdd4dec78cef2be532ad8cccb351b0c/</link>
      <pubDate>Mon, 17 Nov 2014 10:29:04 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9bdd4dec78cef2be532ad8cccb351b0c/</guid>
      <description>// 图片处理 0 半灰色 1 灰度 2 深棕色 3 反色 +(UIImage*)imageWithImage:(UIImage*)image grayLevelType:(UIImageGrayLevelType)type; //色值 变暗多少 0.0 - 1.0 +(UIImage*)imageWithImage:(UIImage*)image darkValue:(float)darkValue; /** 获取网络图片的Size, 先通过文件头来获取图片大小 如果失败 会下载完整的图片Data 来计算大小 所以最好别放在主线程 如果你有使用SDWebImage就会先看下 SDWebImage有缓存过改图片没有 支持文件头大小的格式 png、gif、jpg http://www.cocoachina.com/bbs/read.php?tid=165823 */ +(CGSize)downloadImageSizeWithURL:(id)imageURL; 派生到我的代码片 //讨厌警告 -(id)diskImageDataBySearchingAllPathsForKey:(id)key{return nil;} +(CGSize)downloadImageSizeWithURL:(id)imageURL { NSURL* URL = nil; if([imageURL isKindOfClass:[NSURL class]]){ URL = imageURL; } if([imageURL isKindOfClass:[NSString class]]){ URL = [NSURL URLWithString:imageURL]; } if(URL == nil) return CGSizeZero; NSString* absoluteString = URL.absoluteString; #ifdef dispatch_main_sync_safe if([[SDImageCache sharedImageCache] diskImageExistsWithKey:absoluteString]) { UIImage* image = [[SDImageCache sharedImageCache] imageFromMemoryCacheForKey:absoluteString]; if(!</description>
    </item>
    <item>
      <title>半色调技术简介</title>
      <link>https://anwangtanmi.github.io/posts/2260cdceb4f23e07438fa243a8663d27/</link>
      <pubDate>Thu, 06 Nov 2014 09:11:18 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2260cdceb4f23e07438fa243a8663d27/</guid>
      <description>引言 现有的半色调技术种类繁多，生成的半色调图像的视觉效果也越发逼真，不过相应的代价就是较高的时间复杂度。有序抖动方法最简单而且能够完全并行处理整幅图像，但是生成的半色调图像视觉效果最差。误差传递方法产生的半色调图像视觉效果很好，但由于它的生成半色调图像过程的所要求的传递性，不能和有序抖动一样并行完成半色调过程。直接二元搜索法生成的半色调图像的视觉效果最好，但是计算复杂度却相当高，不合适在要求实时输出半色调图像的场合下使用。现在应用较广的误差传递方法在表现图像细节上仍需改进，其生成的半色调图像纹理从人眼观测看来不够细腻。我们需要根据人眼视觉系统的特性设计出视觉效果更加生动的半色调算法。&#xA;半色调技术是&#xA;指用少量的色彩将一幅连续色调图像(如灰度图像和彩色图像)量化为一幅二值图像或是只有少数几种色彩的彩色图像，并且量化后图像在一定距离的视觉效果和原始图像相似的技术。众所周知，数字半色调技术是指基于人眼视觉特性和图像呈色特性，利用数学、计算机等工具，在单色/多色二值呈色设备上实现图像的最优再现的一门技术。数字半色调是&#xA;利用人眼的低通特性，当在一定距离下观察时，人眼将图像中空间上接近的部分视为一个整体。利用此特性，人眼观察到的半色调图像局部平均灰度近似于原始图像的局部平均灰度值，从而整体上形成连续色调的效果。&#xA;连续调图像与半色调图像 在我们的日常生活中，所遇到的图像可大致分为两大类：连续调图像(Continuous-Tone Image)和半色调图像(Halftone Image)。&#xA;如我们常见的彩色照片就是一种连续调图像，在这种图像上，存在着由淡到浓或由深到浅的色调变化并且浓淡或深浅是以单位面积成像物质颗粒密度来构成的，并且这种图像的深浅变化有无数多级。另外,印刷工艺中的照相分色底片的连续调，是由单位面积内由金属银颗粒密度构成的；而各种彩色画稿的连续调，是由单位面积内由各种颜料颗粒密度构成的，单位面积内颜料颗粒多即为深色调，否则为浅色调。 半色调图像如常见的印刷品图像，其由浅到深或由淡到浓的变化，是靠网点面积大小或网点覆盖率来表现的。一般用于复制诸如照片之类的连续调原稿时，会采用这种半色调技术，它将图像分成许多点，通过点的不同大小来表现颜色的深浅。在印刷品的印刷时，印刷机用有限数量的一套油墨(只有黑墨或青、品红、黄、黑墨)来印刷数量不同、大小不同的细小点，印刷品画面上色彩和浓淡就是靠这些细小的点来表示的，由此可以给人眼产生许多灰度级或许多颜色的错觉。当观察印品画面时，网点面积大，颜色就深，称为暗调；网点面积小，颜色就浅，则称为亮调。由于网点在空间上是有一定的距离的，呈离散型分布，并且由于加网的线数总有一定的限制，在图像的层次变化上不能像连续调图像一样实现无级变化，故称加网图像为半色调图像。如加网的阳片菲林、阴片菲林、印刷图像等都是半色调图像。 半色调技术的发展历史&#xA;在印刷工艺中，半色调技术也称为加网技术，加网技术发展到现在己有上百年的历史。从早期的照相加网到现代的数字加网，从调幅加网到调频加网，从粗网点到精细网点，共经历了三个阶段：照相加网(即模拟加网)、电子加网和计算机数字加网三个发展阶段。&#xA;模拟加网 模拟加网是指利用网屏对光线的分割作用，将连续调图像分解成大小不同网点的网目调图像的方法。它的发展可分为玻璃网屏加网和接触网屏加网两个阶段。 网屏加网是德国的GocgrMeisnebach在1882年首创将连续调图像经加网分解成网目调图像进行制版印刷的加网技术，于1886年由美国人Ives和Ivey制造成功。投影网屏加网是加网技术的鼻祖，使印刷产品能反映图像的层次变化，但技术要求高，操作复杂。在制作玻璃网屏时，选用优质的光学玻璃，表面涂上耐强酸的漆膜，在专门雕刻机上划成等长的平行直线，线条的疏密由网屏的线数来确定。刻好后在玻璃背面也涂上耐酸保护膜，用氢氟酸腐蚀。经腐蚀凹下的部分涂上黑色油质，然后洗去耐酸膜层，这样就制成了透明与不透明相间的玻璃线条版。用两块这样的线条版垂直胶合在一起，边上镶好金属框，即制成玻璃网屏。加网时，光线通过网屏的网孔投射在感光版上。由于光线通过网孔后的衍射作用，就在湿版上产生大小不同的网点。通过网孔的光量强，网点就大；光量弱，网点就小。这样，就把原稿图像的明暗变化，映射到感光版上后转变成网点的大小变化。这种用网点大小表现明暗深浅的阶调，在工艺上称半色调(halftone)。由于网屏上网线的宽度与透明方格的宽度相同，4条网线才围出一个透光方格。所以，整个网屏面积中只有四分之一的面积是透光的。为了在感光版上形成足够大小的网点，就必须有足够的感光量。由于感光版感光性能本来就低，再加上玻璃网屏的透光率也低，所以加网必须有功率强大的光源和较长的曝光时间。这种网屏在七十年代以前照相制版中用了很长时间。玻璃网屏除有笨重、价昂等缺点外，使用时网屏与感光片之间必须有一定的距离(网距)，易损失图像层次，影响画面清晰度，所以逐渐被接触网屏取代. 接触网屏加网是Kodak公司在1940年根据半影理论第一次用工业方法研究成功的第二代加网技术。它克服了投影网屏的诸多缺陷：如在高光和暗调部位，网点的表现效果较差；需要备用各种点形、反差、线数的网屏；耐用性较差等。德国Klimsch厂在60年代生产了全阶调Gardar网屏，该网屏的线型是透明的，所以能把原稿上明亮部分的色调予以调节，有利于保证高调细节的完整性。 这种网屏一般是用玻璃网屏作母版，在制版照相机上用硬性感光胶片制成的。接触网屏为胶片状，用放大镜仔细观察，上面布满了网点，每个圆网点中间有一个很黑的核心，边缘密度递减。从整体上看网点的排列，每四个黑点中间有一个透明孔，或每四个透明孔中间有一个黑点。接触网屏具有价廉、体轻，加网时与感光胶片密合接触，进行曝光，光线通过接触网屏就会在感光胶片上形成大小不同的光洁网点，其加网效果同用玻璃网屏基本一样，减少了翻拍中的层次损失，具有使用方便，提高了画面清晰度等优点。 电子加网 电子加网是指将图像信号数字化后经计算机计算处理并转换成网点输出的新型加网技术，这种技术产生于80年代初，是在电子分色机上通过电子网点发生装置对原稿完成加网的过程。 其原理是：经过电子分色机处理的代表图像不同密度级次的数字信号，送入电子分色机图像输出记录系统的网点计算机，并通过比较回路形成网点大小、形状、角度的地址指令，由地址指令从网点计算机中获得控制激光记录系统的控制信号，加在电光调制器上，控制各个电光调制器的输出工作状态，最后把由调制器控制的光信号记录在感光材料上，就能获得与原稿图像信息相一致的、具有特定大小、形状和角度的网点。 由于电子加网是数字控制的，网点是由激光记录所得，因此具有网点实、密度高、边缘清晰；网点层次多，细微层次丰富并且可以根据需要在不同阶调处产生形状不同的网点，易于控制网点扩大等优点。 计算机数字加网 数字加网是指桌面出版系统中采用基于PosstcriPt框架下的网点技术。根据记录点的分布状态，可以将数字加网分为调幅加网和调频加网两大技术。&#xA;数字调幅加网技术(点聚集态网点技术)在数字加网技术中，调幅加网是用不可见的、行列排列有序的网格分割图像，每个网格按照一定的角度、加网线数生成面积不同的网点。在生成记录网点的黑/白(0/l)时，总要受到加网角度、网点形状和加网线数的限制，所以在每个位置上设置0/1的自由度相对较低。调幅加网技术又称为点聚集态网点技术，这种加网技术得到的是聚集态的记录点。我们通常将聚集在一起的点称为传统网点，它由图像信息的灰度层次来控制网点的增长。 数字调频加网技术(点离散态网点技术)调频加网从某种意义上来说又可以称为点离散态网点技术，该技术得到的是离散分布的记录点。这种加网计算不再受网格的限制，直接以发散无序的记录点阵象素群构成图像，在每个可记录位置上设置0/l的自由度高于调幅加网，这使得调频加网图像具有较高的信息容量。调频加网又称随机加网，/随机性0在数学中用于描述分析和预言粒子在粘稠液体中运动的过程，印刷中则用于描述精确摆放点子的方法，以产生随机性并产生色调级的感觉。实际上，网点位置是基于/计算的随机性0。网点的空间分布是通过算法来分配的，根据色调的统计估算值和图像邻近部分的细节来分布点子，不会出现明显的堆积或不需要的微型点累积。 半色调技术分类 目前半色调技术最普遍的分类法是按照它的处理方式分为：抖动法，误差扩散法，迭代法三大类。 抖动法 抖动法是点处理类方法的一种典型算法，主要分为随机抖动和有序抖动两大类。这两种算法都需要一个模板，也称为抖动矩阵或阈值矩阵，抖动矩阵不仅决定了当亮度或灰度值减小时网点变成黑点的顺序.而且也决定了半色调图像的质量，所以抖动算法的关键是抖动矩阵的构造。该算法与抖动矩阵进行比较，矩阵中的每个阈值的取值范围是图像的最大灰度值和最小灰度值之间，其数学公式化如公式1： ……………………….公式 1&#xA;式中f(i，j)代表连续色调图像中的像素点灰度值，t(i，j)代表抖动矩阵的阈值，而h(i，j)代表半色调后的图像灰度值。 随机抖动矩阵是通过完全随机产生的，所以半色调后的图像质量常常很不理想，在实际中已经基本不再使用。但有序抖动的抖动矩阵是有规律的，具有良好的图像效果和高效的处理速度而被各大打印机厂商采用，有序抖动矩阵主要有两种类型：分散型和聚集型。典型的分散型抖动矩阵是Bayer有序抖动矩阵，而点局部聚簇整体分散是典型的聚集型矩阵，如图1所示。 (a) Bayer有序抖动阈值矩阵 (b)聚簇型抖动阈值矩阵&#xA;图&#xA;1 有序抖动阈值模板 虽然有序抖动算法比较简单，且具有较好的半色调图像质量，但其也具有致命的缺点，即含有明显的周期性人工纹理。即使抖动矩阵设计的非常完美，其输出的半色调图像依然存在瑕疵，且其半色调图像质量不如通过误差扩散算法获得的半色调图像。图2是有序抖动方法的输出结果 .&#xA;(a) 输入图像 (b) Bayer有序抖动结果&#xA;图 2 输入图像及Bayer有序抖动结&#xA;误差扩散法Error Diffusion 在有序抖动处理中，利用了像素点与抖动矩阵比较来判断是否在一个位置放置微点，实质是一种点处理过程。在1976年Floyd和Steinberg提出了误差扩散算法，它将半色调加网从“点处理”过渡到“邻域处理”。误差扩散算法的提出为半色调加网带来了革命性的技术变革，也是半色调技术上的里程碑，并促进了半色调技术的飞速发展。通过误差扩散处理后的半色调图像像素分布各异且无规律性，色调丰富，视觉效果较好。直到目前为止，它依然被视为易于实现且视觉效果较好的半色调技术之一，被广泛应用.&#xA;图 3 Floyd-Steinberg误差扩散示意图&#xA;给定阈值t，设原图像为x(i，j)，输出为y(i，j)。对整幅图像按从左到右、从上到下的顺序逐点的依次执行两步操作：&#xA;阈值化y(i，j): 把量化误差扩散到邻近的未被处理过的点。量化误差是指输入与输出之间的差：量化误差扩散即改变空间上相邻像素x(i，j+1)，x(i+l，j-1)，x(i+1，j)，x(i+l，j+1)的值，将当前像素的量化误差按7：3：5：1的比例转移并叠加到邻近的像素 图4为输入图像与采用误差扩散输出结果,如下图所示.&#xA;(a) 输入图像 (b) 误差扩散结果&#xA;图 4 输入图像与误差扩散结果&#xA;换句话说，误差扩散相当于把中值阈值法在每一个点上产生的误差再加到周围的点上，从而保持局部区域的总体灰度基本不变。量化误差扩散的参数可以用一个矩阵描述，称之为误差扩散过滤器，设为P，P={Nr，s}，(r，s)D为P中所有非零元素对应的下标集合。这里的下标r，s是整数，可以为正、为负或为0。这样上述公式可写为：&#xA;Floyod-Steinberg算法中的P如图3。矩阵中的圆点代表N0，0(以下类似)。圆点和空自位置的参数都为0。因为像素处理顺序是从左到右、从上到下，所以量化误差只能向右方、左下、下方和右下的相邻像素扩散。&#xA;阈值r的选取通常取中间值0.5。曾有人对其他阈值做过尝试，但结果没有明显的优点，所以大多数算法仍然简单的用0.5作为阈值。现举例说明阈值比较和扩散的过程：假设当前输入像素灰度为0.7，阈值为0.5，则该点输出1。量化误差为-0.3。Floyd-Steinberg算法的参数是7/16，3/16，5/16，1/16，即将量化误差-0.3分别乘以7/16，3/16，5/16和1/16叠加到四个相邻像素如图4所示。</description>
    </item>
    <item>
      <title>真实感渲染之Tone Mapping</title>
      <link>https://anwangtanmi.github.io/posts/0164c4151c5cdceebeff1a96015d3d8d/</link>
      <pubDate>Sat, 17 Nov 2012 15:04:18 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0164c4151c5cdceebeff1a96015d3d8d/</guid>
      <description>Rendering in Computer Graphic水太深，慢慢学习ING…&#xA;好的效果是最有说服力的：&#xA;以下内容转载自网络，Just search it：&#xA;Tone Mapping原是摄影学中的一个术语，因为打印相片所能表现的亮度范围不足以表现现实世界中的亮度域，而如果简单的将真实世界的整个亮度域线性压缩到照片所能表现的亮度域内，则会在明暗两端同时丢失很多细节，这显然不是所希望的效果，Tone Mapping就是为了克服这一情况而存在的，既然相片所能呈现的亮度域有限则我们可以根据所拍摄场景内的整体亮度通过光圈与曝光时间的长短来控制一个合适的亮度域，这样既保证细节不丢失，也可以不使照片失真。人的眼睛也是相同的原理，这就是为什么当我们从一个明亮的环境突然到一个黑暗的环境时，可以从什么都看不见到慢慢可以适应周围的亮度，所不同的是人眼是通过瞳孔来调节亮度域的。&#xA;而这个问题同样存在在计算机图形上，为了让图像更真实的显示在显示器上，同样需要Tone Mapping来辅助。&#xA;整个Tone Mapping的过程就是首先要根据当前的场景推算出场景的平均亮度，再根据这个平均亮度选取一个合适的亮度域，再将整个场景映射到这个亮度域得到正确的结果。其中最重要的几个参数：&#xA;Middle grey：整个场景的平均灰度，关系到场景所应处在亮度域。&#xA;Key：场景的Key将决定整个场景的亮度倾向，倾向偏亮亦或是偏暗。&#xA;首先我们需要做的是计算出整个场景的平均亮度，有很多种计算平均亮度的方法，目前常用的的是使用log-average亮度来作为场景的平均亮度，通过下面的公式可以计算得到：&#xA;其中Lw(x,y)是像素点x,y的亮度，N是场景内的像素数，δ是一个很小的数用来应对像素点纯黑的情况。&#xA;上面的公式用来映射亮度域，α即是前面所讲的Key值，用来控制场景的亮度倾向，一般来说，会使用几个特定的值，0.18是一个适中的Key，0.36或者0.72相对偏亮，0.09甚至0.045则是偏暗。完成映射的场景为了满足计算机能显示的范围还要将亮度范围再映射到[0,1]区间，可以通过下面的公式简单的得到[0,1]区间的亮度。&#xA;不过这样得到的结果并不总是令人满意的，所以一般扩展为如下面的公式，公式中的参数Lwhite用来控制场景中的曝光，凡是亮度超过Lwhite的像素都会被置为纯白。如果Lwhite的值非常大，则这个参数在公式中将不起任何作用，如果非常小则场景将变为几乎全白。Ld即为我们所要的映射后的x,y像素点的亮度值。&#xA;Tone Mapping一般作为HDR算法中的一部分存在，在使用中会灵活很多，但基本的原理都是相同的。</description>
    </item>
    <item>
      <title>柔和材质SoftShader (Maya节点)</title>
      <link>https://anwangtanmi.github.io/posts/2a2a4bd112d94cb8959a0320ad0e0330/</link>
      <pubDate>Thu, 01 Dec 2011 21:01:34 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2a2a4bd112d94cb8959a0320ad0e0330/</guid>
      <description> 看料理鼠王的动作预览时，感觉他们的画面很柔和，没有死黑，暗部细节很自然，不像环境光那样平，而且光源很统一，没有错乱的明暗交界线，下午，实现了一下，3delight的shaderdl出了点问题不能编译rsl了，于是继续用Maya节点搞定…&#xA;原理 原理很简单，简单说下吧，说多了有被众神笑话的危险。&#xA;普通的Lambert是根据Shader表面法线和光线的点积计算亮度，结果小于0时截取到0，&#xA;C=clamp(0,1,(L dot N))&#xA;所谓的SoftShader就是以能显示更多的细节为目的，以失去写实度为代价，稍微改进了下，将点积的结果[-1,1]转换到了[0,1]，这样原本小于0被截取的细节重新归一化后就成为了0~0.5之间的值，而原本0~1的亮部压缩到了0.5~1.&#xA;C=(L dot N)*.5+.5&#xA;实现 以下是Maya的实现节点网络：&#xA;说明 主要路线为灯光方向向量和法线向量点乘，输入到ramp的v坐标，从而调节ramp的亮度分布可以任意调节shader的颜色。&#xA;黄色区域使用相机的矩阵把camera space的法线转化为world space。&#xA;蓝色区域为取得灯光的阴影，调节亮度后输入到ramp的color gain来形成阴影。&#xA;里面一个locator，受灯光旋转约束，用来取得方向向量。&#xA;对比 左为默认lambert，暗部死黑&#xA;中为普通lambert，调亮ambient，虽然暗部变亮，但毫无细节&#xA;右为软软的SoftShader，很好的利用了0~1的范围&#xA;加Bump后更明显：&#xA;同样是一盏灯，只有右图的暗部能看到细节。&#xA;测试 上图开启阴影，Maya Software渲染，高抗锯齿，时间七八秒，场景只有1平行光，但效果不错，有点全局光的味道。 </description>
    </item>
    <item>
      <title>图像LUT</title>
      <link>https://anwangtanmi.github.io/posts/17973864cff521afc62d0d2acee511a6/</link>
      <pubDate>Wed, 30 Nov 2011 10:31:38 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/17973864cff521afc62d0d2acee511a6/</guid>
      <description> 显示&#xA;查找表（Look-Up-Table)简称为LUT，LUT LUT&#xA;本质上就是一个RAM。它把数据事先写入RAM后，每当输入一个信号就等于输入一个地址进行查表，找出地址对应的内容，然后输出。 LUT(Look-Up Table)实际上就是一张像素灰度值的映射表，它将实际采样到的像素灰度值经过一定的变换如阈值、反转、二值化、对比度调整、线性变换等，变成了另外一个与之对应的灰度值，这样可以起到突出图像的有用信息，增强图像的光对比度的作用。很多PC系列卡具有8/10/12/16甚到32位的LUT，具体在LUT里进行什么样的变换是由软件来定义的。 LUT:Local User Terminal 本地用户终端（通信用语） 自从桌面出版走到计算机上，显示器成为印前流程中重要的设备之一。从扫描、数码摄影、相片修改、排版等工序中，也可找到显示器的存在。但往往很多同业却忽略了显示器的一环，但求有影像，不理会画面的颜色是否正确。知否一个颜色准确（或比较准确）的显示器可用作SoftProofing（需另加印刷的ICCProfile），帮助员工提高产品的色彩质素，也可替公司节省成本。幸好近年色彩管理渐渐流行，显示器调校也开始被正视。 </description>
    </item>
    <item>
      <title>让你变成ps高手</title>
      <link>https://anwangtanmi.github.io/posts/661b7daa09e2c4b0c6d5c33d4ea16bd0/</link>
      <pubDate>Sat, 29 May 2010 16:24:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/661b7daa09e2c4b0c6d5c33d4ea16bd0/</guid>
      <description>1.复制图层，混合模式为滤色， 2.涂层2蒙板反向擦出眼睛 3.盖印（alt+ctrl+shift+E） 4.通道混合气，选中单色，红色+54，绿色+54，蓝色+32 6.混合模式-正叠 7.对盖印层调整，图像-调整-暗影/高光-暗影：50，50，144 高光：0，50，30 中间调：20，0 0.01，0.01 8.再次盖印 9.用图章擦去右腮的阳光 10.用减淡工具-范围为高光，强度50%，擦出眼球反光，只擦眼黑， 11.再次盖印，滤镜-其他-高反差，参数80 12.改高反差那一层模式为柔光，加强对比， 13.用自己喜欢的方法锐化 14.长时间少色相饱和的红色，或别的微调试试&#xA;1、选择素材图。 2、创建新画布，背景填充黑色，并将人物素材拖到画布中，使用橡皮擦工具将脸部之外的部分擦除。 3、创建新透明画布（20 x 20px），将前景色设为白色，使用1px的铅笔工具对画布左侧和下侧描边。 4、编辑 &amp;gt;&amp;gt; 变换 &amp;gt;&amp;gt; 变形，按下图调整网格层节点。 5、将图层不透明度设为20%。双击网格层打开图层样式窗口，选择外发光，混合模式为“颜色减淡”，不透明度100%，颜色为白色，扩展0，大小5。 6、创建新图层，使用钢笔工具沿网格选出若干方块，并用黑色填充。 7、按住Ctrl 键并点击上述黑色方块层获得选区，选择人像层，剪切并粘贴（粘贴后的图层命名为“碎片”），并通过自由变换（Ctrl + T）将其缩小一点后移动到如下位置。&#xA;8、选择碎片层，按住 Alt 键，并同时点击键盘向右方向键（点9次），得到类似立体效果。此时看到图层面板中出现10个碎片层，将除了顶层外的其他碎片层合并，并将合并后的图层命名为“侧边”。双击侧边层，打开图层样式窗口，选择渐变叠加，将不透明度设为80%，角度180，使用#000000 到 #D58761渐变，其他属性取默认值。 9、利用画笔工具为侧边添加一些纹理效果。 10、重复执行步骤 6-步骤 9，得到如下效果。 11、将所有的黑色方块层合并，双击打开图层样式窗口，选择斜面与浮雕，样式为“内斜面”，深度“75%”，大小5px，软化0px，角度0，高度50。高光模式为“滤色”，高光颜色#DCA57E，不透明度100%。 阴影模式“正片叠底”，阴影颜色#000000，不透明度75%。 12、在碎片层下方创建新图层，使用烟雾笔刷添加烟雾效果。选择碎片层，使用黑色柔角笔刷描绘画布右侧，得到如下渐隐效果。 13、在人物层上方创建新图层，再次使用钢笔工具随意选出若干方块，并用黑色填充。将图层混合模式设为“柔光”。 14、操作与步骤 13相同，只是填充颜色换成白色。 15、选择人物层，滤镜 &amp;gt;&amp;gt; 模糊 &amp;gt;&amp;gt; 表面模糊，半径5px，阀值15。在所有图层上方创建调整图层，图层 &amp;gt;&amp;gt; 创建调整图层 &amp;gt;&amp;gt; 色阶，属性设为10，1.25，244。 16、为了得到酷酷的感觉，可以在胳膊上添加条形码纹身。&#xA;1.新建灰色涂层，用曲线调整亮度，152，124 2.抠出眼睛和嘴，做个强调， 3.降低整体饱和度，饱和度-56 4.用颜色叠加给整体颜色倾向， 5.隐藏色相和颜色叠加两层，，打开通道面板，复制R通道， 6.新建图层粘贴，并用滤色模式调整透明度&#xA;1.复制图层,新建通道混合调整图层,灰色通道，红色+64%，绿色+2%，蓝色+10%，单色 2.新建纯色混合图层，选择575046颜色，混合模式为颜色， 3.新建曲线调整图层，①-输入68，输出32，②-输入114，输出68，③-输入208，输出197， 4.新建一纯色图层，选择443B25颜色，混合模式为颜色，按住Alt单击蒙板， 5.Ctrl+D恢复默认的背景颜色，Ctrl+Delete给蒙版填充黑色的背景色， 6.改变画面，中高光与暗部过渡部分的色调，以丰富画面的色彩，用画笔在蒙板 插出需要上色的部分，300像素，不透明度10%，流量100% 7.</description>
    </item>
    <item>
      <title>室外地形技术概览</title>
      <link>https://anwangtanmi.github.io/posts/04e2f3b9d02f793a95564132f6be3635/</link>
      <pubDate>Wed, 11 Jun 2008 20:44:00 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/04e2f3b9d02f793a95564132f6be3635/</guid>
      <description> 1.高度图 1．高度图代表了地形网格上每个顶点的高度值。 2．地形实时生成，所以会有一张高度图。 3．高度图级别：一个字节0－255级别，两个字节0－65535级别。为了节省资源，不同场景采用不同的级别。 4．地形网格顶点的法线可以通过周边顶点采样计算，也可以采用斜坡算法(即采用相邻四个顶点的高度及算法的k值计算当前顶点法线。K值需要手工调整，以达到较好的明暗效果。) 5.大家注意到了，高度图只纪录Z值，这样的地形是不能调整xy值做出象山这样的地形。不过目前我也没看到可以支持调整xy值的网游或编辑器。 2. 地形瓷砖 单位编辑块。一个大的地形是由无数个瓷砖组成的。高度图及材质最终都是应用到地形瓷砖上。因为硬件的限制,单个提交的地形Patch顶点数不能超过65535。我们的设计是一个Patch=33*33个顶点，顶点距为１米，当然顶点距也可以是２米但地形就不细腻了。 3.地形材质 1．纹理混合 一般做法采用多张贴图＋1张混合贴图，用混合贴图的rgba四个通道依次混合。这是目前网游比较通用的做法。魔兽是最多５张(4张地形纹理(ａ为高光通道)+１张混合纹理(使用rgb通道，ａ通道用于标示阴影)。 地形也可以支持高光，只要在纹理Alpha通道上加入高光信息再混合即可。只是对美术要求比较高。像魔兽世界里的藤条路、石头路等都是使用高光之后才逼真了许多。 建议不要使用NormalMap，因为地形会占大部分的显示区域，对PS的压力太大渲染效率会明显降低。 2．美术制作地形贴图时，必须是无缝的。 3．地形自阴影 4．地形静态物体阴影。 5．不仅仅是对应图形上的“材质”概念，还包含地形其他属性。如每层Surface对应的声音属性、自动出现的植被，角色在该地形上行走时是否留下脚印等。 通过上面地形材质需实现的效果可知，固定管线是满足不了需求的，需要自己编写多个相对于的PS/VS。 第１、3、４点找时间做专题介绍。 4. 地形LOD 简单的说LOD在需要的时候使用较多的多边形，而在不需要的时候使用较少的多边形。 ROAM：实时最优适配网格算法。 VDPM：基于观察点的渐进网格。 Geomipmap：将地形分成Ｎ块,然后每块都有几个不同的lod级别的顶点索引，渲染时根据距离进行lod选择。要注意避免产生裂缝。 我们目前实现的就是GeoMipMap+游戏编程精粹2《使用联锁分片简化地形》算法，这也是Fracry采用的算法。顶点索引预先计算好，运行时根据地形Patch离相交的距离调整LOD，但相邻Patch之间的LOD级别相差不会超过1。 地形Patch最好采用三角形带，不要用三角形列表。这样会有个明显的好处就是降低了索引缓冲所占用的内存，速度上也会有所提升。 5. 地表装饰物 这是个很大的话题，等有了更深入的了解实现再来探讨。 6. 参考资料： 1.Focus.On.3D.Terrain.Programming 这是做地形必看的一本书，基本上覆盖了室外地形相关技术。 2.游戏编程精粹2《使用联锁分片简化地形》 3.魔兽世界地形Shader。用MyWarCraftStudio打开WOW的misc.mpq包，shader / pixel / 目录下以”terrain”打头的bls文件就是地形渲染使用的shader，带有”_s”后缀的是带高光的渲染，否则就是不带高光的。 4.http://www.cognigraph.com/ROAM_homepage/ 5.Farcry SandBox 6.战地1942编辑器 </description>
    </item>
  </channel>
</rss>
