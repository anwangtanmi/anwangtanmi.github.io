<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 27 May 2019 10:38:59 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI：问题思考</title>
      <link>https://anwangtanmi.github.io/posts/2e2b3381174d34233a6ef1e2f1c4bc6f/</link>
      <pubDate>Mon, 27 May 2019 10:38:59 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/2e2b3381174d34233a6ef1e2f1c4bc6f/</guid>
      <description> 人类在识别物体的时候也是分层次进行识别的，如果物体是一个比较简单的易于识别的物体，那么可能很快就会被识别出来，但是一个比较复杂的物体（比如被遮挡的物体，或者是明暗不明显的物体）可能需要进行仔细的观察然后才能识别出物体的具体信息。&#xA;该想法是否可以应用到AI领域（卷积神经网络中），即在卷积神经网络的不同层对已经识别的特征进行输出，这样可以减少网络的计算量，如果在较浅层无法输出清晰正确的结果，那么可以在深层继续进行输出。 在图片分类中采用线性分类，更多的是一种对颜色敏感的分类模型，如果是采用灰度图像，那么线性分类器的效果可能会很差。 对图片的线性分类中的权重矩阵W，可以取出每一行的数据，将其还原成和原始图片大小相同的图片，可以还原出和分类数量相同的图片数量。还原的每一张图片表示其分类中的颜色权重值。 正则化的作用：&#xA;a. 增加模型的泛化作用&#xA;b. 对权重矩阵进行特征选择，L2可以让权重矩阵更加的平局，L1可以获得稀疏的权重空间，本质上正则化就是权重矩阵W解空间的一个选择器 ，在同样满足条件的W空间中，选取具有想要特征的W值 比较不同损失函数之间的区别&#xA;softmax 加交叉熵的损失&#xA;SVM hinge loss&#xA;softmax 会考虑到所有的输出数值，而svm只会考虑到和真值相聚较小的数值（相距较大的数值被忽略，不会对损失造成影响） softmax计算损失的时候，为什么需要对数化目标概率&#xA;为了更加方便的计算，同时可以将损失缩放还原到一个相对合理的计算空间中。&#xA;也是为了数学上的优美 </description>
    </item>
    <item>
      <title>matlab中关于矩阵数组数据图像化的方法总结</title>
      <link>https://anwangtanmi.github.io/posts/cba91888f5081de350361ff7a8c84262/</link>
      <pubDate>Wed, 24 Apr 2019 10:10:07 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/cba91888f5081de350361ff7a8c84262/</guid>
      <description>应用目的&#xA;1、材料研究中试样接头处不同部位的硬度是不一样的，有时候为了形象的表示该现象，需要将不同位置的硬度数据图像化。&#xA;2、能谱面扫描时，牛津仪器给出的图像时依据明暗调节的，图像并不好看，如果可以保留原始数据，zaimatlab中将明暗做成云图将会让结果非常好看。&#xA;源数据类型&#xA;与位置相关的数据。可以理解为一个大的二维矩阵，每个位置存在不同的数据。&#xA;方法及命令&#xA;imshow等。&#xA;原始数据&#xA;。。。。。。。。。。。。。。。&#xA;方法0：最笨的方法&#xA;X=[对应每个点的横坐标,,,,,,,,,,]&#xA;Y=[对应每个点的纵坐标,,,,,,,,,,]&#xA;Z=[对应每个点的具体数值,,,,,,,,,]&#xA;fill(X,Y,Z)&#xA;shading interp;&#xA;colorbar;&#xA;axis equal;&#xA;方法1：&#xA;首先需要导入excel数据并保存呈mat格式&#xA;将mat格式打开并采用table2array命令转换成矩阵&#xA;然后采用imshow命令&#xA;load(hardness.mat)&#xA;A=table2array(hardness) %该函数可以将table数据变为数组，即double.&#xA;imshow(A)&#xA;然而存在如下问题&#xA;1 图片为白色；2 图片为灰度图&#xA;在matlab中，我们常使用imshow()函数来显示图像，而此时的图像矩阵可能经过了某种运算。在matlab中，为了保证精度，经过了运算的图像矩阵A其数据类型会从unit8型变成double型。如果直接运行imshow(A)，我们会发现显示的是一个白色的图像。这是因为imshow()显示图像时对double型是认为在0~1范围内，即大于1时都是显示为白色，而imshow显示uint8型时是0~255范围。而经过运算的范围在0-255之间的double型数据就被不正常得显示为白色图像了。&#xA;即使采用采取各个数据与最大值相除，然而=1的位置仍为白色，因此imshow命令并不合适&#xA;imshow(B,[])等效于C=B/max(max(B)) imshow(C,[])&#xA;方法2：&#xA;linspace是Matlab中的一个指令，用于产生指定范围内的指定数量点数，相邻数据跨度相同，并返回一个行向量。&#xA;调用方法：linspace(x1,x2,N)&#xA;功 能：用于产生x1，x2之间的N点行矢量，相邻数据跨度相同。其中x1、x2、N分别为起始值、终止值、元素个数。若缺省N，默认点数为100。&#xA;举个例子 A=linspace（-6，6，4）&#xA;运行结果如下：A=-6 -2 2 6&#xA;意思就是 -6为起点 6为终点 4指向量的个数 且是均匀的分段的。&#xA;如在命令窗口中输入：&#xA;X=linspace(5,100,20)&#xA;将输出：&#xA;X =&#xA;5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100</description>
    </item>
    <item>
      <title>全局低照度图像增强matlab</title>
      <link>https://anwangtanmi.github.io/posts/4e43086d91860364afb9b04f79cefdd6/</link>
      <pubDate>Sun, 20 Jan 2019 18:51:05 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4e43086d91860364afb9b04f79cefdd6/</guid>
      <description>clear; clc; close all; %读入图片 A=imread(&#39;before.png&#39;); % A= rgb2gray(A); % mean2(A) % std2(A) %显示源图片 figure ; imshow(A); title(&#39;RGB Original Image&#39;) %调用ALTM outval = ALTM(A); % mean2(outval) % std2(outval) %输出目标图像 figure ; imshow(outval); %图像增强函数 function outval = ALTM(I) II = im2double(I); Ir=double(II(:,:,1)); Ig=double(II(:,:,2)); Ib=double(II(:,:,3)); % % % % Global Adaptation % input world luminance values Lw = 0.299 * Ir + 0.587 * Ig + 0.114 * Ib; % Lw = im2double(I); % % % the maximum luminance value Lwmax = max(max(Lw)); [m, n] = size(Lw);%[]矩阵表示 % % % log-average luminance Lwaver = exp(sum(sum(log(0.</description>
    </item>
    <item>
      <title>MATLAB中如何设置网格线的颜色，透明度，粗细等</title>
      <link>https://anwangtanmi.github.io/posts/9b9666a59cf6e026110f45d59196ce88/</link>
      <pubDate>Fri, 07 Dec 2018 16:29:53 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/9b9666a59cf6e026110f45d59196ce88/</guid>
      <description> 在MATLAB的命令窗中输入：&#xA;help grid 打开grid的帮助页，最下面会有各种属性的设置命令：&#xA;从图中可见，GridColor可以调整颜色，GridAlpha可以调整网格线颜色深浅（也可以说是透明度），LineWidth可以调整线宽等等。&#xA;使用举例：&#xA;set(gca,&#39;ygrid&#39;,&#39;on&#39;,&#39;gridlinestyle&#39;,&#39;--&#39;,&#39;Gridalpha&#39;,0.4) </description>
    </item>
    <item>
      <title>MATLAB中的plot()函数</title>
      <link>https://anwangtanmi.github.io/posts/54bcb8976f645657763a8f7ca69f6770/</link>
      <pubDate>Thu, 15 Nov 2018 16:38:57 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/54bcb8976f645657763a8f7ca69f6770/</guid>
      <description>xk=315:0.01:1575; temp=157.5; a=temp ./xk; plot(xk,a); %hold on %plot(xk,a); %hold off grid minor; axis([250 1650 0 0.6]); xlabel(&#39;5级暗纹的距离&#39;); ylabel(&#39;a的距离&#39;); grid minor; 添加细密的网格线&#xA;grid on；添加网格线&#xA;axis([xmin xmax ymin ymax]); 设置最大最小的xy坐标范围&#xA;xlabel(‘5级暗纹的距离’); 设置xy坐标的名称&#xA;ylabel(‘a的距离’); hold on 继续向plot()函数图像上添加图像&#xA;hold off 结束这个动作</description>
    </item>
    <item>
      <title>MNIST手写数字数据集的读取，基于python3</title>
      <link>https://anwangtanmi.github.io/posts/f13b59db7aa8426e3fc35e2be345f277/</link>
      <pubDate>Sun, 23 Sep 2018 18:20:15 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f13b59db7aa8426e3fc35e2be345f277/</guid>
      <description>MNIST 是一个入门级别的计算机视觉数据库，也是机器学习领域最有名的数据集之一。当我们开始学习编程的时候，第一件事往往就是打“Hello world”。而在机器学习中，识别 MNIST 就相当于编程中的“Hello world”。&#xA;MNIST 中包含了手写数字0~9的图片以及他们对应的标签。如下图所示：&#xA;MNIST 数据集的官网是http://yann.lecun.com/exdb/mnist/，我们可以从这里手动下载数据集。&#xA;你可以在官网上下载到下面四个压缩包：&#xA;train-images-idx3-ubyte.gz train-labels-idx1-ubyte.gz t10k-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz 解压后得到四个文件：&#xA;训练集图像：t10k-images.idx3-ubyte 训练集标签：t10k-labels.idx1-ubyte 测试集图像：train-images.idx3-ubyte 测试集标签：train-labels.idx1-ubyte MNIST数据集文件格式说明 训练集中有60,000个样本，测试集中有5,000个样本。所有图像都被标准化为28*28像素，像素值在0~255之间，0表示背景，255表示前景。&#xA;文件格式说明（以训练集为例）： 图片文件格式说明： ---------------------------------------- [字节位置] [类型] [值] [描述] 0000 32位整型 2051 幻数 0004 32位整型 60000 图片数 0008 32位整型 28 行数 0012 32位整型 28 列数 0016 无符号字节 ?? 像素 0017 无符号字节 ?? 像素 ...... xxxx 无符号字节 ?? 像素 ---------------------------------------- 标签文件格式说明： ---------------------------------------- [字节位置] [类型] [值] [描述] 0000 32位整型 2049 幻数 0004 32位整型 60000 标签数 0008 无符号字节 ?</description>
    </item>
    <item>
      <title>MATLAB打开USB摄像头的操作以及常见问题</title>
      <link>https://anwangtanmi.github.io/posts/d074dee0204201e5f9894a7eba3b5a32/</link>
      <pubDate>Mon, 30 Jul 2018 15:48:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d074dee0204201e5f9894a7eba3b5a32/</guid>
      <description>1 前言 2 打开USB摄像头并写视频 2.1 摄像头相关操作 2.2 写视频相关操作 3 查询摄像头设备信息指令 4 常见问题 1 前言 如果你是视频技术处理研究领域的工程人员，那么用MATLAB&amp;amp;OpenCV打开摄像头，以及获取相关信息，是再常规不过的操作了。&#xA;本文是对MATLAB打开USB摄像头操作的知识点梳理和总结。&#xA;操作环境&#xA;MATLAB 2015b Logitech HD720P Windows 10 Enterprise 64Bit 2 打开USB摄像头并写视频 2.1 摄像头相关操作 video_source = videoinput(&#39;winvideo&#39;,1); preview(video_source); 这两行指令就可以打开USB摄像头，并预览显示了，当然，这里采用的都是默认参数，如果想控制视频参数，可以通过set方法。&#xA;video_source = videoinput(&#39;winvideo&#39;,1，‘RGB24_640x480’); set(video_source,&#39;ReturnedColorSpace&#39;,&#39;rgb&#39;); preview(video_source); 此时打开摄像头成功，会有一个弹窗出现显示实时的画面。&#xA;如果想在某一时刻获取视频帧，可以这样操作&#xA;frame = getsnapshot(video_source); image(frame); 获取的帧是从视频输入流中直接获取，可以通过时间函数来控制获取的频率，获取的帧可以存储在一个矩阵当中，作为一个视频流，方便计算。&#xA;2.2 写视频相关操作 打开摄像头，更多的操作是保存一段视频，这要用到一个保存操作。&#xA;video_source = videoinput(&#39;winvideo&#39;,1,&#39;RGB24_640x480&#39;); set(video_source,&#39;ReturnedColorSpace&#39;,&#39;rgb&#39;); preview(video_source); file_name = &#39;test.avi&#39;; % 创建一个写文件的对象 writer = VideoWriter(file_name,&#39;Motion JPEG AVI&#39;); writer.FrameRate = 30.0; % 通过总帧数来控制视频长度 length = 300; % MATLAB里的视频文件其实是包含了两个部分的复合结构体， % 这两个部分，一个是色域colormap，一个是视频数据内容 file.</description>
    </item>
    <item>
      <title>2017CS231n李飞飞深度视觉识别笔记（一）——计算机视觉概述和历史背景</title>
      <link>https://anwangtanmi.github.io/posts/a2f668f631d19c6a131166bd31a03eda/</link>
      <pubDate>Mon, 09 Jul 2018 20:26:35 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/a2f668f631d19c6a131166bd31a03eda/</guid>
      <description>第一章 计算机视觉概述和历史背景&#xA;课时1 计算机视觉概述&#xA;计算机视觉：针对视觉数据的研究。&#xA;关键是如何用算法来开发可以利用和理解的数据，视觉数据存在的问题是它们很难理解，有时把视觉数据称为“互联网的暗物质”，它们构成了网络上传输的大部分数据。&#xA;根据YouTube的一个统计实例：大概每秒钟，有长达5小时的数据内容会被上传到YouTube，所以通过人工给每个视频标上注释、分类是非常困难甚至不可能的，计算机视觉是解决这种问题的重要技术，它能够对照片进行标签、分类，处理视频的每一帧。&#xA;计算机视觉是一个与很多领域紧密关联的学科，它涉及到比如说工程、物理、生物等许多不同的领域：&#xA;对于CS231n这么课程，它专注于一类特定的算法，围绕神经网络，特别是卷积神经网络，并将其应用于各种视觉识别任务。&#xA;课时2 计算机视觉历史背景&#xA;视觉的历史可以追溯到很久以前，动物拥有视觉的开端：&#xA;如今，视觉成为了最重要的感知系统，人类的大脑皮层中有几乎一半的神经元与视觉有关，这项最重要的感知系统可以使人们生存、工作、运动等等，视觉对人们真的至关重要。&#xA;以上谈到了人类的视觉，那么人类让计算机获得视觉的历史又是怎么样的呢？&#xA;现在知道的最早的相机追溯到17世纪文艺复兴时期的暗箱，这是一种通过小孔成像的相机，这和动物早期的眼睛非常相似，通过小孔接收光线，后面的平板手机信息并且投影成像。&#xA;同时，生物学家开始研究视觉的机理，最具影响力并且启发了计算机视觉的一项研究是在五六十年代，休伯尔和威泽尔使用电生理学的研究，他们提出了“哺乳动物的视觉处理机制是怎样的”，通过观察何种刺激会引起视觉皮层神经的激烈反应，他们发现猫的大脑的初级视觉皮层有各种各样的细胞，其中最重要的是当它们朝着某个特定方向运动时，对面向边缘产生回应的细胞。&#xA;他们发现视觉处理是始于视觉世界的简单结构，面向边缘，沿着视觉处理的途径的移动信息也在变化，大脑建立了复杂的视觉信息，直到它可以识别更为复杂的视觉世界。&#xA;计算机视觉的历史是从60年代开始，从Larry Roberts的计算机视觉的第一篇博士论文开始。&#xA;1966年，一个如今非常著名的MIT暑期项目“Summer Vision Project”，它试图有效的使用暑期工作时间来构建视觉系统的重要组成部分，五十年来，计算机视觉领域已经从哪个夏季项目发展成为全球数千名研究人员的领域，并且仍然处理一些最根本的问题，这个领域已经成长为人工智能领域最重要和发展最快的领域之一。&#xA;70年代后期David Marr撰写的一本非常有影响力的书，内容包括了他是如何理解计算机视觉和应该如何开发可以使计算机识别世界的算法，他指出了为了拍摄一幅图像并获得视觉世界的最终全面3D表现必须经历的几个过程，如下图所示：&#xA;这是一个非常理想化的思想过程，也是一个非常直观化的方式并考虑如何解构视觉信息。&#xA;70年代另一个重要的开创性问题：如何越过简单的块状世界并开始识别或表示现实世界的对象？&#xA;一个被称为“广义圆柱体”，一个被称为“图形结构”，他们的基本思想是每个对象都是由简单的几何图形单位组成，所以任何一种表示方法是将物体的复杂结构简约成一个集合体。&#xA;80年代David lowe思考的如何重建或识别由简单的物体结构组成的视觉空间，它尝试识别剃须刀，通过线和边缘进行构建，其中大部分是直线之间的组合。&#xA;从60年代到80年代，考虑的问题是计算机视觉的任务是什么，要解决物体识别的问题非常难。所以，当思考解决视觉问题过程中出现的问题时，另一个重要的问题产生：如果识别目标太难，首先要做的是目标分割。&#xA;这个任务就是把一张图片中的像素点归类到有意义的区域，可能不知道这些像素点组合到一起是一个人形，但可以把属于人的像素点从背景中抠出来，这个过程就叫作图像分割。&#xA;下面是Malik和Jianbo shi完成的用一个算法对图像进行分割：&#xA;还有一个重要的研究是由Paul Viola和Michael Jones完成的，使用AdaBoost算法进行实时面部检测，在这个研究后不久推出了第一个能在数码相机中实现实时面部检测的数码相机，所以这是从基础科学研究到实际应用的一个快速转化。&#xA;关于如何做到更好的目标识别，是可以继续研究的领域，，所以在90年代末和21世纪的前几年，一个重要的思想方法就是基于特征的目标识别。由David Lowe完成的叫做SIFT特征，思路是匹配整个目标。&#xA;通过观察目标的某些部分、某些特征，它们往往能够在变化中具有表现性和不变性，所以目标识别的首要任务是在目标上确认这些关键的特征，然后把这些特征与相似的目标进行匹配，它比匹配整个目标要容易的多。例如，上图中一个stop标识中的SIFT特征与另一个stop标识中的SIFT特征相匹配。&#xA;有些工作是把这些特征放在一起以后，研究如何在实际图片中比较合理地设计人体姿态和辨认人体姿态，这方面一个工作被称为“方向梯度直方图”，另一个被称为“可变部件模型”。&#xA;所以，从60年代、70年代、80年代一直到21世纪，图片的质量随着互联网的发展，计算机视觉领域也能拥有更好的数据了，直到21世纪早期，才开始真正拥有标注的数据集能够衡量在目标识别方面取得的成果，其实一个最著名的数据集叫做PASCAL Visual Challenge。&#xA;与此同时，提出了一个重要的问题：是否具备了识别真是世界中的每一个物体的能力或者说大部分物体。这个问题也是由机器学习中的一个现象驱动：大部分的机器学习算法，无论是图模型还是SVM、AdaBoost都可能会在训练过程中过拟合。因此，有这两方面的动力，一是单纯想识别自然界中的万物，二是要回归机器学习克服瓶颈—过拟合问题，开始开展了一个ImageNet的项目，汇集所有能找到的图片，组建一个尽可能大的数据集。&#xA;这是当时AI领域最大的数据集，将目标检测算法的发展推到了一个新的高度，尤其重要的是如何推动基准测试的进展。&#xA;下面是ImageNet挑战赛的从2010到2015的图像分类结果：&#xA;横轴表示年份，纵轴表示比赛结果的错误率，可以看到错误率正在稳步下降。可以看到图中2012的错误率下降的非常显著，这一年的算法是一种卷积神经网络模型，这也将是这门课程学习的重点，深入研究什么是卷积神经网络模型，也就是现在被熟知的深度学习。&#xA;课时3 CS321n课程概述&#xA;CS321n将聚焦于视觉识别问题，第一个主要问题就是图像分类问题：让算法接收一张图作为输入，从固定的类别集合中选出该图像所属的类别。这个基本的分类器在很多地方都有不同的应用。&#xA;在CS231n课程中，将讨论一些其他的视觉识别问题，它们都建立在专门为图像分类而开发的各种工具之上，一些和图像分类的问题，比如目标检测或图像摘要生成。&#xA;图像分类关注的是大图整体，目标检测则告诉你物体具体出现在图片的哪个位置以及物体之间的联系是什么，图像摘要是当给到一幅图像，需要生成一段句子来描述这幅图像。&#xA;CNN，卷积神经网络只是深度学习架构的一种，但是它的成功是压倒性的，成为了目标识别的重要工具。回到ImageNet挑战赛中，2012年Krizhevsky和他的导师提出了卷积神经网络，并夺得了冠军；而在这之前，一直都是特征+支持向量机的结构，一种分层结构；而在这之后，获得冠军的算法都是卷积神经网络。&#xA;然而，卷积神经网络并不是一夜之间就成功的，事实上，这些算法可以追溯到更早的时候，与卷积神经网络有关的其中一项基础性工作是由Yann LeCun和他的伙伴于90年代完成的，1998年他们利用卷积神经网络进行数字识别。&#xA;所有既然这些算法在90年代就很突出，为什么到最近几年才变得这么流行呢？从数学的角度来说，有很重要的两点引起了深度学习架构的复兴，一个是摩尔定律，计算能力在变得越来越高；另一个是数据，算法需要大量的数据，需要给它们提供非常多的带标签的图像和像素，以便能最终取得更好的效果，有了大数据集，可以实现更强大的模型。&#xA;在计算机视觉领域，正尝试着制造一个拥有和人类一样视觉能力的机器，这样可以利用这些视觉系统可以实现很多惊奇的事情，但是当继续在该领域深入的时候，仍然有着大量的挑战和问题亟待解决，比如对整个照片进行密集标记、感知分组、使能够确定每个像素点的归属，这些仍是研究中的问题，所以需要持续不断地改进算法，从而做到更好。&#xA;与简单的“在物体上贴标签”比起来，我们往往希望深入地理解图片中的人们在做什么、各个物体之间的关系是什么，于是我们开始探究物体之间的联系，这是一个被称为视觉基因组的项目。&#xA;计算机视觉领域的一个愿景即是“看图说故事”，人类的生物视觉系统是非常强大的，看到一张图片，就能够描述图片的内容，并且只需不到一秒种的时间，如果能够让计算机也能做的同样的事情，那毋庸置疑是一项重大的突破；如果要实现真实深刻的图像理解，如今的计算机视觉算法仍然有很长的路要走。&#xA;计算机视觉能让世界变得更加美好，它还可以被应用到类似医学诊断、自动驾驶、机器人或者和这些完全版不同的领域。</description>
    </item>
    <item>
      <title>DL学习笔记-图像预处理</title>
      <link>https://anwangtanmi.github.io/posts/ece1ca5941bf4f0ee95811e0bede0546/</link>
      <pubDate>Mon, 11 Jun 2018 16:43:58 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ece1ca5941bf4f0ee95811e0bede0546/</guid>
      <description>一、为什么使用图像预处理 1、图像的亮度、对比度等属性对图像的影响是非常大的，相同物体在不同的亮度，对比度下差别非常大。在图像识别的问题中，我们经常会遇到阴影、强曝光之类的图片，这些因素都不应该影响最后的识别结果，所以我们要对图像进行预处理，使得得到的神经网络模型尽可能小的被无关因素所影响。 2、在我们遇到图像样本过少，或者不均衡时，也可以使用图像预处理，增加样本数量。 3、有时物体拍摄的角度不同，也会有很大的差异，所以刻意将图像进行随机的翻转，可以提高模型健壮性。 二、图像处理函数 1、读取图像的原始数据。 图像在存储是并不是直接记录这些矩阵中的数字，而是记录经过压缩编码之后的结果，所以要将一个图片还原成三维矩阵，需要一个解码的过程。 image_raw_data = tf.gfile.FastGFile(&#39;images/image_0043.jpg&#39;, &#39;rb&#39;).read() img_data = tf.image.decode_jpeg(image_raw_data) 2、图像大小调整 图像大小调整有两种方法， 第一种是通过算法使得新的图像尽量保存原始图像上的所有信息。 第二种是对图像进行裁剪或者填充，只获得感兴趣区域。 TensorFlow提供了四种缩放图像的算法，注意要加入截断函数，防止数值越界。 #重新调整图片大小 def resize(image_data): with tf.Session() as sess: image_data = tf.image.convert_image_dtype(image_data, dtype=tf.float32) methods=[&#39;Bilinear Interpolation&#39;, &#39;Nearest neighbor interpolation&#39;, &#39;Bicubic interpolation&#39;, &#39;Area interpolation&#39;] plt.subplot(231), plt.imshow(image_data.eval()), plt.title(&#39;original&#39;) step = 231 for i in range(4): step += 1 resized = tf.image.resize_images(image_data, [300, 300], method=i) resized = tf.clip_by_value(resized, 0.0, 1.0) plt.subplot(step),plt.imshow(resized.eval()), plt.title(methods[i]) plt.show() 显示图像如下： 剪裁和填充图片，通过tf.image.resize_image_with_crop_or_pad函数实现，第一参数是输入图像，后面是裁剪以后的大小。如果小于输入图像，那么就裁剪输入图像居中部分的大小；如果大于输入图像，就填充输入图像四周。 #裁剪和填充图片 def crop_and_pad(image_data): with tf.</description>
    </item>
    <item>
      <title>先验算法（Apriori Algorithm）原理及python代码实现</title>
      <link>https://anwangtanmi.github.io/posts/6b1f3aed14c9caa77249bff4ae77aa63/</link>
      <pubDate>Thu, 31 May 2018 09:34:03 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/6b1f3aed14c9caa77249bff4ae77aa63/</guid>
      <description>先验算法（Apriori Algorithm）是关联规则学习的经典算法之一。先验算法的设计目的是为了处理包含交易信息内容的数据库（例如,顾客购买的商品清单，或者网页常访清单。）而其他的算法则是设计用来寻找无交易信息（如Winepi算法和Minepi算法）或无时间标记（如DNA测序）的数据之间的联系规则。关联分析的目的是从大规模数据集中寻找有趣关系的任务。这些关系可以有两种形式：频繁项集或者关联规则。频繁项集(frequent item sets)是指经常出现在一起的物品的集合，关联关系(association rules)暗示两种物品之间可能存在很强的关系。&#xA;先验算法采用广度优先搜索算法进行搜索并采用树结构来对候选项目集进行高效计数。它通过长度为 k-1的候选项目集来产生长度为k的候选项目集，然后从中删除包含不常见子模式的候选项。根据向下封闭性引理,该候选项目集包含所有长度为 k的频繁项目集。之后，就可以通过扫描交易数据库来决定候选项目集中的频繁项目集。&#xA;from __future__ import division, print_function import numpy as np import itertools class Rule(): def __init__(self, antecedent, concequent, confidence, support): self.antecedent = antecedent self.concequent = concequent self.confidence = confidence self.support = support class Apriori(): &#34;&#34;&#34;A method for determining frequent itemsets in a transactional database and also for generating rules for those itemsets. Parameters: ----------- min_sup: float The minimum fraction of transactions an itemets needs to occur in to be deemed frequent min_conf: float: The minimum fraction of times the antecedent needs to imply the concequent to justify rule &#34;</description>
    </item>
    <item>
      <title>AI强化学习-策略迭代实战</title>
      <link>https://anwangtanmi.github.io/posts/84bd1f8273826a6fd8d6abe053f82e62/</link>
      <pubDate>Tue, 29 May 2018 11:45:32 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/84bd1f8273826a6fd8d6abe053f82e62/</guid>
      <description>以下代码演示策略迭代强化算法&#xA;前提：&#xA;python语言 OpenAI gym库 主要演示AI自动寻路的算法。如图：&#xA;图中格子从做到右，从上到下依次编号，1~8.暗黄色的圆球，初始随机出现在1~5位置，在格子上移动。移动到黑色点失败，移动到黄色点胜利。&#xA;首先，写一个gym环境：grid_map.py，代码如下&#xA;import logging import numpy import random from gym import spaces import gym logger = logging.getLogger(__name__) class GridEnv(gym.Env): metadata = { &#39;render.modes&#39;: [&#39;human&#39;, &#39;rgb_array&#39;], &#39;video.frames_per_second&#39;: 2 } def __init__(self): self.states = [1,2,3,4,5,6,7,8] #状态空间 self.x = [140,220,300,380,460,140,300,460] self.y = [250,250,250,250,250,150,150,150] self.terminate_states = dict() #终止状态为字典格式 self.terminate_states[6] = 1 self.terminate_states[7] = 1 self.terminate_states[8] = 1 self.actions = [&#39;n&#39;,&#39;e&#39;,&#39;s&#39;,&#39;w&#39;] self.rewards = dict() #回报的数据结构为字典 self.rewards[&#39;1_s&#39;] = -1.0 self.</description>
    </item>
    <item>
      <title>机器学习、计算机视觉面经整理（持续完善整理中……）</title>
      <link>https://anwangtanmi.github.io/posts/c2f3e6f23abb8166a2d4067a7f73a8a5/</link>
      <pubDate>Sun, 15 Apr 2018 19:23:49 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c2f3e6f23abb8166a2d4067a7f73a8a5/</guid>
      <description>算法岗计算机视觉方向求职经验总结&#xA;进入11月份，楼主找工作也基本进入尾声了，从7月份开始关注牛客网，在求职的过程中学到了不少，感谢牛客提供这样一个平台，让自己的求职历程不再孤单。&#xA;先说一下楼主教育背景，本科西部末流985，研究生调剂到帝都某文科学校.专业都是CS专业，求职方向都是计算机视觉算法。有某外企以及二线互联网实习经历，本科虽然CS出身，但实际动手能力并不强。研究生的研究方向并不是计算机视觉方向。实习的时候开始接触计算机视觉，自己比较感兴趣，开始转CV方向。回想这几个月的求职经历，其中的辛苦只有自己知道。最终拿到了百度SP，京东SSP，美团无人驾驶SP，顺丰科技SP，拼多多SP，以及虹软SP，思科，中电29等offer。 想把我学习与求职路上的一些心得告诉学弟学妹们。 1.一定要有一门自己比较熟悉的语言。 我由于使用C++比较多，所以简历上只写了C++。C++的特性要了解，C++11要了解一些，还有STL。面试中常遇到的一些问题，手写代码实现一个string类，手写代码实现智能指针类，以及STL中的容器的实现机制，多态和继承，构造函数， 析构函数等。推荐看一下网易云课堂翁恺老师的C++的视频以及经典的几本书。 2.一定要刷题 楼主主要刷了剑指offer以及leetcode上的easy,middle的题目。如果编程能力不是很强，推荐可以分类型进行刷题，按照tag来刷，对于某一类型的题目，可以先看一下该算法的核心思想，然后再刷题。楼主在求职的过程中，遇到好多跟leetcode上类似的题目，刷题的目的不是为了碰见原题，而是为了熟练算法。当然能够碰见原题最好不过啦。 3.机器学习的一些建议 推荐西瓜书，以及李航老师的统计学方法。另外熟悉一种深度学习框架。学习计算机，一定要实战，毕竟只有在实战的过程中，才能懂得更透彻。可以多参加一些比赛，比如kaggle,天池，滴滴的一些比赛。这对找工作的用处很大。&#xA;4.能实习就尽量实习。 如果导师是学术大牛，可以带你发顶会的论文，并且自己对方向比较感兴趣，那可以在实验室待着好好搞科研。如果你研究生的研究方向跟你以后的求职方向不一致，建议早点出来实习，找个对口的实习，实习才能发现，实际工作和在学校学习的东西差距比较大。&#xA;楼主能不能分享下面试问了哪些视觉的问题啊&#xA;问到的问题主要跟我实习做的东西有关，有关于视频拆分的一些算法，以及三维点云的一些问题，传统的图像处理的一些基本操作还是要了解的，比如滤波，边缘检测，以及常用的一些传统的特征，SIFT,SURF，HOG等。深度学习这方面，我主要做过目标检测，所以问到的就是rcnn,fast-rcnn,faster-rnn,yolo,ssd这些算法。另外，问过一些调参，正则化，Batch normalization,drop out,激活函数的选择。手动推导BP,LR,SVM，算法题主要有常规的排序，二分查找，BP相关的题目，还有一些就是关于二叉树的递归和非递归遍历，层次遍历，最近公共祖先等，其余的题目记得不太清楚了&#xA;深度学习面经&#xA;（1）代码题（leetcode类型），主要考察数据结构和基础算法，以及代码基本功&#xA;虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。&#xA;大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free. 并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。&#xA;以下是我所遇到的一些需要当场写出完整代码的题目： &amp;lt;1&amp;gt; 二分查找。分别实现C++中的lower_bound和upper_bound. &amp;lt;2&amp;gt; 排序。 手写快速排序，归并排序，堆排序都被问到过。 &amp;lt;3&amp;gt; 给你一个数组，求这个数组的最大子段积 时间复杂度可以到O(n) &amp;lt;4&amp;gt; 给你一个数组，在这个数组中找出不重合的两段，让这两段的字段和的差的绝对值最大。 时间复杂度可以到O(n) &amp;lt;5&amp;gt; 给你一个数组，求一个k值，使得前k个数的方差 + 后面n-k个数的方差最小&#xA;时间复杂度可以到O(n) &amp;lt;6&amp;gt; 给你一个只由0和1组成的字符串，找一个最长的子串，要求这个子串里面0和1的数目相等。 时间复杂度可以到O(n) &amp;lt;7&amp;gt; 给你一个数组以及一个数K， 从这个数组里面选择三个数，使得三个数的和小于等于K， 问有多少种选择的方法？ 时间复杂度可以到O(n^2) &amp;lt;8&amp;gt; 给你一个只由0和1组成的矩阵，找出一个最大的子矩阵，要求这个子矩阵是方阵，并且这个子矩阵的所有元素为1 时间复杂度可以到O(n^2) &amp;lt;9&amp;gt; 求一个字符串的最长回文子串 时间复杂度可以到O(n) (Manacher算法) &amp;lt;10&amp;gt; 在一个数轴上移动，初始在0点，现在要到给定的某一个x点， 每一步有三种选择，坐标加1，坐标减1，坐标乘以2，请问最少需要多少步从0点到x点。 &amp;lt;11&amp;gt; 给你一个集合，输出这个集合的所有子集。 &amp;lt;12&amp;gt; 给你一个长度为n的数组，以及一个k值（k &amp;lt; n) 求出这个数组中每k个相邻元素里面的最大值。其实也就是一个一维的max pooling 时间复杂度可以到O(n) &amp;lt;13&amp;gt; 写一个程序，在单位球面上随机取点，也就是说保证随机取到的点是均匀的。 &amp;lt;14&amp;gt; 给你一个长度为n的字符串s，以及m个短串（每个短串的长度小于10），每个字符串都是基因序列，也就是说只含有A,T,C,G这四个字母。在字符串中找出所有可以和任何一个短串模糊匹配的子串。模糊匹配的定义，两个字符串长度相等，并且至多有两个字符不一样，那么我们就可以说这两个字符串是模糊匹配的。</description>
    </item>
    <item>
      <title>MATLAB surfl函数 surfc函数 效果图</title>
      <link>https://anwangtanmi.github.io/posts/e57c2f88d9b33d4c0532bf98b39eaf43/</link>
      <pubDate>Wed, 23 Aug 2017 18:00:30 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/e57c2f88d9b33d4c0532bf98b39eaf43/</guid>
      <description>function shili21 h0=figure(&#39;toolbar&#39;,&#39;none&#39;,... &#39;position&#39;,[200 100 450 450],... &#39;name&#39;,&#39;实例21&#39;); [x,y,z]=peaks(30); subplot(2,1,1) x=x(1,:); y=y(:,1); i=find(y&amp;gt;0.8&amp;amp;y&amp;lt;1.2); j=find(x&amp;gt;-0.6&amp;amp;x&amp;lt;0.5); z(i,j)=nan*z(i,j); surfc(x,y,z) xlabel(&#39;X轴&#39;); ylabel(&#39;Y轴&#39;); zlabel(&#39;Z轴&#39;); title(&#39;Figure1:surfc函数形成的曲面&#39;) subplot(2,1,2) x=x(1,:); y=y(:,1); i=find(y&amp;gt;0.8&amp;amp;y&amp;lt;1.2); j=find(x&amp;gt;-0.6&amp;amp;x&amp;lt;0.5); z(i,j)=nan*z(i,j); surfl(x,y,z) xlabel(&#39;X轴&#39;); ylabel(&#39;Y轴&#39;); zlabel(&#39;Z轴&#39;); title(&#39;Figure2:surfl函数形成的曲面&#39;) surfl画的三维曲面有光照效果 surfc画的三维曲面在曲面底部有等高线图&#xA;有关nan的介绍： http://jingyan.baidu.com/article/afd8f4de42c3ab34e286e9bf.html</description>
    </item>
    <item>
      <title>数字图像处理 – 图像分割 – 阈值处理</title>
      <link>https://anwangtanmi.github.io/posts/80e1b69f47c7ab6eb71751f592fd206c/</link>
      <pubDate>Sat, 08 Apr 2017 09:45:38 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/80e1b69f47c7ab6eb71751f592fd206c/</guid>
      <description>最近工作需要，开始学习图像处理啦。我们使用的软件是Adaptive Vision Studio。不过是收费的哟。无基础学习。所以先加强一下子基础啦咯 前期准备： 直方图 灰度直方图：不同灰度值的像素分量分别占像素总数的概率分布 p(rk) = nk/MN; rk：第k级的像素个数 MN：像素总数/行数 * 像素总数/列数 P(rk)：归一化概率直方图&#xA;图像分割 输入图像，提取图像属性 一张图像R，可以分割为不同区域R1，R2，…，Rn&#xA;阈值处理（区域分割） 灰度阈值处理基础 —–《数字图像处理》&#xA;粗略的计算一下，阈值T T= （对象的灰度值+背景的灰度值）/ 2</description>
    </item>
    <item>
      <title>【图像处理】Tensorflow:简易超分辨重建与坑</title>
      <link>https://anwangtanmi.github.io/posts/1c1e092adad079baccb5abc04ba452ce/</link>
      <pubDate>Fri, 31 Mar 2017 10:32:55 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/1c1e092adad079baccb5abc04ba452ce/</guid>
      <description>超分辨重建是图像复原领域的一大热点，能在硬件有限的情况下最大还原原始场景的信号，在天文探索、显微成像等领域有重要作用。成像设备对物体成像时，由于距离较远，成像会模糊，可以类比多尺度高斯滤波；受限于成像机能，成像像素达不到最理想条件，可类比为对原始像进行一个下采样。超分辨重建就是要在这种条件下复原原始图像。 假设上帝有最好的成像设备，成像为X；我们成像设备成像为B，高斯滤波模板设为G；为了防止问题病态，加入lasso正则。那么有：&#xA;argmin [subsampling(conv(X,G))−B]2+λX&#xA;现在的问题是，Tensorflow如何表示subsampling并进行优化？&#xA;Tensorflow支持以下几种图像缩放/采样:&#xA;tf.image.resize_images，支持最近邻、双线性、双三次等缩放方法 tf.nn.max_pool 最大值下采样 tf.nn.avg_pool 均值下采样&#xA;现在我们逐个测试一下。图像经过三倍下采样： 1、tf.image.resize_images，双线性采样，振铃不严重，条纹很多： 2、tf.nn.max_pool，没有条纹、振铃，但是有一堆噪声，参数调了几次都没有什么更好的效果： 3、tf.nn.avg_pool，无条纹、噪声，有振铃，与原图相比颜色变暗，对比度下降: 4、来与原图做个对比 可以看出，效果最好的就是avg_pool了，在只有高斯模板参数，完全没有其他先验信息的情况，一秒钟内得到这个结果，已经让人非常惊讶了。猜测image-resize和max_pool其实在上下采样中都丢失相当多的信号，而avg_pool则保留了最多的信号，因此重建效果较好。 fast-neural-style文章提到过用感知特征来对图像进行超分辨重建，可以重建同样风格的细节，这个需要用生成网络对大量的图像进行训练，或者直接上vgg慢慢地计算感知特征来仿制风格细节。</description>
    </item>
    <item>
      <title>Haar-like、HoG 、LBP 三种描述方法在目标识别中的优劣</title>
      <link>https://anwangtanmi.github.io/posts/7754d86800f9ad1910a32dbc8eea46fd/</link>
      <pubDate>Fri, 17 Feb 2017 10:54:42 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/7754d86800f9ad1910a32dbc8eea46fd/</guid>
      <description>Haar-like的优势在于能更好的描述明暗变化，因此用于检测正面的人脸&#xA;HoG的优势在于能更好的描述形状，在行人识别方面有很好的效果&#xA;LBP比haar快很多倍，但是提取的准确率会低（10-20% 取决于训练对象）如果是嵌入式或者移动端的开发，推荐使用LBP。 这也解释了为什么haar应用于人的正面检测要明显好于应用于侧脸检测：正脸由于鼻子等凸起的存在，使得脸上的光影变化十分明显。而侧脸侧脸最重要的特征是形状和轮廓。 所以用HoG描述符检测侧脸更加有效。&#xA;参考原文：&#xA;https://www.quora.com/Why-are-HOG-features-more-accurate-than-Haar-features-in-pedestrian-detection It’s important to look at the most prominent feature of pedestrians. There can be more than one prominent feature but the defining feature of a typical pedestrian is the outline, the legs and head shape. Hence the detection method that best captures or describes the pedestrian outline will ultimately solve the pedestrian detection problem more accurately. HoG features are capable of capturing the pedestrian or object outline/shape better than Haar features.</description>
    </item>
    <item>
      <title>骰子点数识别之图像分割</title>
      <link>https://anwangtanmi.github.io/posts/0e1508e3ad945b221557076073aa83c7/</link>
      <pubDate>Thu, 05 Jan 2017 13:55:51 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0e1508e3ad945b221557076073aa83c7/</guid>
      <description>链接1：利用卷积神经网络识别骰子点数&#xA;链接1：利用神经网络识别骰子点数&#xA;前言 前段时间借用神经网络和卷积神经网络实现了骰子点数的识别，但是一个很严重的问题一直困扰我，那就是当两个骰子叠在一起的时候，将两个骰子分开并不是一件简单的事情。&#xA;下图是我在识别过程中产生的不能识别的，叠加在一起的图片素材。&#xA;面对这些形态各异的图片，有的时候是两个骰子一个角连在一起，有的是一条边，有的是三个骰子叠在一起。所以，很难找到一个满意的办法解决这个问题。&#xA;第一思路就是从原始的RGB图像着手，通过调整二值化阈值，希望能够将骰子对象分割开来，但是遗憾的是我试了好几种方法，都是不行的，原因在于原图像在交接的地方本来就很模糊，颜色变化很小，所以使用二值化阈值调整很难得到完美的解决方案。&#xA;期间我尝试了不同的方法 1. 分水岭 close all clc figure(1) subplot(231) RGB_img=imread(&#39;161220S010129.jpg&#39;); imgsize =size(RGB_img); RGB_img = imcrop(RGB_img,[imgsize(1,2)*0.418 imgsize(1,1)*0.655 215 134]);%大部分图像布局固定 imshow(RGB_img) %% subplot(232) %imhist(A(:,:,1)); bw=im2bw(rgb2gray(RGB_img)); bw=medfilt2(bw); planes=bwareaopen(bw,100); imshow(planes) %% subplot(233) D=bwdist(imcomplement(planes)); D=mat2gray(D); imshow(D) figure subimage(D) hold on [C,h]=imcontour(D,0.2:0.2:0.8); set(h,&#39;ShowText&#39;,&#39;on&#39;,&#39;TextStep&#39;,get(h,&#39;LevelStep&#39;)*2) text_handle = clabel(C,h,&#39;color&#39;,&#39;g&#39;); figure(1) %% subplot(234) M=imimposemin(imcomplement(D),D&amp;gt;.8); imshow(M); %% subplot(236) L=watershed(M); r=L &amp;amp; planes; imshow(r) %%%%%%%%%%%% stats=regionprops(r,&#39;BoundingBox&#39;,&#39;Centroid&#39;); hold on c=cat(1,stats.Centroid); plot(c(:,1),c(:,2),&#39;r*&#39;) bb={stats.BoundingBox}; cellfun(@(x) rectangle(&#39;Position&#39;,x,&#39;EdgeColor&#39;,&#39;y&#39;),bb) %% subplot(235) L(r)=5; imshow(L,[]) 2.</description>
    </item>
    <item>
      <title>matlab使用摄像头</title>
      <link>https://anwangtanmi.github.io/posts/ed34f4a9840aafae5dfcd0d7aaf98aa5/</link>
      <pubDate>Mon, 28 Mar 2016 19:13:11 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/ed34f4a9840aafae5dfcd0d7aaf98aa5/</guid>
      <description>1.整个过程需要做如下几件事情： 1)查询USB2.0Camera 的具体参数(imaqhwinfo) 2)创建视频输入对象(videoinput) 3)图像预览和显示(preview、stoppreview、closepreview和image) 4)获取视频图像(getsnapshot) 5)图像获取设备的获取和设置(get和set) 6)关闭视频对象(delete) 2.图像获取工具箱术语： 图像获取设备：比如摄像头、扫描仪 图像获取适配器：主要的目的是通过驱动在Matlab和图像获取设备之间传递信息 ROI：region-of-interest 感兴趣区域 3.常用函数 1)getselectedsource 2)imaqfind 3)isvalid 4)peekdata 5)getdata 6)imaqmontage 4.查询USB2.0Camera 的具体参数(imaqhwinfo) &amp;gt;&amp;gt; info=imaqhwinfo info = InstalledAdaptors: {&#39;coreco&#39; &#39;winvideo&#39;} MATLABVersion: &#39;7.10(R2010a)&#39; ToolboxName: &#39;Image Acquisition Toolbox&#39; ToolboxVersion: &#39;3.5(R2010a)&#39; &amp;gt;&amp;gt; win_info=imaqhwinfo(&#39;winvideo&#39;) win_info = AdaptorDllName:&#39;D:\Program Files\MATLAB\R2010A\toobox\imaqadaptors\win32\mwwinvideoimaq.dll&#39; AdaptorDllVersion: &#39;3.5 (R2010a)&#39; AdaptorName: &#39;winvideo&#39; DeviceIDs: {[1]} DeviceInfo: [1x1 struct] &amp;gt; win_info.DeviceIDs ans = [1] &amp;gt;&amp;gt; dev_win_info=win_info.DeviceInfo dev_win_info = DefaultFormat: &#39;YUV2_160x120&#39; DeviceFileSupported: 0 DeviceName: &#39;USB 口 口 口 口&#39; DeviceID: 1 ObjectConstructor: &#39;videoinput(&#39;winvideo&#39;, 1)&#39; SupportedFormats: {&#39;YUV2_160x120&#39; &#39;YUV2_176x144&#39; &#39;YUV2_320x240&#39; &#39;YUV2_352x288&#39; &#39;YUV2_640x480&#39;} &amp;gt;&amp;gt; dev_win_info.</description>
    </item>
    <item>
      <title>基于深度学习的图像去噪（论文总结）</title>
      <link>https://anwangtanmi.github.io/posts/4c99d94ed29bc9dcccc1de28ae0e37b3/</link>
      <pubDate>Sun, 24 Jan 2016 00:24:47 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4c99d94ed29bc9dcccc1de28ae0e37b3/</guid>
      <description>2015&#xA;深度学习、自编码器、低照度图像增强&#xA;Lore, Kin Gwn, Adedotun Akintayo, and Soumik Sarkar. “LLNet: A Deep Autoencoder Approach to Natural Low-light Image Enhancement.” arXiv preprint arXiv:1511.03995 (2015).&#xA;利用深度学习的自编码器方法训练不同低照度图像信号的特征来实现自适应变亮和去噪，主要是通过非线性暗化和添加高斯噪声的方法来模拟低照度环境，进行图像对比度增强和去噪。&#xA;2014&#xA;深度学习、深度卷积神经网络、图像去卷积&#xA;Xu, Li, et al. “Deep convolutional neural network for image deconvolution.”Advances in Neural Information Processing Systems. 2014.&#xA;利用深度卷积神经网络进行图像去卷积，实现图像复原，优点：相比于当前其他方法，有更好的PSNR值和视觉效果。&#xA;2014&#xA;深度学习、稀疏编码、自编码器、图像去噪&#xA;Li, HuiMing. “Deep Learning for Image Denoising.” International Journal of Signal Processing, Image Processing and Pattern Recognition 7.3 (2014): 171-180.&#xA;利用稀疏编码（sparsecoding）与自编码器（Auto-encoder）两种方法结合来实现图像去噪，不足之处是只对图像进行处理，没有涉及视频。&#xA;2014</description>
    </item>
    <item>
      <title>泊松融合</title>
      <link>https://anwangtanmi.github.io/posts/d766612d6bd61db7d33abcb5e0ed0301/</link>
      <pubDate>Mon, 16 Nov 2015 16:02:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/d766612d6bd61db7d33abcb5e0ed0301/</guid>
      <description>拖拖拉拉快一个月了 这是探索出来的 但感觉效果不是特别好 我目前只能做到这样子了：想把左图的人放进右图的水池中 用泊松融合想实现无缝自然的效果：&#xA;左图是直接做mask镶嵌的结果 右图是解了泊松方程后的最终的结果 我所能得到的：&#xA;感觉效果不够好 因为这个理想结果是这样：&#xA;看这个就比我的自然很多很多 我的应该是有问题的 但我是按照步骤来的 目前不知道错在哪里 在找：[今天经过网友的提醒，找到了，原来是迭代次数问题，我这个效果是只迭代一次的，再迭代几次就能达到理想效果，那篇论文中没有提过这个迭代！害死我了]&#xA;A=imread(‘F:\fisheye\pond.jpg’);&#xA;src=A;&#xA;[ma,na,ka]=size(A);&#xA;dst=imread(‘F:\fisheye\swim.jpg’);&#xA;se=strel(‘diamond’,10);&#xA;B_erode=imerode(dst,se);&#xA;Berodelogical=im2bw(B_erode);&#xA;imshow(Berodelogical)&#xA;B=dst;&#xA;[mb,nb,kb]=size(B);&#xA;&amp;gt;&amp;gt; for i=1:mb&#xA;for j=1:nb&#xA;if(Berodelogical(i,j)==1)&#xA;Berodelogical(i,j)=0;&#xA;else&#xA;Berodelogical(i,j)=1;&#xA;end&#xA;end&#xA;end&#xA;dstX=100;&#xA;dstY=100;&#xA;for i=dstY:dstY+mb-1&#xA;for j=dstX:dstX+nb-1&#xA;ii=i-(dstY-1);&#xA;jj=j-(dstX-1);&#xA;if(Berodelogical(ii,jj)==1)&#xA;A(i,j,1)=B(ii,jj,1);&#xA;A(i,j,2)=B(ii,jj,2);&#xA;A(i,j,3)=B(ii,jj,3);&#xA;end&#xA;end&#xA;end&#xA;ROI=uint8(zeros(mb,nb,3));&#xA;for i=dstY:dstY+mb-1&#xA;for j=dstX:dstX+nb-1&#xA;ii=i-(dstY-1);&#xA;jj=j-(dstX-1);&#xA;ROI(ii,jj,1)=A(i,j,1);&#xA;ROI(ii,jj,2)=A(i,j,2);&#xA;ROI(ii,jj,3)=A(i,j,3);&#xA;end&#xA;end&#xA;w=[0,-1,1];&#xA;ROIgradienty=imfilter(double(ROI),w,’conv’);&#xA;ROIgradientx=imfilter(double(ROI),w’,’conv’);&#xA;%接下来对梯度求偏导得到融合图像的散度 lap</description>
    </item>
    <item>
      <title>使用matlab绘画柱状图，且使用不同的图案填充</title>
      <link>https://anwangtanmi.github.io/posts/edde0ad68472d98b7d4a377bb442032f/</link>
      <pubDate>Thu, 16 Jul 2015 22:30:45 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/edde0ad68472d98b7d4a377bb442032f/</guid>
      <description>﻿﻿ 在论文中，图表往往发挥着极为重要的作用，好的图表将能进一步提升论文的质量。在书写论文时，很多时候需要绘制柱状图，然而不同的柱状图如果采用颜色区分，当论文打印以后，视觉效果大打折扣，甚至无法区分。在遇到这个问题时，我通过网站论坛搜索，终于找到了在matlab中绘制柱状图，并采用不同的图案进行表示。主要利用下面的代码。 代码出自：http://www.aos.wisc.edu/~dvimont/matlab/Graphics_Tools/applyhatch.html&#xA;function applyhatch(h,patterns,colorlist)&#xA;%APPLYHATCH Apply hatched patterns to a figure&#xA;% APPLYHATCH(H,PATTERNS) creates a new figure from the figure H by&#xA;% replacing distinct colors in H with the black and white&#xA;% patterns in PATTERNS. The format for PATTERNS can be&#xA;% a string of the characters ‘/’, ‘\’, ‘|’, ‘-‘, ‘+’, ‘x’, ‘.’&#xA;% a cell array of matrices of zeros (white) and ones (black)</description>
    </item>
    <item>
      <title>18大经典数据挖掘算法小结</title>
      <link>https://anwangtanmi.github.io/posts/4766cd206629065c1decfe08cecd9936/</link>
      <pubDate>Fri, 27 Feb 2015 10:04:01 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/4766cd206629065c1decfe08cecd9936/</guid>
      <description>本文所有涉及到的数据挖掘代码的都放在了我的github上了。&#xA;地址链接: https://github.com/linyiqun/DataMiningAlgorithm&#xA;大概花了将近2个月的时间，自己把18大数据挖掘的经典算法进行了学习并且进行了代码实现，涉及到了决策分类，聚类，链接挖掘，关联挖掘，模式挖掘等等方面。也算是对数据挖掘领域的小小入门了吧。下面就做个小小的总结，后面都是我自己相应算法的博文链接，希望能够帮助大家学习。&#xA;1.C4.5算法。C4.5算法与ID3算法一样，都是数学分类算法，C4.5算法是ID3算法的一个改进。ID3算法采用信息增益进行决策判断，而C4.5采用的是增益率。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42395865&#xA;2.CART算法。CART算法的全称是分类回归树算法，他是一个二元分类，采用的是类似于熵的基尼指数作为分类决策，形成决策树后之后还要进行剪枝，我自己在实现整个算法的时候采用的是代价复杂度算法，&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42558235&#xA;3.KNN(K最近邻)算法。给定一些已经训练好的数据，输入一个新的测试数据点，计算包含于此测试数据点的最近的点的分类情况，哪个分类的类型占多数，则此测试点的分类与此相同，所以在这里,有的时候可以复制不同的分类点不同的权重。近的点的权重大点，远的点自然就小点。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42613011&#xA;4.Naive Bayes(朴素贝叶斯)算法。朴素贝叶斯算法是贝叶斯算法里面一种比较简单的分类算法，用到了一个比较重要的贝叶斯定理，用一句简单的话概括就是条件概率的相互转换推导。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42680161&#xA;5.SVM(支持向量机)算法。支持向量机算法是一种对线性和非线性数据进行分类的方法，非线性数据进行分类的时候可以通过核函数转为线性的情况再处理。其中的一个关键的步骤是搜索最大边缘超平面。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42780439&#xA;6.EM(期望最大化)算法。期望最大化算法，可以拆分为2个算法，1个E-Step期望化步骤,和1个M-Step最大化步骤。他是一种算法框架，在每次计算结果之后，逼近统计模型参数的最大似然或最大后验估计。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/42921789&#xA;7.Apriori算法。Apriori算法是关联规则挖掘算法，通过连接和剪枝运算挖掘出频繁项集，然后根据频繁项集得到关联规则，关联规则的导出需要满足最小置信度的要求。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43059211&#xA;8.FP-Tree(频繁模式树)算法。这个算法也有被称为FP-growth算法，这个算法克服了Apriori算法的产生过多侯选集的缺点，通过递归的产生频度模式树，然后对树进行挖掘，后面的过程与Apriori算法一致。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43234309&#xA;9.PageRank(网页重要性/排名)算法。PageRank算法最早产生于Google,核心思想是通过网页的入链数作为一个网页好快的判定标准，如果1个网页内部包含了多个指向外部的链接，则PR值将会被均分，PageRank算法也会遭到Link Span攻击。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43311943&#xA;10.HITS算法。HITS算法是另外一个链接算法，部分原理与PageRank算法是比较相似的，HITS算法引入了权威值和中心值的概念，HITS算法是受用户查询条件影响的，他一般用于小规模的数据链接分析，也更容易遭受到攻击。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43311943&#xA;11.K-Means(K均值)算法。K-Means算法是聚类算法，k在在这里指的是分类的类型数，所以在开始设定的时候非常关键，算法的原理是首先假定k个分类点，然后根据欧式距离计算分类，然后去同分类的均值作为新的聚簇中心，循环操作直到收敛。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43373159&#xA;12.BIRCH算法。BIRCH算法利用构建CF聚类特征树作为算法的核心，通过树的形式，BIRCH算法扫描数据库，在内存中建立一棵初始的CF-树，可以看做数据的多层压缩。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43532111&#xA;13.AdaBoost算法。AdaBoost算法是一种提升算法，通过对数据的多次训练得到多个互补的分类器，然后组合多个分类器，构成一个更加准确的分类器。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43635115&#xA;14.GSP算法。GSP算法是序列模式挖掘算法。GSP算法也是Apriori类算法，在算法的过程中也会进行连接和剪枝操作，不过在剪枝判断的时候还加上了一些时间上的约束等条件。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43699083&#xA;15.PreFixSpan算法。PreFixSpan算法是另一个序列模式挖掘算法，在算法的过程中不会产生候选集，给定初始前缀模式，不断的通过后缀模式中的元素转到前缀模式中，而不断的递归挖掘下去。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43766253&#xA;16.CBA(基于关联规则分类)算法。CBA算法是一种集成挖掘算法，因为他是建立在关联规则挖掘算法之上的，在已有的关联规则理论前提下，做分类判断，只是在算法的开始时对数据做处理，变成类似于事务的形式。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43818787&#xA;17.RoughSets(粗糙集)算法。粗糙集理论是一个比较新颖的数据挖掘思想。这里使用的是用粗糙集进行属性约简的算法，通过上下近似集的判断删除无效的属性，进行规制的输出。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43876001&#xA;18.gSpan算法。gSpan算法属于图挖掘算法领域。，主要用于频繁子图的挖掘，相较于其他的图算法，子图挖掘算法是他们的一个前提或基础算法。gSpan算法用到了DFS编码，和Edge五元组，最右路径子图扩展等概念，算法比较的抽象和复杂。&#xA;详细介绍链接：http://blog.csdn.net/androidlushangderen/article/details/43924273</description>
    </item>
  </channel>
</rss>
