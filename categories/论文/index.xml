<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>论文 on 暗网探秘</title>
    <link>https://anwangtanmi.github.io/categories/%E8%AE%BA%E6%96%87/</link>
    <description>Recent content in 论文 on 暗网探秘</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 08 Jul 2019 10:55:43 +0800</lastBuildDate>
    <atom:link href="https://anwangtanmi.github.io/categories/%E8%AE%BA%E6%96%87/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learning sparse network using target dropout（文末有代码链接）</title>
      <link>https://anwangtanmi.github.io/posts/6e3cd52bc8079e565e8800c076966293/</link>
      <pubDate>Mon, 08 Jul 2019 10:55:43 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/6e3cd52bc8079e565e8800c076966293/</guid>
      <description>摘要 当神经网络的权值的数量超过了从输入映射到输出需要的权值数量，神经网络会更容易优化。这里暗存了一个两个阶段的学习进程：首先学习一个大的网络，然后删除连接或隐藏的单元。但是，标准的训练并不一定会使得网络易于修剪。于是，我们介绍了一种训练神经网络的方法——target dropout（定向dropout），使其对后续剪枝具有较强的鲁棒性。在计算每次权值更新的梯度之前，定向dropout使用简单的自增强稀疏性准则，推测地选择一组要删除的单元或权重，然后计算剩余权重的梯度。所得到的网络对于删除集中经常出现的权值或单位的事后剪枝具有很强的鲁棒性。该方法改进了更复杂的稀疏正则器，实现简单，易于调优。&#xA;介绍 神经网络是一种功能强大的模型，可以在对象识别、语音识别和机器翻译等广泛任务上达到最新水平。其中原因之一是，神经网络强大的灵活性，因为它们有许多可学习的参数。然而，这种灵活性随之而来的是过拟合，并且会增加不必要的计算和存储压力。&#xA;在模型压缩的方法方面已经做了大量的工作。一种直观的策略是稀疏化:从网络中删除权重或整个神经元。在学习过程中，可以使用稀疏性诱导规则来鼓励网络稀疏化，比如L1，L0正则化等。也可以使用post hoc剪枝：训练一个全尺寸的网络，然后根据一些修剪策略进行稀疏化。理想情况下，从任务性能方面考虑，我们将删除为任务提供最少增益的权重或单元。一般来说，寻找最优集是一个困难的组合问题，即使是一个贪婪策略也存在不现实的任务评估数量，因为通常模型有数百万个参数。因此，常用的剪枝策略侧重于快速逼近，例如删除幅度最小的权重，或者根据任务性能相对于权重的敏感性对权重进行排序，然后删除最不敏感的权重。我们希望这些近似与任务性能很好地相关，这样剪枝就会产生高度压缩的网络，同时对任务性能几乎没有负面影响，但情况可能并非总是如此。&#xA;我们的方法是基于观察到dropout正则化，在训练过程中，它通过每次向前传递时对网络进行稀疏化来增强稀疏性容忍度。这鼓励网络学习对特定形式的事后稀疏化具有鲁棒性的表示形式，在本例中，是删除随机节点集。我们的假设是，如果我们计划进行显式的事后稀疏化，那么我们可以通过将dropout应用于我们之前认为是最没用的节点集来做得更好。我们称之为定向dropout。其思想是根据一些快速的、近似的重要性度量(如大小)对权重或单位进行排序，然后将dropout主要应用于那些被认为不重要的因素。与dropout正则的观察相似，我们表明，这使得网络去学习一种表示，其中权重或单位的重要性更接近我们的近似。换句话说，网络学会了对我们选择的事后剪枝策略保持了鲁棒性。&#xA;与其他方法相比，定向dropout的优势在于，它使网络对所选择的事后剪枝策略有鲁棒性，能够对所需的稀疏模式进行密切控制，并且容易实施。该方法在广泛的体系结构和数据集上实现了令人印象深刻的稀疏率;值得注意的是，在cifar10上，ResNet-32体系结构的稀疏性为99%，测试集精度下降不到4%。&#xA;背景 为了展示定向dropout，我们首先简要介绍了一些符号，并回顾了dropout和基于数值大小的剪枝的概念。&#xA;符号 假设我们正处理一个特定的网络架构，用 代表从候选集 中提取的神经网络参数向量， 是参数的个数， 表示参数为 的神经网络中权值矩阵的集合。因此， 是在网络连接层与层的权重矩阵。我们只考虑权重，为了收敛而忽略偏置，在剪枝过程中，偏置并没有移除。为简洁起见，我们使用符号 ，下标o表示连接下一层到第 个输出节点的权重（即权矩阵的第 列）， 表示W中的列数， 指的是行数。每一列对应的是一个隐藏神经元，或者是或者卷积层的特征图。注意,压扁和连接所有的在 中的权重矩会恢复。&#xA;Dropout&#xA;我们的工作使用了两种最流行的伯努利dropout，分别是Hintion等人提出的节点dropout，和Wan等人提出的权重dropout（dropconnect），对于全连接层，输入为张量X，权重矩阵是W，输出张量为Y，掩模&#xA;，我们定义两种技术如下：&#xA;Unit dropout：&#xA;节点dropout在训练的每一步中，随机drop节点（通常认为是神经元）以降低以减少节点之间的依赖，防止过度拟合。 Weight dropout:&#xA;权重dropout在训练的每一步中随机drop单独的权重。直观地说，这是在减少层之间的连接，迫使网络在每个训练步骤中适应不同的连接。&#xA;基于数值的剪枝&#xA;一个比较常用的剪枝方式是基于数值的剪枝策略，这些策略认为，只有值最大的k个权重才是重要的，我们调用argmax-k从而可以获得考虑的所有元素中的top-k元素（节点或者是权重）。&#xA;节点剪枝：&#xA;考虑L2范数下权重矩阵的节点(列向量)（根据每一层的权重L2范数判定节点重不重要？）&#xA;权重剪枝：（根据权重的绝对值，判定权重重不重要）&#xA;而在较粗的剪枝下，权值剪枝倾向于保留更多的任务性能，节点剪枝允许相当大的计算节省。特别是权值剪枝网络可以通过稀疏线性代数运算来实现，它只在足够稀疏的条件下提供加速；当节点剪枝网络在较低维张量上执行标准线性代数运算时，对于给定的固定稀疏率，这往往是一个更快的选择。&#xA;定向dropout&#xA;考虑一个神经网络的参数为 ，重要性判别标准（上述定义的w( )），我们希望找到一个最优参数 ，这样loss函数 就会变小，同时， ，也就是我们只保留网络最大的k个权重。确定的剪枝函数，会挑出 个，并把它们dropout。但是我们希望的是，如果低的权重值在训练期间变得重要，那么它的权重值就应该得到增加。因为，我们在这个过程中引入了随机变量，一个是定向概率 和drop概率 ，定向概率指的是我们挑选最小的 个权重作为dropout的候选集，在里面我们独立地drop掉 %的元素。这意味着每一轮定向dropout后保持的节点数为 。接下来我们会看到，结果是减少了重要子网络对不重要子网络的依赖，从而减少了由于在训练结束时进行修剪而导致的性能下降。&#xA;重要子网络和不重要子网络之间的依赖关系&#xA;定向dropout的目标是降低重要子集对其补集的依赖。一个常用的看法是，dropout的目的是防止节点之间相互适应，也就是，当某一节点被dropout后，剩余的网络不能再依赖于该节点对函数的贡献，必须学会通过更可靠的渠道传播该节点的信息。另一种描述是dropout最大化了同一层中节点之间的相互信息，从而减少了节点丢失的影响。与我们的方法类似，dropout可用于指导表示的属性。例如，嵌套的dropout，根据与每个单元相关联的特定下降速率，在单元之间强加“层次结构”。Dropout本身也可以解释为贝叶斯近似。&#xA;在我们特定的剪枝场景中，我们可以从一个演示的例子中得到更相关的直觉，在这个例子中，重要的子网与不重要的子网完全分离。假设一个神经网络有两个互不重叠的子网络构成，每个子网络都能够自己产生正确的输出，网络输出作为两个子网络输出的平均值。如果我们的重要性准则认为第一个子网络重要，第二个子网不重要（更具体地说，它的权值更小），然后，给不重要的子网络（第二个）加上噪声（也就是，dropout）意味着在非零概率下，我们将破坏网络输出。因为重要已经可以预测出正确的结果，为了降低loss，我们必须将不重要的子网络输出层的权值降低到零，实际上，“杀死”了那个子网，并加强了重要子网和不重要子网之间的分离。&#xA;这些解释清楚了为什么dropout应该被认为是修剪应用的自然工具。我们可以通过比较训练好的网络和没有经过定向dropout的网络，并且检查黑森矩阵和梯度来确定网络对要修剪的权值/单位的依赖关系，从而实证地确定定向dropout对权重依赖的影响。如LeCun提出，我们可以通过考虑损失变化的二阶泰勒展开来估计剪枝权值的影响，&#xA;（要去掉的权重）否则就为0,。 是loss的梯度，H是黑塞矩阵。在训练结束时，如果我们找到了临界点， ，并且， ，就只留黑塞矩阵一项。在我们的实验中，我们证明了目标target降低了重要子网络和不重要子网络之间的依赖性。&#xA;相关工作 神经网络的剪枝和稀疏化研究已有近30年的历史，由于在移动电话和asic等资源有限的设备上实现了剪枝和稀疏化，因此人们对神经网络的兴趣大增。早期的工作，如脑损伤（OBD）和optimal brain surgeon；最近的工作，有的利用二阶泰勒展开式对训练到局部最小值的权值周围的损失函数进行展开式，以收集选择参数剪枝顺序的策略。Han等人将权值量化与剪枝相结合，得到了令人印象深刻的网络压缩结果，大幅降低网络的空间成本。Dong等人通过各个分层独立的假设，提高了最优脑外科手术的效率。Wen等人提出在卷积滤波器上使用组Lasso，并且能够从ResNet-20网络中删除多达6层，从而增加1%的误差。&#xA;为了开发改进的剪枝启发式算法和稀疏化正则化器，已经付出了大量的努力，这些一般包括两个部分：第一种是将正则化方案纳入训练中，使重要的子网络易于识别为事后剪枝策略; 第二种是一种特殊的事后剪枝策略，它在预先训练好的网络上运行，去掉不重要的子网络。&#xA;和我们工作最相关的两个是L0正则化和变分dropout。Louizos等人改编了concrete dropout对网络权重进行调整，并调节dropout率，以使网络稀疏化。类似地，Molchanov等人将变分dropout应用于网络的权重上，并注意到前者通过选择较大的drop率隐式地稀疏了参数。除了我们的方法能更有效地缩小重要子网的大小，有针对性的dropout使用了两个直观的超参数， 和 ，并直接控制整个训练的稀疏性（即，达到预定的稀疏阈值）。相比之下，Louizos使用的hard-concrete使用了三个超参数，并且为每一个参数引入了门参数，使得可训练参数量翻倍。Molchanov等人增加了两个超参数，并将可训练参数的数量增加了一倍。在我们的实验中，我们也和L1正则化进行了比较，L1正则化的目的是使得不重要的权重趋近于零。</description>
    </item>
    <item>
      <title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems（用于Web级推荐系统的图形卷积神经网络）</title>
      <link>https://anwangtanmi.github.io/posts/c63629bfc04e210195eb67afbaba6cf1/</link>
      <pubDate>Sun, 07 Jul 2019 09:52:26 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/c63629bfc04e210195eb67afbaba6cf1/</guid>
      <description>Graph Convolutional Neural Networks for Web-Scale Recommender Systems 用于Web级推荐系统的图形卷积神经网络 ABSTRACT Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge.&#xA;Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.</description>
    </item>
    <item>
      <title>DCN:Deep &amp; Cross Network for Ad Click Predictions简介</title>
      <link>https://anwangtanmi.github.io/posts/0b2bedc004e34953f1e5bea95cd9ac17/</link>
      <pubDate>Sat, 16 Mar 2019 16:16:39 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/0b2bedc004e34953f1e5bea95cd9ac17/</guid>
      <description>Deep &amp;amp; Cross Network for Ad Click Predictions 摘要 作者起草了DCN，该网络可以保持DNN的优点（隐式地生成特征之间的交互），同时又利用交叉网络来对特征进行显式的交叉计算。这也不要求手工的特征工程，同时只是在DNN的基础上加了一些可容忍的复杂度。实验证明DCN已经在CTR预估与分类问题上超过了sota。&#xA;介绍 对于web伸缩型的推荐系统，因为其产生的数据较为稀疏，对于线性模型来说已经不太好处理了。因此交叉特征变得很重要，但是这经常要求我们手动特征工程，为了减少这方面的工作，交叉网络应运而生。同时联合DNN，发挥两者的共同优势。&#xA;嵌入和堆叠层 对于离散数据，一般处理时会被编码成one-hot向量，对于实际应用中维度会非常高，因此使用&#xA;来将这些离散特征转换成实数值的稠密向量，最后将嵌入向量和连续特征向量堆叠在一起形成一个向量。&#xA;交叉网络 对于每层的计算，使用下述公式：&#xA;一层交叉层的可视化如下图所示：&#xA;该网络可以使交叉特征的次数随着层数的增加而不断变大，对于l层其最高多项式次数为l+1。&#xA;计算的时间与空间复杂度都是线性，因此DCN的效率与DNN是一个量级的。&#xA;深度网络 深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：&#xA;链接层 将两个网络的输出联合，送进标准的logits层。&#xA;正则化的log loss函数：&#xA;实验 比较DCN，DC，DNN，FM，LR模型的最好的logloss&#xA;比较实现对应的logloss，DNN和DCN需要的参数数量&#xA;在固定参数下实现最好的logloss所需要的内存&#xA;在层数与结点一致的情况下，比较DNN与DCN的logloss(X&#xA;1&#xA;0&#xA;−&#xA;2&#xA;10^{-2}&#xA;10−2)差距，负值代表DCN表现好于DNN&#xA;最后展示了不同设置的变化趋势。&#xA;可以参考我的github来看看源代码，如有错误，欢迎交流。</description>
    </item>
    <item>
      <title>《Learning Deep Structured Semantic Models for Web Search using Clickthrough Data 》论文总结</title>
      <link>https://anwangtanmi.github.io/posts/019e8f7496190591ea3208fbc0562fbd/</link>
      <pubDate>Tue, 12 Mar 2019 22:14:10 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/019e8f7496190591ea3208fbc0562fbd/</guid>
      <description>1.背景 DSSM是Deep Structured Semantic Model的缩写，即我们通常说的基于深度网络的语义模型，其核心思想是将query和doc映射到到共同维度的语义空间中，通过最大化query和doc语义向量之间的余弦相似度，从而训练得到隐含语义模型，达到检索的目的。DSSM有很广泛的应用，比如：搜索引擎检索，广告相关性，问答系统，机器翻译等。&#xA;2. DSSM 2.1简介&#xA;DSSM [1]（Deep Structured Semantic Models）的原理很简单，通过搜索引擎里 Query 和 Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。&#xA;DSSM 从下往上可以分为三层结构：输入层、表示层、匹配层&#xA;典型的DNN结构是将原始的文本特征映射为在语义空间上表示的特征。DNN在搜索引擎排序中主要是有下面2个作用：&#xA;将query中term的高维向量映射为低维语义向量 根据语义向量计算query与doc之间的相关性分数 通常， x用来表示输入的term向量， y表示输出向量， l_{i}，i=1,…,N-1 表示隐藏层， Wi表示第 i层的参数矩&#xA;阵， bi表示 第 i个偏置项。&#xA;我们使用 tanh作为输出层和隐藏层的激活函数，有下列公式。&#xA;在搜索排序中，我们使用 Q来表示一个query， D来表示一个doc，那么他们的相关性分数可以用下面的公式衡量：&#xA;其中， yQ与 yD是query与doc的语义向量。在搜索引擎中，给定一个query，会返回一些按照相关性分数排序的文档。&#xA;通常情况下，输入的term向量使用最原始的bag of words特征，通过one-hot进行编码。但是在实际场景中，词典的大小将会非常大，如果直接将该数据输入给DNN，神经网络是无法进行训练和预测的。因此，在DSSM中引入了word hashing的方法，并且作为DNN中的第一层。&#xA;2.2 word hashing&#xA;word hashing方法是用来减少输入向量的维度，该方法基于字母的 -gram。给定一个单词（good），我们首先增加词的开始和结束部分（#good#），然后将该词转换为字母 -gram的形式（假设为trigrams：#go，goo，ood，od#）。最后该词使用字母 -gram的向量来表示。&#xA;这种方法的问题在于有可能造成冲突，因为两个不同的词可能有相同的 -gram向量来表示。下图显示了word hashing在2个词典中的统计。与原始的ont-hot向量表示的词典大小相比，word hashing明显降低了向量表示的维度。&#xA;2.3 DSSM的学习&#xA;点击日志里通常包含了用户搜索的query和用户点击的doc，可以假定如果用户在当前query下对doc进行了点击，则该query与doc是相关的。通过该规则，可以通过点击日志构造训练集与测试集。&#xA;首先，通过softmax 函数可以把query 与样本 doc 的语义相似性转化为一个后验概率：</description>
    </item>
    <item>
      <title>DehazeNet: An End-to-End System for Single Image Haze Removal</title>
      <link>https://anwangtanmi.github.io/posts/f57dbb49bb524efa33d0c4c9c9b5d686/</link>
      <pubDate>Fri, 04 May 2018 19:15:37 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/f57dbb49bb524efa33d0c4c9c9b5d686/</guid>
      <description>项目主页：http://caibolun.github.io/DehazeNet/&#xA;GitHub代码 ：https://github.com/caibolun/DehazeNet&#xA;BReLU+Caffe ：https://github.com/zlinker/mycaffe&#xA;其他复现：（1）https://github.com/zlinker/DehazeNet （2）https://github.com/allenyangyl/dehaze&#xA;总结：&#xA;提出一种名为DehazeNet的可训练的端到端系统，用于传输值估计。 DehazeNet将模糊图像作为输入，并输出其中间透射图，随后用于通过大气散射模型恢复无雾图像。 DehazeNet采用基于卷积神经网络的深层架构，其层专门设计用于体现图像去雾中已建立的假设/先验。具体而言，Maxout单位的图层用于特征提取，这可以生成几乎所有与雾相关的特征。我们还在DehazeNet中提出了一种新的非线性激活函数，称为双边整流线性单元，它能够提高恢复的无雾图像的质量。我们在提议的DehazeNet的组件与现有方法中使用的组件之间建立连接。基准图像的实验表明，DehazeNet比现有方法具有更高的性能，同时保持高效和易用。&#xA;摘要 背景：Single image haze removal is a challenging ill-posed problem.&#xA;现存方法：Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image.&#xA;提出的方法：In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model.</description>
    </item>
    <item>
      <title>深度学习之图像修复</title>
      <link>https://anwangtanmi.github.io/posts/51a42ff71f44b546bcf6a25e9b01c196/</link>
      <pubDate>Sun, 19 Mar 2017 18:25:44 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/51a42ff71f44b546bcf6a25e9b01c196/</guid>
      <description>图像修复问题就是还原图像中缺失的部分。基于图像中已有信息，去还原图像中的缺失部分。&#xA;从直观上看，这个问题能否解决是看情况的，还原的关键在于剩余信息的使用，剩余信息中如果存在有缺失部分信息的patch，那么剩下的问题就是从剩余信息中判断缺失部分与哪一部分相似。而这，就是现在比较流行的PatchMatch的基本思想。&#xA;CNN出现以来，有若干比较重要的进展：&#xA;被证明有能力在CNN的高层捕捉到图像的抽象信息。 Perceptual Loss的出现证明了一个训练好的CNN网络的feature map可以很好的作为图像生成中的损失函数的辅助工具。 GAN可以利用监督学习来强化生成网络的效果。其效果的原因虽然还不具可解释性，但是可以理解为可以以一种不直接的方式使生成网络学习到规律。 基于上述三个进展，参考文献[1]提出了一种基于CNN的图像复原方法。&#xA;CNN网络结构 该算法需要使用两个网络，一个是内容生成网络，另一个是纹理生成网络。内容生成网络直接用于生成图像，推断缺失部分可能的内容。纹理生成网络用于增强内容网络的产出的纹理，具体则为将生成的补全图像和原始无缺失图像输入进纹理生成网络，在某一层feature_map上计算损失，记为Loss NN。&#xA;内容生成网络需要使用自己的数据进行训练，而纹理生成网络则使用已经训练好的VGG Net。这样，生成图像可以分为如下几个步骤：&#xA;定义缺失了某个部分的图像为x0&#xA;x0输入进内容生成网络得到生成图片x x作为最后生成图像的初始值 保持纹理生成网络的参数不变，使用Loss NN对x进行梯度下降，得到最后的结果。 关于内容生成网络的训练和Loss NN的定义，下面会一一解释&#xA;内容生成网络 生成网络结构如上，其损失函数使用了L2损失和对抗损失的组合。所谓的对抗损失是来源于对抗神经网络.&#xA;在该生成网络中，为了是训练稳定，做了两个改变：&#xA;将所有的ReLU/leaky-ReLU都替换为ELU层 使用fully-connected layer替代chnnel-wise的全连接网络。 纹理生成网络 纹理生成网络的Loss NN如下：&#xA;它分为三个部分，即Pixel-wise的欧式距离，基于已训练好纹理网络的feature layer的perceptual loss，和用于平滑的TV Loss。&#xA;α和β都是5e-6，&#xA;Pixel-wise的欧氏距离如下：&#xA;TV Loss如下：&#xA;Perceptual Loss的计算比较复杂，这里利用了PatchMatch的信息，即为缺失部分找到最近似的Patch，为了达到这一点，将缺失部分分为很多个固定大小的patch作为query，也将已有的部分分为同样固定大小的patch，生成dataset PATCHES，在匹配query和PATCHES中最近patch的时候，需要在纹理生成网络中的某个layer的激活值上计算距离而不是计算像素距离。&#xA;但是，寻找最近邻Patch这个操作似乎是不可计算导数的，如何破解这一点呢？同MRF+CNN类似，在这里，先将PATCHES中的各个patch的的feature_map抽取出来，将其组合成为一个新的卷积层，然后得到query的feature map后输入到这个卷积层中，最相似的patch将获得最大的激活值，所以将其再输入到一个max-pooling层中，得到这个最大值。这样，就可以反向传播了。&#xA;高清图像上的应用 本算法直接应用到高清图像上时效果并不好，所以，为了更好的初始化，使用了Stack迭代算法。即先将高清图像down-scale到若干级别[1,2,3,…,S]，其中S级别为原图本身，然后在级别1上使用图像均值初始化缺失部分，得到修复后的结果，再用这个结果，初始化下一级别的输入。以此类推。&#xA;效果 上图从上往下一次为，有缺失的原图，PatchMatch算法，Context Decoder算法（GAN+L2)和本算法。&#xA;内容生成网络的作用 起到了内容限制的作用，上图比较了有内容生成网络和没有内容生成网络的区别，有的可以在内容上更加符合原图。&#xA;应用 图像的语义编辑，从左到右依次为原图，扣掉某部分的原图，PatchMatch结果，和本算法结果。&#xA;可知，该方法虽然不可以复原真实的图像，但却可以补全成一张完整的图像。这样，当拍照中有不想干的物体或人进入到摄像头中时，依然可以将照片修复成一张完整的照片。&#xA;总结 CNN的大发展，图像越来越能够变得语义化了。有了以上的图像复原的基础，尽可以进行发挥自己的想象，譬如：在图像上加一个东西，但是光照和颜色等缺明显不搭，可以用纹理网络进行修复。&#xA;该方法的缺点也是很明显：&#xA;性能和内存问题 只用了图片内的patch，而没有用到整个数据集中的数据。 参考文献 [1]. Yang C, Lu X, Lin Z, et al. High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis[J].</description>
    </item>
    <item>
      <title>暗黑破坏神 2 私服 sf 114.215.178.67</title>
      <link>https://anwangtanmi.github.io/posts/53f5a55f1f9ca9f4b1c4b01a08d7aca3/</link>
      <pubDate>Wed, 09 Mar 2016 14:22:55 +0800</pubDate>
      <guid>https://anwangtanmi.github.io/posts/53f5a55f1f9ca9f4b1c4b01a08d7aca3/</guid>
      <description>注册表&#xA;REGEDIT4&#xA;[HKEY_CURRENT_USER\Software\Blizzard Entertainment\Diablo II]&#xA;“BNETIP”=”114.215.178.67”&#xA;1.11b 原版&#xA;QQ群：487460007</description>
    </item>
  </channel>
</rss>
